<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>MIT Parallel Computing and Scientific Machine Learning - 8&nbsp; Solving Stiff Ordinary Differential Equations</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./estimation_identification.html" rel="next">
<link href="./automatic_differentiation.html" rel="prev">
<link href="./sciml-book-logo.svg" rel="icon" type="image/svg+xml">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./automatic_differentiation.html">Modern Approaches</a></li><li class="breadcrumb-item"><a href="./stiff_odes.html"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Solving Stiff Ordinary Differential Equations</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./sciml-book-logo.svg" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">MIT Parallel Computing and Scientific Machine Learning</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/jvaverka/SciMLBook" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./MIT-Parallel-Computing-and-Scientific-Machine-Learning.pdf">
              <i class="bi bi-bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./MIT-Parallel-Computing-and-Scientific-Machine-Learning.epub">
              <i class="bi bi-bi-journal pe-1"></i>
            Download ePub
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction to Parallel Computing</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./optimize.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Optimizing Serial Code</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sciml.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction to Scientific Machine Learning through Physics-Informed Neural Networks</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Parallel Computing</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dynamical_systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">How Loops Work, An Introduction to Discrete Dynamics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./parallelism_overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">The Basics of Single Node Parallel Computing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./styles_of_parallelism.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">The Different Flavors of Parallelism</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./discretizing_odes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Ordinary Differential Equations, Applications and Discretizations</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Modern Approaches</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./automatic_differentiation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Forward-Mode Automatic Differentiation (AD) via High Dimensional Algebras</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./stiff_odes.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Solving Stiff Ordinary Differential Equations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./estimation_identification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Basic Parameter Estimation, Reverse-Mode AD, and Inverse Problems</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./adjoints.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Differentiable Programming and Neural Differential Equations</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Modern Architectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mpi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Introduction to MPI.jl</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./gpus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">GPU programming</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">Advanced Differential Equations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pdes_and_convolutions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">PDEs, Convolutions, and the Mathematics of Locality</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./diffeq_machine_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Mixing Differential Equations and Neural Networks for Physics-Informed Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./probabilistic_programming.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">From Optimization to Probabilistic Programming</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./global_sensitivity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Global Sensitivity Analysis</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
 <span class="menu-text">Advanced Topics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./profiling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Code Profiling and Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./uncertainty.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Uncertainty Programming, Generalized Uncertainty Quantification</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#youtube-video-link" id="toc-youtube-video-link" class="nav-link active" data-scroll-target="#youtube-video-link"><span class="header-section-number">8.1</span> Youtube Video Link</a></li>
  <li><a href="#newtons-method-and-jacobians" id="toc-newtons-method-and-jacobians" class="nav-link" data-scroll-target="#newtons-method-and-jacobians"><span class="header-section-number">8.2</span> Newton’s Method and Jacobians</a>
  <ul class="collapse">
  <li><a href="#some-quick-notes" id="toc-some-quick-notes" class="nav-link" data-scroll-target="#some-quick-notes"><span class="header-section-number">8.2.1</span> Some Quick Notes</a></li>
  </ul></li>
  <li><a href="#generation-of-the-jacobian" id="toc-generation-of-the-jacobian" class="nav-link" data-scroll-target="#generation-of-the-jacobian"><span class="header-section-number">8.3</span> Generation of the Jacobian</a>
  <ul class="collapse">
  <li><a href="#dense-finite-differences-and-forward-mode-ad" id="toc-dense-finite-differences-and-forward-mode-ad" class="nav-link" data-scroll-target="#dense-finite-differences-and-forward-mode-ad"><span class="header-section-number">8.3.1</span> Dense Finite Differences and Forward-Mode AD</a></li>
  <li><a href="#sparse-differentiation-and-matrix-coloring" id="toc-sparse-differentiation-and-matrix-coloring" class="nav-link" data-scroll-target="#sparse-differentiation-and-matrix-coloring"><span class="header-section-number">8.3.2</span> Sparse Differentiation and Matrix Coloring</a></li>
  </ul></li>
  <li><a href="#linear-solving" id="toc-linear-solving" class="nav-link" data-scroll-target="#linear-solving"><span class="header-section-number">8.4</span> Linear Solving</a></li>
  <li><a href="#jacobian-free-newton-krylov-jfnk" id="toc-jacobian-free-newton-krylov-jfnk" class="nav-link" data-scroll-target="#jacobian-free-newton-krylov-jfnk"><span class="header-section-number">8.5</span> Jacobian-Free Newton Krylov (JFNK)</a>
  <ul class="collapse">
  <li><a href="#jacobian-vector-products-as-directional-derivatives" id="toc-jacobian-vector-products-as-directional-derivatives" class="nav-link" data-scroll-target="#jacobian-vector-products-as-directional-derivatives"><span class="header-section-number">8.5.1</span> Jacobian-Vector Products as Directional Derivatives</a></li>
  <li><a href="#krylov-subspace-methods-for-solving-linear-systems" id="toc-krylov-subspace-methods-for-solving-linear-systems" class="nav-link" data-scroll-target="#krylov-subspace-methods-for-solving-linear-systems"><span class="header-section-number">8.5.2</span> Krylov Subspace Methods For Solving Linear Systems</a></li>
  </ul></li>
  <li><a href="#intermediate-conclusion" id="toc-intermediate-conclusion" class="nav-link" data-scroll-target="#intermediate-conclusion"><span class="header-section-number">8.6</span> Intermediate Conclusion</a></li>
  <li><a href="#the-need-for-speed" id="toc-the-need-for-speed" class="nav-link" data-scroll-target="#the-need-for-speed"><span class="header-section-number">8.7</span> The Need for Speed</a>
  <ul class="collapse">
  <li><a href="#preconditioning" id="toc-preconditioning" class="nav-link" data-scroll-target="#preconditioning"><span class="header-section-number">8.7.1</span> Preconditioning</a></li>
  <li><a href="#jacobian-re-use" id="toc-jacobian-re-use" class="nav-link" data-scroll-target="#jacobian-re-use"><span class="header-section-number">8.7.2</span> Jacobian Re-use</a></li>
  <li><a href="#adaptive-timestepping" id="toc-adaptive-timestepping" class="nav-link" data-scroll-target="#adaptive-timestepping"><span class="header-section-number">8.7.3</span> Adaptive Timestepping</a></li>
  </ul></li>
  <li><a href="#methodological-summary" id="toc-methodological-summary" class="nav-link" data-scroll-target="#methodological-summary"><span class="header-section-number">8.8</span> Methodological Summary</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/jvaverka/SciMLBook/edit/main/stiff_odes.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/jvaverka/SciMLBook/issues/new" class="toc-action">Report an issue</a></p><p><a href="https://github.com/jvaverka/SciMLBook/blob/main/stiff_odes.qmd" class="toc-action">View source</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Solving Stiff Ordinary Differential Equations</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="youtube-video-link" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="youtube-video-link"><span class="header-section-number">8.1</span> <a href="https://youtu.be/bY2VCoxMuo8">Youtube Video Link</a></h2>
<p>We have previously shown how to solve non-stiff ODEs via optimized Runge-Kutta methods, but we ended by showing that there is a fundamental limitation of these methods when attempting to solve stiff ordinary differential equations. However, we can get around these limitations by using different types of methods, like implicit Euler. Let’s now go down the path of understanding how to efficiently implement stiff ordinary differential equation solvers, and its interaction with other domains like automatic differentiation.</p>
<p>When one is solving a large-scale scientific computing problem with MPI, this is almost always the piece of code where all of the time is spent, so let’s understand how what it’s doing.</p>
</section>
<section id="newtons-method-and-jacobians" class="level2" data-number="8.2">
<h2 data-number="8.2" class="anchored" data-anchor-id="newtons-method-and-jacobians"><span class="header-section-number">8.2</span> Newton’s Method and Jacobians</h2>
<p>Recall that the implicit Euler method is the following:</p>
<p><span class="math display">\[u_{n+1} = u_n + \Delta t f(u_{n+1},p,t + \Delta t)\]</span></p>
<p>If we wanted to use this method, we would need to find out how to get the value <span class="math inline">\(u_{n+1}\)</span> when only knowing the value <span class="math inline">\(u_n\)</span>. To do so, we can move everything to one side:</p>
<p><span class="math display">\[u_{n+1} - \Delta t f(u_{n+1},p,t + \Delta t) - u_n = 0\]</span></p>
<p>and now we have a problem</p>
<p><span class="math display">\[g(u_{n+1}) = 0\]</span></p>
<p>This is the classic rootfinding problem <span class="math display">\[g(x)=0\]</span>, find <span class="math inline">\(x\)</span>. The way that we solve the rootfinding problem is, once again, by replacing this problem about a continuous function <span class="math inline">\(g\)</span> with a discrete dynamical system whose steady state is the solution to the <span class="math display">\[g(x)=0\]</span>. There are many methods for this, but some choices of the rootfinding method effect the stability of the ODE solver itself since we need to make sure that the steady state solution is a stable steady state of the iteration process, otherwise the rootfinding method will diverge (will be explored in the homework).</p>
<p>Thus for example, fixed point iteration is not appropriate for stiff differential equations. Methods which are used in the stiff case are either Anderson Acceleration or Newton’s method. Newton’s is by far the most common (and generally performs the best), so we can go down this route.</p>
<p>Let’s use the syntax <span class="math display">\[g(x)=0\]</span>. Here we need some starting value <span class="math inline">\(x_0\)</span> as our first guess for <span class="math inline">\(u_{n+1}\)</span>. The easiest guess is <span class="math inline">\(u_{n}\)</span>, though additional information about the equation can be used to compute a better starting value (known as a <em>step predictor</em>). Once we have a starting value, we run the iteration:</p>
<p><span class="math display">\[x_{k+1} = x_k - J(x_k)^{-1}g(x_k)\]</span></p>
<p>where <span class="math inline">\(J(x_k)\)</span> is the Jacobian of <span class="math inline">\(g\)</span> at the point <span class="math inline">\(x_k\)</span>. However, the mathematical formulation is never the syntax that you should use for the actual application! Instead, numerically this is two stages:</p>
<ul>
<li>Solve <span class="math inline">\(Ja=g(x_k)\)</span> for <span class="math inline">\(a\)</span></li>
<li>Update <span class="math inline">\(x_{k+1} = x_k - a\)</span></li>
</ul>
<p>By doing this, we can turn the matrix inversion into a problem of a linear solve and then an update. The reason this is done is manyfold, but one major reason is because the inverse of a sparse matrix can be dense, and this Jacobian is in many cases (PDEs) a large and dense matrix.</p>
<p>Now let’s break this down step by step.</p>
<section id="some-quick-notes" class="level3" data-number="8.2.1">
<h3 data-number="8.2.1" class="anchored" data-anchor-id="some-quick-notes"><span class="header-section-number">8.2.1</span> Some Quick Notes</h3>
<p>The Jacobian of <span class="math inline">\(g\)</span> can also be written as <span class="math inline">\(J = I - \gamma \frac{df}{du}\)</span> for the ODE <span class="math inline">\(u' = f(u,p,t)\)</span>, where <span class="math inline">\(\gamma = \Delta t\)</span> for the implicit Euler method. This general form holds for all other (SDIRK) implicit methods, changing the value of <span class="math inline">\(\gamma\)</span>. Additionally, the class of Rosenbrock methods solves a linear system with exactly the same <span class="math inline">\(J\)</span>, meaning that essentially all implicit and semi-implicit ODE solvers have to do the same Newton iteration process on the same structure. This is the portion of the code that is generally the bottleneck.</p>
<p>Additionally, if one is solving a mass matrix ODE: <span class="math inline">\(Mu' = f(u,p,t)\)</span>, exactly the same treatment can be had with <span class="math inline">\(J = M - \gamma \frac{df}{du}\)</span>. This works even if <span class="math inline">\(M\)</span> is singular, a case known as a <em>differential-algebraic equation</em> or a DAE. A DAE for example can be an ODE with constraint equations, and these structures can be represented as an ODE where these constraints lead to a singularity in the mass matrix (a row of all zeros is a term that is only the right hand side equals zero!).</p>
</section>
</section>
<section id="generation-of-the-jacobian" class="level2" data-number="8.3">
<h2 data-number="8.3" class="anchored" data-anchor-id="generation-of-the-jacobian"><span class="header-section-number">8.3</span> Generation of the Jacobian</h2>
<section id="dense-finite-differences-and-forward-mode-ad" class="level3" data-number="8.3.1">
<h3 data-number="8.3.1" class="anchored" data-anchor-id="dense-finite-differences-and-forward-mode-ad"><span class="header-section-number">8.3.1</span> Dense Finite Differences and Forward-Mode AD</h3>
<p>Recall that the Jacobian is the matrix of <span class="math inline">\(\frac{df_i}{dx_j}\)</span> for <span class="math inline">\(f\)</span> a vector-valued function. The simplest way to generate the Jacobian is through finite differences. For each <span class="math inline">\(h_j = h e_j\)</span> for <span class="math inline">\(e_j\)</span> the basis vector of the <span class="math inline">\(j\)</span>th axis and some sufficiently small <span class="math inline">\(h\)</span>, then we can compute column <span class="math inline">\(j\)</span> of the Jacobian by:</p>
<p><span class="math display">\[\frac{f(x+h_j)-f(x)}{h}\]</span></p>
<p>Thus <span class="math inline">\(m+1\)</span> applications of <span class="math inline">\(f\)</span> are required to compute the full Jacobian.</p>
<p>This can be improved by using forward-mode automatic differentiation. Recall that we can formulate a multidimensional duel number of the form</p>
<p><span class="math display">\[d = x + v_1 \epsilon_1 + \ldots + v_m \epsilon_m\]</span></p>
<p>We can then seed the vectors <span class="math inline">\(v_j = h_j\)</span> so that the differentiation directions are along the basis vectors, and then the output dual is the result:</p>
<p><span class="math display">\[f(d) = f(x) + J_1 \epsilon_1 + \ldots + J_m \epsilon_m\]</span></p>
<p>where <span class="math inline">\(J_j\)</span> is the <span class="math inline">\(j\)</span>th column of the Jacobian. And thus with one calculation of the <em>primal</em> (f(x)) we have calculated the entire Jacobian.</p>
</section>
<section id="sparse-differentiation-and-matrix-coloring" class="level3" data-number="8.3.2">
<h3 data-number="8.3.2" class="anchored" data-anchor-id="sparse-differentiation-and-matrix-coloring"><span class="header-section-number">8.3.2</span> Sparse Differentiation and Matrix Coloring</h3>
<p>However, when the Jacobian is sparse we can compute it much faster. We can understand this by looking at the following system:</p>
<p><span class="math display">\[f(x)=\left[\begin{array}{c}
x_{1}+x_{3}\\
x_{2}x_{3}\\
x_{1}
\end{array}\right]\]</span></p>
<p>Notice that in 3 differencing steps we can calculate:</p>
<p><span class="math display">\[f(x+\epsilon e_{1})=\left[\begin{array}{c}
x_{1}+x_{3}+\epsilon\\
x_{2}x_{3}\\
x_{1}+\epsilon
\end{array}\right]\]</span></p>
<p><span class="math display">\[f(x+\epsilon e_{2})=\left[\begin{array}{c}
x_{1}+x_{3}\\
x_{2}x_{3}+\epsilon x_{3}\\
x_{1}
\end{array}\right]\]</span></p>
<p><span class="math display">\[f(x+\epsilon e_{3})=\left[\begin{array}{c}
x_{1}+x_{3}+\epsilon\\
x_{2}x_{3}+\epsilon x_{2}\\
x_{1}
\end{array}\right]\]</span></p>
<p>and thus:</p>
<p><span class="math display">\[\frac{f(x+\epsilon e_{1})-f(x)}{\epsilon}=\left[\begin{array}{c}
1\\
0\\
1
\end{array}\right]\]</span></p>
<p><span class="math display">\[\frac{f(x+\epsilon e_{2})-f(x)}{\epsilon}=\left[\begin{array}{c}
0\\
x_{3}\\
0
\end{array}\right]\]</span></p>
<p><span class="math display">\[\frac{f(x+\epsilon e_{3})-f(x)}{\epsilon}=\left[\begin{array}{c}
1\\
x_{2}\\
0
\end{array}\right]\]</span></p>
<p>But notice that the calculation of <span class="math inline">\(e_1\)</span> and <span class="math inline">\(e_2\)</span> do not interact. If we had done:</p>
<p><span class="math display">\[\frac{f(x+\epsilon e_{1}+\epsilon e_{2})-f(x)}{\epsilon}=\left[\begin{array}{c}
1\\
x_{3}\\
1
\end{array}\right]\]</span></p>
<p>we would still get the correct value for every row because the <span class="math inline">\(\epsilon\)</span> terms do not collide (a situation known as <em>perturbation confusion</em>). If we knew the sparsity pattern of the Jacobian included a 0 at (2,1), (1,2), and (3,2), then we would know that the vectors would have to be <span class="math inline">\([1 0 1]\)</span> and <span class="math inline">\([0 x_3 0]\)</span>, meaning that columns 1 and 2 can be computed simultaneously and decompressed. This is the key to sparse differentiation.</p>
<p><img src="https://user-images.githubusercontent.com/1814174/66027457-efd7cc00-e4c8-11e9-8346-accf468541fb.PNG" class="img-fluid"></p>
<p>With forward-mode automatic differentiation, recall that we calculate multiple dimensions simultaneously by using a multidimensional dual number seeded by the vectors of the differentiation directions, that is:</p>
<p><span class="math display">\[d = x + v_1 \epsilon_1 + \ldots + v_m \epsilon_m\]</span></p>
<p>Instead of using the primitive differentiation directions <span class="math inline">\(e_j\)</span>, we can instead replace this with the mixed values. For example, the Jacobian of the example function can be computed in one function call to <span class="math inline">\(f\)</span> with the dual number input:</p>
<p><span class="math display">\[d = x + (e_1 + e_2) \epsilon_1 + e_3 \epsilon_2\]</span></p>
<p>and performing the decompression via the sparsity pattern. Thus the sparsity pattern gives a direct way to optimize the construction of the Jacobian.</p>
<p>This idea of independent directions can be formalized as a <em>matrix coloring</em>. Take <span class="math inline">\(S_{ij}\)</span> the sparsity pattern of some Jacobian matrix <span class="math inline">\(J_{ij}\)</span>. Define a graph on the nodes 1 through m where there is an edge between <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> if there is a row where <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are non-zero. This graph is the column connectivity graph of the Jacobian. What we wish to do is find the smallest set of differentiation directions such that differentiating in the direction of <span class="math inline">\(e_i\)</span> does not collide with differentiation in the direction of <span class="math inline">\(e_j\)</span>. The connectivity graph is setup so that way this cannot be done if the two nodes are adjacent. If we let the subset of nodes differentiated together be a <em>color</em>, the question is, what is the smallest number of colors s.t. no adjacent nodes are the same color. This is the classic <em>distance-1 coloring problem</em> from graph theory. It is well-known that the problem of finding the <em>chromatic number</em>, the minimal number of colors for a graph, is generally NP-complete. However, there are heuristic methods for performing a distance-1 coloring quite quickly. For example, a greedy algorithm is as follows:</p>
<ul>
<li>Pick a node at random to be color 1.</li>
<li>Make all nodes adjacent to that be the lowest color that they can be (in this step that will be 2).</li>
<li>Now look at all nodes adjacent to that. Make all nodes be the lowest color that they can be (either 1 or 3).</li>
<li>Repeat by looking at the next set of adjacent nodes and color as conservatively as possible.</li>
</ul>
<p>This can be visualized as follows:</p>
<p><img src="https://user-images.githubusercontent.com/1814174/66027433-e189b000-e4c8-11e9-8c2e-3999954cda28.PNG" class="img-fluid"></p>
<p>The result will color the entire connected component. While not giving an optimal result, it will still give a result that is a sufficient reduction in the number of differentiation directions (without solving an NP-complete problem) and thus can lead to a large computational saving.</p>
<p>At the end, let <span class="math inline">\(c_i\)</span> be the vector of 1’s and 0’s, where it’s 1 for every node that is color <span class="math inline">\(i\)</span> and 0 otherwise. Sparse automatic differentiation of the Jacobian is then computed with:</p>
<p><span class="math display">\[d = x + c_1 \epsilon_1 + \ldots + c_k \epsilon_k\]</span></p>
<p>that is, the full Jacobian is computed with one dual number which consists of the primal calculation along with <span class="math inline">\(k\)</span> dual dimensions, where <span class="math inline">\(k\)</span> is the computed chromatic number of the connectivity graph on the Jacobian. Once this calculation is complete, the colored columns can be decompressed into the full Jacobian using the sparsity information, generating the original quantity that we wanted to compute.</p>
<p>For more information on the graph coloring aspects, find the paper titled “What Color Is Your Jacobian? Graph Coloring for Computing Derivatives” by Gebremedhin.</p>
<section id="note-on-sparse-reverse-mode-ad" class="level4" data-number="8.3.2.1">
<h4 data-number="8.3.2.1" class="anchored" data-anchor-id="note-on-sparse-reverse-mode-ad"><span class="header-section-number">8.3.2.1</span> Note on Sparse Reverse-Mode AD</h4>
<p>Reverse-mode automatic differentiation can be though of as a method for computing one row of a Jacobian per seed, as opposed to one column per seed given by forward-mode AD. Thus sparse reverse-mode automatic differentiation can be done by looking at the connectivity graph of the column and using the resulting color vectors to seed the reverse accumulation process.</p>
</section>
</section>
</section>
<section id="linear-solving" class="level2" data-number="8.4">
<h2 data-number="8.4" class="anchored" data-anchor-id="linear-solving"><span class="header-section-number">8.4</span> Linear Solving</h2>
<p>After the Jacobian has been computed, we need to solve a linear equation <span class="math inline">\(Ja=b\)</span>. While mathematically you can solve this by computing the inverse <span class="math inline">\(J^{-1}\)</span>, this is not a good way to perform the calculation because even if <span class="math inline">\(J\)</span> is sparse, then <span class="math inline">\(J^{-1}\)</span> is in general dense and thus may not fit into memory (remember, this is <span class="math inline">\(N^2\)</span> as many terms, where <span class="math inline">\(N\)</span> is the size of the ordinary differential equation that is being solved, so if it’s a large equation it is very feasible and common that the ODE is representable but its full Jacobian is not able to fit into RAM). Note that some may say that this is done for numerical stability reasons: that is incorrect. In fact, under reasonable assumptions for how the inverse is computed, it will be as numerically stable as other techniques we will mention.</p>
<p>Thus instead of generating the inverse, we can instead perform a <em>matrix factorization</em>. A matrix factorization is a transformation of the matrix into a form that is more amenable to certain analyses. For our purposes, a general Jacobian within a Newton iteration can be transformed via the <em>LU-factorization</em> or (<em>LU-decomposition</em>), i.e.</p>
<p><span class="math display">\[J = LU\]</span></p>
<p>where <span class="math inline">\(L\)</span> is lower triangular and <span class="math inline">\(U\)</span> is upper triangular. If we write the linear equation in this form:</p>
<p><span class="math display">\[LUa = b\]</span></p>
<p>then we see that we can solve it by first solving <span class="math inline">\(L(Ua) = b\)</span>. Since <span class="math inline">\(L\)</span> is lower triangular, this is done by the backsubstitution algorithm. That is, in a lower triangular form, we can solve for the first value since we have:</p>
<p><span class="math display">\[L_{11} a_1 = b_1\]</span></p>
<p>and thus by dividing we solve. For the next term, we have that</p>
<p><span class="math display">\[L_{21} a_1 + L_{22} a_2 = b_2\]</span></p>
<p>and thus we plug in the solution to <span class="math inline">\(a_1\)</span> and solve to get <span class="math inline">\(a_2\)</span>. The lower triangular form allows this to continue. This occurs in 1+2+3+…+n operations, and is thus O(n^2). Next, we solve <span class="math inline">\(Ua = b\)</span>, which once again is done by a backsubstitution algorithm but in the reverse direction. Together those two operations are O(n^2) and complete the inversion of <span class="math inline">\(LU\)</span>.</p>
<p>So is this an O(n^2) algorithm for computing the solution of a linear system? No, because the computation of <span class="math inline">\(LU\)</span> itself is an O(n^3) calculation, and thus the true complexity of solving a linear system is still O(n^3). However, if we have already factorized <span class="math inline">\(J\)</span>, then we can repeatedly use the same <span class="math inline">\(LU\)</span> factors to solve additional linear problems <span class="math inline">\(Jv = u\)</span> with different vectors. We can exploit this to accelerate the Newton method. Instead of doing the calculation:</p>
<p><span class="math display">\[x_{k+1} = x_k - J(x_k)^{-1}g(x_k)\]</span></p>
<p>we can instead do:</p>
<p><span class="math display">\[x_{k+1} = x_k - J(x_0)^{-1}g(x_k)\]</span></p>
<p>so that all of the Jacobians are the same. This means that a single O(n^3) factorization can be done, with multiple O(n^2) calculations using the same factorization. This is known as a Quasi-Newton method. While this makes the Newton method no longer quadratically convergent, it minimizes the large constant factor on the computational cost while retaining the same dynamical properties, i.e.&nbsp;the same steady state and thus the same overall solution. This makes sense for sufficiently large <span class="math inline">\(n\)</span>, but requires sufficiently large <span class="math inline">\(n\)</span> because the loss of quadratic convergence means that it will take more steps to converge than before, and thus more <span class="math inline">\(O(n^2)\)</span> backsolves are required, meaning that the difference between factorizations and backsolves needs to be large enough in order to offset the cost of extra steps.</p>
<section id="note-on-sparse-factorization" class="level4" data-number="8.4.0.1">
<h4 data-number="8.4.0.1" class="anchored" data-anchor-id="note-on-sparse-factorization"><span class="header-section-number">8.4.0.1</span> Note on Sparse Factorization</h4>
<p>Note that LU-factorization, and other factorizations, have generalizations to sparse matrices where a <em>symbolic factorization</em> is utilized to compute a sparse storage of the values which then allow for a fast backsubstitution. More details are outside the scope of this course, but note that Julia and MATLAB will both use the library SuiteSparse in the background when <code>lu</code> is called on a sparse matrix.</p>
</section>
</section>
<section id="jacobian-free-newton-krylov-jfnk" class="level2" data-number="8.5">
<h2 data-number="8.5" class="anchored" data-anchor-id="jacobian-free-newton-krylov-jfnk"><span class="header-section-number">8.5</span> Jacobian-Free Newton Krylov (JFNK)</h2>
<p>An alternative method for solving the linear system is the Jacobian-Free Newton Krylov technique. This technique is broken into two pieces: the <em>jvp</em> calculation and the Krylov subspace iterative linear solver.</p>
<section id="jacobian-vector-products-as-directional-derivatives" class="level3" data-number="8.5.1">
<h3 data-number="8.5.1" class="anchored" data-anchor-id="jacobian-vector-products-as-directional-derivatives"><span class="header-section-number">8.5.1</span> Jacobian-Vector Products as Directional Derivatives</h3>
<p>We don’t actually need to compute <span class="math inline">\(J\)</span> itself, since all that we actually need is the <code>v = J*w</code>. Is it possible to compute the <em>Jacobian-Vector Product</em>, or the jvp, without producing the Jacobian?</p>
<p>To see how this is done let’s take a look at what is actually calculated. Written out in the standard basis, we have that:</p>
<p><span class="math display">\[w_i = \sum_{j}^{m} J_{ij} v_{j}\]</span></p>
<p>Now write out what <span class="math inline">\(J\)</span> means and we see that:</p>
<p><span class="math display">\[w_i = \sum_j^{m} \frac{df_i}{dx_j} v_j = \nabla f_i(x) \cdot v\]</span></p>
<p>that is, the <span class="math inline">\(i\)</span>th component of <span class="math inline">\(Jv\)</span> is the directional derivative of <span class="math inline">\(f_i\)</span> in the direction <span class="math inline">\(v\)</span>. This means that in general, the jvp <span class="math inline">\(Jv\)</span> is actually just the directional derivative in the direction of <span class="math inline">\(v\)</span>, that is:</p>
<p><span class="math inline">\(Jv = \nabla f \cdot v\)</span></p>
<p>and therefore it has another mathematical representation, that is:</p>
<p><span class="math display">\[Jv = \lim_{\epsilon \rightarrow 0} \frac{f(x+v \epsilon) - f(x)}{\epsilon}\]</span></p>
<p>From this alternative form it is clear that <strong>we can always compute a jvp with a single computation</strong>. Using finite differences, a simple approximation is the following:</p>
<p><span class="math display">\[Jv \approx \frac{f(x+v \epsilon) - f(x)}{\epsilon}\]</span></p>
<p>for non-zero <span class="math inline">\(\epsilon\)</span>. Similarly, recall that in forward-mode automatic differentiation we can choose directions by seeding the dual part. Therefore, using the dual number with one partial component:</p>
<p><span class="math display">\[d = x + v \epsilon\]</span></p>
<p>we get that</p>
<p><span class="math display">\[f(d) = f(x) + Jv \epsilon\]</span></p>
<p>and thus a single application with a single partial gives the jvp.</p>
<section id="note-on-reverse-mode-automatic-differentiation" class="level4" data-number="8.5.1.1">
<h4 data-number="8.5.1.1" class="anchored" data-anchor-id="note-on-reverse-mode-automatic-differentiation"><span class="header-section-number">8.5.1.1</span> Note on Reverse-Mode Automatic Differentiation</h4>
<p>As noted earlier, reverse-mode automatic differentiation has its primitives compute rows of the Jacobian in the seeded direction. This means that the seeded reverse-mode call with the vector <span class="math inline">\(v\)</span> computes <span class="math inline">\(v^T J\)</span>, that is the <em>vector (transpose) Jacobian transpose</em>, or <em>vjp</em> for short. When discussing parameter estimation and adjoints, this shorthand will be introduced as a way for using a traditionally machine learning tool to accelerate traditionally scientific computing tasks.</p>
</section>
</section>
<section id="krylov-subspace-methods-for-solving-linear-systems" class="level3" data-number="8.5.2">
<h3 data-number="8.5.2" class="anchored" data-anchor-id="krylov-subspace-methods-for-solving-linear-systems"><span class="header-section-number">8.5.2</span> Krylov Subspace Methods For Solving Linear Systems</h3>
<section id="basic-iterative-solver-methods" class="level4" data-number="8.5.2.1">
<h4 data-number="8.5.2.1" class="anchored" data-anchor-id="basic-iterative-solver-methods"><span class="header-section-number">8.5.2.1</span> Basic Iterative Solver Methods</h4>
<p>Now that we have direct access to quick calculations of <span class="math inline">\(Jv\)</span>, how would we use this to solve the linear system <span class="math inline">\(Jw = v\)</span> quickly? This is done through <em>iterative linear solvers</em>. These methods replace the process of solving for a factorization with, you may have guessed it, a discrete dynamical system whose solution is <span class="math inline">\(w\)</span>. To do this, what we want is some iterative process so that</p>
<p><span class="math display">\[Jw - b = 0\]</span></p>
<p>So now let’s split <span class="math inline">\(J = A - B\)</span>, then if we are iterating the vectors <span class="math inline">\(w_k\)</span> such that <span class="math inline">\(w_k \rightarrow w\)</span>, then if we plug this into the previous (residual) equation we get</p>
<p><span class="math display">\[A w_{k+1} = Bw_k + b\]</span></p>
<p>since when we plug in <span class="math inline">\(w\)</span> we get zero (the sequence must be Cauchy so the difference <span class="math inline">\(w_{k+1} - w_k \rightarrow 0\)</span>). Thus if we can split our matrix <span class="math inline">\(J\)</span> into a component <span class="math inline">\(A\)</span> which is easy to invert and a part <span class="math inline">\(B\)</span> that is just everything else, then we would have a bunch of easy linear systems to solve. There are many different choices that we can do. If we let <span class="math inline">\(J = L + D + U\)</span>, where <span class="math inline">\(L\)</span> is the lower portion of <span class="math inline">\(J\)</span>, <span class="math inline">\(D\)</span> is the diagonal, and <span class="math inline">\(U\)</span> is the upper portion, then the following are well-known methods:</p>
<ul>
<li>Richardson: <span class="math inline">\(A = \omega I\)</span> for some <span class="math inline">\(\omega\)</span></li>
<li>Jacobi: <span class="math inline">\(A = D\)</span></li>
<li>Damped Jacobi: <span class="math inline">\(A = \omega D\)</span></li>
<li>Gauss-Seidel: <span class="math inline">\(A = D-L\)</span></li>
<li>Successive Over Relaxation: <span class="math inline">\(A = \omega D - L\)</span></li>
<li>Symmetric Successive Over Relaxation: <span class="math inline">\(A = \frac{1}{\omega (2 - \omega)}(D-\omega L)D^{-1}(D-\omega U)\)</span></li>
</ul>
<p>These decompositions are chosen since a diagonal matrix is easy to invert (it’s just the inversion of the scalars of the diagonal) and it’s easy to solve an upper or lower triangular linear system (once again, it’s backsubstitution).</p>
<p>Since these methods give a a linear dynamical system, we know that there is a unique steady state solution, which happens to be <span class="math inline">\(Aw - Bw = Jw = b\)</span>. Thus we will converge to it as long as the steady state is stable. To see if it’s stable, take the update equation</p>
<p><span class="math display">\[w_{k+1} = A^{-1}(Bw_k + b)\]</span></p>
<p>and check the eigenvalues of the system: if they are within the unit circle then you have stability. Notice that this can always occur by bringing the eigenvalues of <span class="math inline">\(A^{-1}\)</span> closer to zero, which can be done by multiplying <span class="math inline">\(A\)</span> by a significantly large value, hence the <span class="math inline">\(\omega\)</span> quantities. While that always works, this essentially amounts to decreasing the stepsize of the iterative process and thus requiring more steps, thus making it take more computations. Thus the game is to pick the largest stepsize (<span class="math inline">\(\omega\)</span>) for which the steady state is stable. We will leave that as outside the topic of this course.</p>
</section>
<section id="krylov-subspace-methods" class="level4" data-number="8.5.2.2">
<h4 data-number="8.5.2.2" class="anchored" data-anchor-id="krylov-subspace-methods"><span class="header-section-number">8.5.2.2</span> Krylov Subspace Methods</h4>
<p>While the classical iterative solver methods give the background for understanding an alternative to direct inversion or factorization of a matrix, the problem with that approach is that it requires the ability to split the matrix <span class="math inline">\(J\)</span>, which we would like to avoid computing. Instead, we would like to develop an iterative solver technique which instead just uses the solution to <span class="math inline">\(Jv\)</span>. Indeed there are such methods, and these are the Krylov subspace methods. A Krylov subspace is the space spanned by:</p>
<p><span class="math display">\[\mathcal{K}_k = \text{span} \{v,Jv,J^2 v, \ldots, J^k v\}\]</span></p>
<p>There are a few nice properties about Krylov subspaces that can be exploited. For one, it is known that there is a finite maximum dimension of the Krylov subspace, that is there is a value <span class="math inline">\(r\)</span> such that <span class="math inline">\(J^{r+1} v \in \mathcal{K}_r\)</span>, which means that the complete Krylov subspace can be computed in finitely many jvp, since <span class="math inline">\(J^2 v\)</span> is just the jvp where the vector is the jvp. Indeed, one can show that <span class="math inline">\(J^i v\)</span> is linearly independent for each <span class="math inline">\(i\)</span>, and thus that maximal value is <span class="math inline">\(m\)</span>, the dimension of the Jacobian. Therefore in <span class="math inline">\(m\)</span> jvps the solution is guaranteed to live in the Krylov subspace, giving a maximal computational cost and a proof of convergence if the vector in there is the “optimal in the space”.</p>
<p>The most common method in the Krylov subspace family of methods is the GMRES method. Essentially, in step <span class="math inline">\(i\)</span> one computes <span class="math inline">\(\mathcal{K}_i\)</span>, and finds the <span class="math inline">\(x\)</span> that is the closest to the Krylov subspace, i.e.&nbsp;finds the <span class="math inline">\(x \in \mathcal{K}_i\)</span> such that <span class="math inline">\(\Vert Jx-v \Vert\)</span> is minimized. At each step, it adds the new vector to the Krylov subspace after orthgonalizing it against the other vectors via Arnoldi iterations, leading to an orthogonal basis of <span class="math inline">\(\mathcal{K}_i\)</span> which makes it easy to express <span class="math inline">\(x\)</span>.</p>
<p>While one has a guaranteed bound on the number of possible jvps in GMRES which is simply the number of ODEs (since that is what determines the size of the Jacobian and thus the total dimension of the problem), that bound is not necessarily a good one. For a large sparse matrix, it may be computationally impractical to ever compute 100,000 jvps. Thus one does not typically run the algorithm to conclusion, and instead stops when <span class="math inline">\(\Vert Jx-v \Vert\)</span> is sufficiently below some user-defined error tolerance.</p>
</section>
</section>
</section>
<section id="intermediate-conclusion" class="level2" data-number="8.6">
<h2 data-number="8.6" class="anchored" data-anchor-id="intermediate-conclusion"><span class="header-section-number">8.6</span> Intermediate Conclusion</h2>
<p>Let’s take a step back and see what our intermediate conclusion is. In order to solve for the implicit step, it just boils down to doing Newton’s method on some <span class="math inline">\(g(x)=0\)</span>. If the Jacobian is small enough, one factorizes the Jacobian and uses Quasi-Newton iterations in order to utilize the stored LU-decomposition in multiple steps to reduce the computation cost. If the Jacobian is sparse, sparse automatic differentiation through matrix coloring is employed to directly fill the sparse matrix with less applications of <span class="math inline">\(g\)</span>, and then this sparse matrix is factorized using a sparse LU factorization.</p>
<p>When the matrix is too large, then one resorts to using a Krylov subspace method, since this only requires being able to do <span class="math inline">\(Jv\)</span> calculations. In general, <span class="math inline">\(Jv\)</span> can be done matrix-free because it is simply the directional derivative in the direction of the vector <span class="math inline">\(v\)</span>, which can be computed through either numerical or forward-mode automatic differentiation. This is then used in the GMRES iterative process to find the solution in the Krylov subspace which is closest to the solution, exiting early when the residual error is small enough. If this is converging too slow, then preconditioning is used.</p>
<p>That’s the basic algorithm, but what are the other important details for getting this right?</p>
</section>
<section id="the-need-for-speed" class="level2" data-number="8.7">
<h2 data-number="8.7" class="anchored" data-anchor-id="the-need-for-speed"><span class="header-section-number">8.7</span> The Need for Speed</h2>
<section id="preconditioning" class="level3" data-number="8.7.1">
<h3 data-number="8.7.1" class="anchored" data-anchor-id="preconditioning"><span class="header-section-number">8.7.1</span> Preconditioning</h3>
<p>However, the speed at GMRES convergences is dependent on the correlations between the vectors, which can be shown to be related to the condition number of the Jacobian matrix. A high condition number makes convergence slower (this is the case for the traditional iterative methods as well), which in turn is an issue because it is the high condition number on the Jacobian which leads to stiffness and causes one to have to use an implicit integrator in the first place!</p>
<p>To help speed up the convergence, a common technique is known as <em>preconditioning</em>. Preconditioning is the process of using a semi-inverse to the matrix in order to split the matrix so that the iterative problem that is being solved is one that has a smaller condition number. Mathematically, it involves decomposing <span class="math inline">\(J = P_l A P_r\)</span> where <span class="math inline">\(P_l\)</span> and <span class="math inline">\(P_r\)</span> are the left and right preconditioners which have simple inverses, and thus instead of solving <span class="math inline">\(Jx=v\)</span>, we would solve:</p>
<p><span class="math display">\[P_l A P_r x = v\]</span></p>
<p>or</p>
<p><span class="math display">\[A P_r x = P_l^{-1}v\]</span></p>
<p>which then means that the Krylov subpace that needs to be solved for is that defined by <span class="math inline">\(A\)</span>: <span class="math inline">\(\mathcal{K} = \text{span}\{v,Av,A^2 v, \ldots\}\)</span>. There are many possible choices for these preconditioners, but they are usually problem dependent. For example, for ODEs which come from parabolic and elliptic PDE discretizations, the <em>multigrid method</em>, such as a geometric multigrid or an algebraic multigrid, is a preconditioner that can accelerate the iterative solving process. One generic preconditioner that can generally be used is to divide by the norm of the vector <span class="math inline">\(v\)</span>, which is a scaling employed by both SUNDIALS CVODE and by DifferentialEquations.jl and can be shown to be almost always advantageous.</p>
</section>
<section id="jacobian-re-use" class="level3" data-number="8.7.2">
<h3 data-number="8.7.2" class="anchored" data-anchor-id="jacobian-re-use"><span class="header-section-number">8.7.2</span> Jacobian Re-use</h3>
<p>If the problem is small enough such that the factorization is used and a Quasi-Newton technique is employed, it then holds that for most steps <span class="math inline">\(J\)</span> is only approximate since it can be using an old LU-factorization. To push it even further, high performance codes allow for <em>jacobian reuse</em>, which is allowing the same Jacobian to be reused between different timesteps. If the Jacobian is too incorrect, it can cause the Newton iterations to diverge, which is then when one would calculate a new Jacobian and compute a new LU-factorization.</p>
</section>
<section id="adaptive-timestepping" class="level3" data-number="8.7.3">
<h3 data-number="8.7.3" class="anchored" data-anchor-id="adaptive-timestepping"><span class="header-section-number">8.7.3</span> Adaptive Timestepping</h3>
<p>In simple cases, like partial differential equation discretizations of physical problems, the resulting ODEs are not too stiff and thus Newton’s iteration generally works. However, in cases like stiff biological models, Newton’s iteration can itself not always be stable enough to allow convergence. In fact, with many of the stiff biological models commonly used in benchmarks, no method is stable enough to pass without using adaptive timestepping! Thus one may need to adapt the timestep in order to improve the ability for the Newton method to converge (smaller timesteps increase the stability of the Newton stepping, see the homework).</p>
<p>This needs to be mixed with the Jacobian re-use strategy, since <span class="math inline">\(J = I - \gamma \frac{df}{du}\)</span> where <span class="math inline">\(\gamma\)</span> is dependent on <span class="math inline">\(\Delta t\)</span> (and <span class="math inline">\(\gamma = \Delta t\)</span> for implicit Euler) means that the Jacobian of the Newton method changes as <span class="math inline">\(\Delta t\)</span> changes. Thus one usually has a tiered algorithm for determining when to update the factorizations of <span class="math inline">\(J\)</span> vs when to compute a new <span class="math inline">\(\frac{df}{du}\)</span> and then refactorize. This is generally dependent on estimates of convergence rates to heuristically guess how far off <span class="math inline">\(\frac{df}{du}\)</span> is from the current true value.</p>
<p>So how does one perform adaptivity? This is generally done through a rejection sampling technique. First one needs some estimate of the error in a step. This is calculated through an <em>embedded method</em>, which is a method that is able to be calculated without any extra <span class="math inline">\(f\)</span> evaluations that is (usually) one order different from the true method. The difference between the true and the embedded method is then an error estimate. If this is greater than a user chosen tolerance, the step is rejected and re-ran with a smaller <span class="math inline">\(\Delta t\)</span> (possibly refactorizing, etc.). If this is less than the user tolerance, the step is accepted and <span class="math inline">\(\Delta t\)</span> is changed.</p>
<p>There are many schemes for how one can change <span class="math inline">\(\Delta t\)</span>. One of the most common is known as the <em>P-control</em>, which stands for the proportional controller which is used throughout control theory. In this case, the control is to change <span class="math inline">\(\Delta t\)</span> in proportion to the current error ratio from the desired tolerance. If we let</p>
<p><span class="math display">\[q = \frac{\text{E}}{\max(u_k,u_{k+1}) \tau_r + \tau_a}\]</span></p>
<p>where <span class="math inline">\(\tau_r\)</span> is the relative tolerance and <span class="math inline">\(\tau_a\)</span> is the absolute tolerance, then <span class="math inline">\(q\)</span> is the ratio of the current error to the current tolerance. If <span class="math inline">\(q&lt;1\)</span>, then the error is less than the tolerance and the step is accepted, and vice versa for <span class="math inline">\(q&gt;1\)</span>. In either case, we let <span class="math inline">\(\Delta t_{new} = q \Delta t\)</span> be the proportional update.</p>
<p>However, proportional error control has many known features that are undesirable. For example, it happens to work in a “bang bang” manner, meaning that it can drastically change its behavior from step to step. One step may multiply the step size by 10x, then the next by 2x. This is an issue because it effects the stability of the ODE solver method (since the stability is not a property of a single step, but rather it’s a property of the global behavior over time)! Thus to smooth it out, one can use a <em>PI-control</em>, which modifies the control factor by a history value, i.e.&nbsp;the error in one step in the past. This of course also means that one can utilize a PID-controller for time stepping. And there are many other techniques that can be used, but many of the most optimized codes tend to use a PI-control mechanism.</p>
</section>
</section>
<section id="methodological-summary" class="level2" data-number="8.8">
<h2 data-number="8.8" class="anchored" data-anchor-id="methodological-summary"><span class="header-section-number">8.8</span> Methodological Summary</h2>
<p>Here’s a quick summary of the methodologies in a hierarchical sense:</p>
<ul>
<li>At the lowest level is the linear solve, either done by JFNK or (sparse) factorization. For large enough systems, this is the brunt of the work. This is thus the piece to computationally optimize as much as possible, and parallelize. For sparse factorizations, this can be done with a distributed sparse library implementation. For JFNK, the efficiency is simply due to the efficiency of your ODE function <code>f</code>.</li>
<li>An optional level for JFNK is the preconditioning level, where preconditioners can be used to decrease the total number of iterations required for Krylov subspace methods like GMRES to converge, and thus reduce the total number of <code>f</code> calls.</li>
<li>At the nonlinear solver level, different Newton-like techniques are utilized to minimize the number of factorizations/linear solves required, and maximize the stability of the Newton method.</li>
<li>At the ODE solver level, more efficient integrators and adaptive methods for stiff ODEs are used to reduce the cost by affecting the linear solves. Most of these calculations are dominated by the linear solve portion when it’s in the regime of large stiff systems. Jacobian reuse techniques, partial factorizations, and IMEX methods come into play as ways to reduce the cost per factorization and reduce the total number of factorizations.</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./automatic_differentiation.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Forward-Mode Automatic Differentiation (AD) via High Dimensional Algebras</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./estimation_identification.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Basic Parameter Estimation, Reverse-Mode AD, and Inverse Problems</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>