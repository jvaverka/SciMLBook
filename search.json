[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MIT Parallel Computing and Scientific Machine Learning",
    "section": "",
    "text": "Preface\nThis book is a compilation of lecture notes from the MIT Course 18.337J/6.338J: Parallel Computing and Scientific Machine Learning. It is meant to be a live document, updating to continuously add the latest details on methods from the field of scientific machine learning and the latest techniques for high-performance computing.\nAll original content was created by Dr. Chris Rackauckas with assistance from David Sanders. Contributors from JuliaHub & MIT continue to make revisions in the hopes to make the SciML ecosystem accessible to all.\nExamples and applications will be shown using the Julia programming langauge.\n\n\n\n\n\n\nImportant\n\n\n\nThis is not the official site for MIT course 18.337J/6.338J and may not contain most up-to-date content. Click here to visit the official course site.\nThis is site serves simply as an alternative viewing option for the course material. Hope you enjoy."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction to Parallel Computing",
    "section": "",
    "text": "There are two main branches of technical computing: machine learning and scientific computing. Machine learning has received a lot of hype over the last decade, with techniques such as convolutional neural networks and TSne nonlinear dimensional reductions powering a new generation of data-driven analytics. On the other hand, many scientific disciplines carry on with large-scale modeling through differential equation modeling, looking at stochastic differential equations and partial differential equations describing scientific laws.\nHowever, there has been a recent convergence of the two disciplines. This field, scientific machine learning, has been showcasing results like how partial differential equation simulations can be accelerated with neural networks. New methods, such as probabilistic and differentiable programming, have started to be developed specifically for enhancing the tools of this domain. However, the techniques in this field combine two huge areas of computational and numerical practice, meaning that the methods are sufficiently complex. How do you backpropagate an ODE defined by neural networks? How do you perform unsupervised learning of a scientific simulator?\nIn this class we will dig into the methods and understand what they do, why they were made, and thus how to integrate numerical methods across fields to accentuate their pros while mitigating their cons. This class will be a survey of the numerical techniques, showcasing how many disciplines are doing the same thing under different names, and using a common mathematical language to derive efficient routines which capture both data-driven and mechanistic-based modeling.\nHowever, these methods will quickly run into a scaling issue if naively coded. To handle this problem, everything will have a focus on performance-engineering. We will start by focusing on algorithm which are inherently serial and learn to optimize serial code. Then we will showcase how logic-heavy code can be parallelized through multithreading and distributed computing techniques like MPI, while direct mathematical descriptions can be parallelized through GPU computing.\nThe final part of the course will be a unique project which pulls together these techniques. As a new field, the students will be exposed to the “low hanging fruit” and will be directed towards an area which they can make a quick impact. For the final project, students will team up to solve a new problem in the field of scientific machine learning, and receive helping writing up a publication-quality analysis about their work."
  },
  {
    "objectID": "optimize.html",
    "href": "optimize.html",
    "title": "1  Optimizing Serial Code",
    "section": "",
    "text": "2 Discussion Questions\nHere’s a few discussion questions to think about performance engineering in scientific tasks:"
  },
  {
    "objectID": "optimize.html#mental-model-of-a-memory",
    "href": "optimize.html#mental-model-of-a-memory",
    "title": "1  Optimizing Serial Code",
    "section": "1.1 Mental Model of a Memory",
    "text": "1.1 Mental Model of a Memory\nTo start optimizing code you need a good mental model of a computer.\n\n1.1.1 High Level View\nAt the highest level you have a CPU’s core memory which directly accesses a L1 cache. The L1 cache has the fastest access, so things which will be needed soon are kept there. However, it is filled from the L2 cache, which itself is filled from the L3 cache, which is filled from the main memory. This bring us to the first idea in optimizing code: using things that are already in a closer cache can help the code run faster because it doesn’t have to be queried for and moved up this chain.\n\nWhen something needs to be pulled directly from main memory this is known as a cache miss. To understand the cost of a cache miss vs standard calculations, take a look at this classic chart.\n(Cache-aware and cache-oblivious algorithms are methods which change their indexing structure to optimize their use of the cache lines. We will return to this when talking about performance of linear algebra.)\n\n\n1.1.2 Cache Lines and Row/Column-Major\nMany algorithms in numerical linear algebra are designed to minimize cache misses. Because of this chain, many modern CPUs try to guess what you will want next in your cache. When dealing with arrays, it will speculate ahead and grab what is known as a cache line: the next chunk in the array. Thus, your algorithms will be faster if you iterate along the values that it is grabbing.\nThe values that it grabs are the next values in the contiguous order of the stored array. There are two common conventions: row major and column major. Row major means that the linear array of memory is formed by stacking the rows one after another, while column major puts the column vectors one after another.\n\nJulia, MATLAB, and Fortran are column major. Python’s numpy is row-major.\n\nA = rand(100, 100)\nB = rand(100, 100)\nC = rand(100, 100)\nusing BenchmarkTools\nfunction inner_rows!(C, A, B)\n  for i in 1:100, j in 1:100\n    C[i, j] = A[i, j] + B[i, j]\n  end\nend\n@btime inner_rows!(C, A, B)\n\n  8.878 μs (0 allocations: 0 bytes)\n\n\n\nfunction inner_cols!(C, A, B)\n  for j in 1:100, i in 1:100\n    C[i, j] = A[i, j] + B[i, j]\n  end\nend\n@btime inner_cols!(C, A, B)\n\n  5.703 μs (0 allocations: 0 bytes)\n\n\n\n\n1.1.3 Lower Level View: The Stack and the Heap\nLocally, the stack is composed of a stack and a heap. The stack requires a static allocation: it is ordered. Because it’s ordered, it is very clear where things are in the stack, and therefore accesses are very quick (think instantaneous). However, because this is static, it requires that the size of the variables is known at compile time (to determine all of the variable locations). Since that is not possible with all variables, there exists the heap. The heap is essentially a stack of pointers to objects in memory. When heap variables are needed, their values are pulled up the cache chain and accessed.\n \n\n\n1.1.4 Heap Allocations and Speed\nHeap allocations are costly because they involve this pointer indirection, so stack allocation should be done when sensible (it’s not helpful for really large arrays, but for small values like scalars it’s essential!)\n\nfunction inner_alloc!(C, A, B)\n  for j in 1:100, i in 1:100\n    val = [A[i, j] + B[i, j]]\n    C[i, j] = val[1]\n  end\nend\n@btime inner_alloc!(C, A, B)\n\n  220.423 μs (10000 allocations: 625.00 KiB)\n\n\n\nfunction inner_noalloc!(C, A, B)\n  for j in 1:100, i in 1:100\n    val = A[i, j] + B[i, j]\n    C[i, j] = val[1]\n  end\nend\n@btime inner_noalloc!(C, A, B)\n\n  8.064 μs (0 allocations: 0 bytes)\n\n\nWhy does the array here get heap-allocated? It isn’t able to prove/guarantee at compile-time that the array’s size will always be a given value, and thus it allocates it to the heap. @btime tells us this allocation occurred and shows us the total heap memory that was taken. Meanwhile, the size of a Float64 number is known at compile-time (64-bits), and so this is stored onto the stack and given a specific location that the compiler will be able to directly address.\nNote that one can use the StaticArrays.jl library to get statically-sized arrays and thus arrays which are stack-allocated:\n\nusing StaticArrays\nfunction static_inner_alloc!(C, A, B)\n  for j in 1:100, i in 1:100\n    val = @SVector [A[i, j] + B[i, j]]\n    C[i, j] = val[1]\n  end\nend\n@btime static_inner_alloc!(C, A, B)\n\n  5.710 μs (0 allocations: 0 bytes)\n\n\n\n\n1.1.5 Mutation to Avoid Heap Allocations\nMany times you do need to write into an array, so how can you write into an array without performing a heap allocation? The answer is mutation. Mutation is changing the values of an already existing array. In that case, no free memory has to be found to put the array (and no memory has to be freed by the garbage collector).\nIn Julia, functions which mutate the first value are conventionally noted by a !. See the difference between these two equivalent functions:\n\nfunction inner_noalloc!(C, A, B)\n  for j in 1:100, i in 1:100\n    val = A[i, j] + B[i, j]\n    C[i, j] = val[1]\n  end\nend\n@btime inner_noalloc!(C, A, B)\n\n  7.863 μs (0 allocations: 0 bytes)\n\n\n\nfunction inner_alloc(A, B)\n  C = similar(A)\n  for j in 1:100, i in 1:100\n    val = A[i, j] + B[i, j]\n    C[i, j] = val[1]\n  end\nend\n@btime inner_alloc(A, B)\n\n  8.782 μs (2 allocations: 78.17 KiB)\n\n\nTo use this algorithm effectively, the ! algorithm assumes that the caller already has allocated the output array to put as the output argument. If that is not true, then one would need to manually allocate. The goal of that interface is to give the caller control over the allocations to allow them to manually reduce the total number of heap allocations and thus increase the speed.\n\n\n1.1.6 Julia’s Broadcasting Mechanism\nWouldn’t it be nice to not have to write the loop there? In many high level languages this is simply called vectorization. In Julia, we will call it array vectorization to distinguish it from the SIMD vectorization which is common in lower level languages like C, Fortran, and Julia.\nIn Julia, if you use . on an operator it will transform it to the broadcasted form. Broadcast is lazy: it will build up an entire .’d expression and then call broadcast! on composed expression. This is customizable and documented in detail. However, to a first approximation we can think of the broadcast mechanism as a mechanism for building fused expressions.\n\nJuliaUnder the HoodR\n\n\nHigh-level code a Julia user would write.\n\nA .+ B .+ C\n\n\n\nThe high-level code lowers to something like:\n\nmap((a, b, c) -&gt; a + b + c, A, B, C)\n\nmap is a mechanism to apply some function to each element of a collection. This general pattern is how to apply some_func elemement-wise to all values of some_collection.\n\nmap((x) -&gt; some_func, some_collection)\n\n\n\nSome form of this pattern is supported in most programming languages. For example, in R the apply, lapply, or sapply may be used depending on the desired return type.\nsapply( some_collection, some_func )\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nTake a quick second to think about why loop fusion may be an optimization.\n\n\nThis about what would happen if you did not fuse the operations. We can write that out as:\n\ntmp = A .+ B\ntmp .+ C;\n\nNotice that if we did not fuse the expressions, we would need some place to put the result of A .+ B, and that would have to be an array, which means it would cause a heap allocation. Thus broadcast fusion eliminates the temporary variable (colloquially called just a temporary).\n\nfunction unfused(A,  B, C)\n  tmp = A .+ B\n  tmp .+ C\nend\n@btime unfused(A, B, C);\n\n  11.294 μs (4 allocations: 156.34 KiB)\n\n\n\nfused(A, B, C) = A .+ B .+ C\n@btime fused(A, B, C);\n\n  4.873 μs (2 allocations: 78.17 KiB)\n\n\nNote that we can also fuse the output by using .=. This is essentially the vectorized version of a ! function:\n\nD = similar(A)\nfused!(D, A, B, C) = (D .= A .+ B .+ C)\n@btime fused!(D, A, B, C);\n\n  3.843 μs (0 allocations: 0 bytes)\n\n\n\n\n1.1.7 Note on Broadcasting Function Calls\nJulia allows for broadcasting the call () operator as well. .() will call the function element-wise on all arguments, so sin.(A) will be the elementwise sine function. This will fuse Julia like the other operators.\n\n\n1.1.8 Note on Vectorization and Speed\nIn articles on MATLAB, Python, R, etc., this is where you will be told to vectorize your code. Notice from above that this isn’t a performance difference between writing loops and using vectorized broadcasts. This is not abnormal! The reason why you are told to vectorize code in these other languages is because they have a high per-operation overhead (which will be discussed further down). This means that every call, like +, is costly in these languages. To get around this issue and make the language usable, someone wrote and compiled the loop for the C/Fortran function that does the broadcasted form (see numpy’s Github repo). Thus A .+ B’s MATLAB/Python/R equivalents are calling a single C function to generally avoid the cost of function calls and thus are faster.\nBut this is not an intrinsic property of vectorization. Vectorization isn’t “fast” in these languages, it’s just close to the correct speed. The reason vectorization is recommended is because looping is slow in these languages. Because looping isn’t slow in Julia (or C, C++, Fortran, etc.), loops and vectorization generally have the same speed. So use the one that works best for your code without a care about performance.\n(As a small side effect, these high level languages tend to allocate a lot of temporary variables since the individual C kernels are written for specific numbers of inputs and thus don’t naturally fuse. Julia’s broadcast mechanism is just generating and JIT compiling Julia functions on the fly, and thus it can accommodate the combinatorial explosion in the amount of choices just by only compiling the combinations that are necessary for a specific code)\n\n\n1.1.9 Heap Allocations from Slicing\nIt’s important to note that slices in Julia produce copies instead of views. Thus for example:\n\nA[50, 50]\n\n0.8678702655632916\n\n\nallocates a new output. This is for safety, since if it pointed to the same array then writing to it would change the original array. We can demonstrate this by asking for a view instead of a copy.\n\n@show A[1]\nE = @view A[1:5, 1:5]\nE[1] = 2.0\n@show A[1]\n\nA[1] = 0.23509670883450928\nA[1] = 2.0\n\n\n2.0\n\n\nHowever, this means that @view A[1:5, 1:5] did not allocate an array (it does allocate a pointer if the escape analysis is unable to prove that it can be elided. This means that in small loops there will be no allocation, while if the view is returned from a function for example it will allocate the pointer, ~80 bytes, but not the memory of the array. This means that it is O(1) in cost but with a relatively small constant).\n\n\n1.1.10 Asymptotic Cost of Heap Allocations\nHeap allocations have to locate and prepare a space in RAM that is proportional to the amount of memory that is calculated, which means that the cost of a heap allocation for an array is O(n), with a large constant. As RAM begins to fill up, this cost dramatically increases. If you run out of RAM, your computer may begin to use swap, which is essentially RAM simulated on your hard drive. Generally when you hit swap your performance is so dead that you may think that your computation froze, but if you check your resource use you will notice that it’s actually just filled the RAM and starting to use the swap.\nBut think of it as O(n) with a large constant factor. This means that for operations which only touch the data once, heap allocations can dominate the computational cost:\n\nusing LinearAlgebra, BenchmarkTools\nfunction alloc_timer(n)\n    A = rand(n, n)\n    B = rand(n, n)\n    C = rand(n, n)\n    t1 = @belapsed $A .* $B\n    t2 = @belapsed ($C .= $A .* $B)\n    t1, t2\nend\nns = 2 .^ (2:11)\nres = [alloc_timer(n) for n in ns]\nalloc   = [x[1] for x in res]\nnoalloc = [x[2] for x in res]\n\nusing Plots\nplot(ns, alloc;\n     label = \"=\",\n     xscale = :log10,\n     yscale = :log10,\n     legend = :bottomright,\n     title = \"Micro-optimizations matter for BLAS1\")\nplot!(ns, noalloc; label = \".=\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHowever, when the computation takes O(n^3), like in matrix multiplications, the high constant factor only comes into play when the matrices are sufficiently small:\n\nusing LinearAlgebra, BenchmarkTools\nfunction alloc_timer(n)\n    A = rand(n, n)\n    B = rand(n, n)\n    C = rand(n, n)\n    t1 = @belapsed $A*$B\n    t2 = @belapsed mul!($C,$A,$B)\n    t1,t2\nend\nns = 2 .^ (2:7)\nres = [alloc_timer(n) for n in ns]\nalloc   = [x[1] for x in res]\nnoalloc = [x[2] for x in res]\n\nusing Plots\nplot(ns, alloc;\n     label = \"*\",\n     xscale = :log10,\n     yscale = :log10,\n     legend = :bottomright,\n     title = \"Micro-optimizations only matter for small matmuls\")\nplot!(ns, noalloc; label = \"mul!\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThough using a mutating form is never bad and always is a little bit better.\n\n\n1.1.11 Optimizing Memory Use Summary\n\nAvoid cache misses by reusing values\nIterate along columns\nAvoid heap allocations in inner loops\nHeap allocations occur when the size of things is not proven at compile-time\nUse fused broadcasts (with mutated outputs) to avoid heap allocations\nArray vectorization confers no special benefit in Julia because Julia loops are as fast as C or Fortran\nUse views instead of slices when applicable\nAvoiding heap allocations is most necessary for O(n) algorithms or algorithms with small arrays\nUse StaticArrays.jl to avoid heap allocations of small arrays in inner loops"
  },
  {
    "objectID": "optimize.html#julias-type-inference-and-the-compiler",
    "href": "optimize.html#julias-type-inference-and-the-compiler",
    "title": "1  Optimizing Serial Code",
    "section": "1.2 Julia’s Type Inference and the Compiler",
    "text": "1.2 Julia’s Type Inference and the Compiler\nMany people think Julia is fast because it is JIT compiled. That is simply not true (we’ve already shown examples where Julia code isn’t fast, but it’s always JIT compiled!). Instead, the reason why Julia is fast is because the combination of two ideas:\n\nType inference\nType specialization in functions\n\nThese two features naturally give rise to Julia’s core design feature: multiple dispatch. Let’s break down these pieces.\n\n1.2.1 Type Inference\nAt the core level of the computer, everything has a type. Some languages are more explicit about said types, while others try to hide the types from the user. A type tells the compiler how to to store and interpret the memory of a value. For example, if the compiled code knows that the value in the register is supposed to be interpreted as a 64-bit floating point number, then it understands that slab of memory like:\n\nImportantly, it will know what to do for function calls. If the code tells it to add two floating point numbers, it will send them as inputs to the Floating Point Unit (FPU) which will give the output.\nIf the types are not known, then… ? So one cannot actually compute until the types are known, since otherwise it’s impossible to interpret the memory.\n\nCompiledInterpretedHybrid\n\n\nIn languages like C, the programmer has to declare the types of variables in the program:\nvoid add(double *a, double *b, double *c, size_t n){\n  size_t i;\n  for(i = 0; i &lt; n; ++i) {\n    c[i] = a[i] + b[i];\n  }\n}\nThe types are known at compile time because the programmer set it in stone.\n\n\nIn many interpreted languages like Python, types are checked at runtime. For example,\na = 2\nb = 4\na + b\nwhen the addition occurs, the Python interpreter will check the object holding the values and ask it for its types, and use those types to know how to compute the + function. For this reason, the add function in Python is rather complex since it needs to decode and have a version for all primitive types!\nNot only is there runtime overhead checks in function calls due to to not being explicit about types, there is also a memory overhead since it is impossible to know how much memory a value with take since that’s a property of its type. Thus the Python interpreter cannot statically guarantee exact unchanging values for the size that a value would take in the stack, meaning that the variables are not stack-allocated. This means that every number ends up heap-allocated, which hopefully begins to explain why this is not as fast as C.\n\n\nThe solution is Julia is somewhat of a hybrid. The Julia code looks like:\n\na = 2\nb = 4\na + b\n\n6\n\n\nHowever, before JIT compilation, Julia runs a type inference algorithm which finds out that A is an Int, and B is an Int. You can then understand that if it can prove that A + B is an Int, then it can propagate all of the types through.\n\n\n\n\n\n1.2.2 Type Specialization in Functions\nJulia is able to propagate type inference through functions because, even if a function is “untyped”, Julia will interpret this as a generic function over possible methods, where every method has a concrete type. This means that in Julia, the function:\n\nf(x, y) = x + y\n\nf (generic function with 1 method)\n\n\nis not what you may think of as a “single function”, since given inputs of different types it will actually be a different function. We can see this by examining the LLVM IR (LLVM is Julia’s compiler, the IR is the Intermediate Representation, i.e. a platform-independent representation of assembly that lives in LLVM that it knows how to convert into assembly per architecture):\n\nusing InteractiveUtils\n@code_llvm f(2, 5)\n\n;  @ In[18]:1 within `f`\ndefine i64 @julia_f_2304(i64 signext %0\n\n\n, i64 signext %1) #0 {\ntop:\n; ┌ @ int.jl:87 within `+`\n   %2 = add i64 %1, %0\n; └\n  ret i64 %2\n}\n\n\n\n@code_llvm f(2.0, 5.0)\n\n;  @ In[18]:1 within `f`\ndefine double @julia_f_2327(double %0, double %1) #0 {\ntop:\n; ┌ @ float.jl:408 within `+`\n   %2 = fadd double %0, %1\n; └\n  ret double %2\n}\n\n\nNotice that when f is the function that takes in two Ints, Ints add to give an Int and thus f outputs an Int. When f is the function that takes two Float64s, f returns a Float64. Thus in the code:\n\nfunction g(x, y)\n  a = 4\n  b = 2\n  c = f(x, a)\n  d = f(b, c)\n  f(d, y)\nend\n\n@code_llvm g(2, 5)\n\n;  @ In[21]:1 within `g`\ndefine i64 @julia_g_2329(i64 signext %0, i64 signext %1) #0 {\ntop:\n;  @ In[21]:5 within `g`\n; ┌ @ In[18]:1 within `f`\n; │┌ @ int.jl:87 within `+`\n    %2 = add i64 %0, 6\n; └└\n;  @ In[21]:6 within `g`\n; ┌ @ In[18]:1 within `f`\n; │┌ @ int.jl:87 within `+`\n    %3 = add i64 %2, %1\n; └└\n  ret i64 %3\n}\n\n\ng on two Int inputs is a function that has Ints at every step along the way and spits out an Int. We can use the @code_warntype macro to better see the inference along the steps of the function:\n\n@code_warntype g(2, 5)\n\nMethodInstance for g(::Int64, ::Int64)\n  from g(x, y) @ Main In[21]:1\nArguments\n  #self#::Core.Const(g)\n\n\n\n  x::Int64\n  y::Int64\nLocals\n  d::Int64\n  c::Int64\n  b::Int64\n  a::Int64\nBody::Int64\n\n\n1 ─      \n\n\n(a = 4)\n\n\n│        (b = 2)\n│        (c = Main.f(x, a\n\n\n::Core.Const(4)))\n│        (d = Main.f(b::Core.Const(2), c))\n│   %5 = Main.f(d, y)::Int64\n└──      return %5\n\n\n\nWhat happens on mixtures?\n\n@code_llvm f(2.0, 5)\n\n;  @ In[18]:1 within `f`\ndefine double @julia_f_2829(double %0, i64 signext %1) #0 {\ntop:\n; ┌ @ promotion.jl:410 within `+`\n; │┌ @ promotion.jl:381 within `promote`\n; ││┌ @ promotion.jl:358 within `_promote`\n; │││┌ @ number.jl:7 within `convert`\n; ││││┌ @ float.jl:159 within `Float64`\n       %2 = sitofp i64 %1 to double\n; │└└└└\n; │ @ promotion.jl:410 within `+` @ float.jl:408\n   %3 = fadd double %2, %0\n; └\n  ret double %3\n}\n\n\nWhen we add an Int to a Float64, we promote the Int to a Float64 and then perform the + between two Float64s. When we go to the full function, we see that it can still infer:\n\n@code_warntype g(2.0, 5)\n\nMethodInstance for g(::Float64, ::Int64)\n  from g(x, y) @ Main In[21]:1\nArguments\n  #self#::Core.Const(g)\n  x::Float64\n  y::Int64\nLocals\n  d::Float64\n  c::Float64\n  b::Int64\n  a::Int64\nBody::Float64\n1 ─      (a = 4)\n│        (b = 2)\n│        (c = Main.f(x, a::Core.Const(4)))\n│        (d = Main.f(b::Core.Const(2), c))\n│   %5 = Main.f(d, y)::Float64\n└──      return %5\n\n\n\nand it uses this to build a very efficient assembly code because it knows exactly what the types will be at every step:\n\n@code_llvm g(2.0, 5)\n\n;  @ In[21]:1 within `g`\ndefine double @julia_g_2832(double %0, i64 signext %1) #0 {\ntop:\n;  @ In[21]:4 within `g`\n; ┌ @ In[18]:1 within `f`\n; │┌ @ promotion.jl:410 within `+` @ float.jl:408\n    %2 = fadd double %0, 4.000000e+00\n; └└\n;  @ In[21]:5 within `g`\n; ┌ @ In[18]:1 within `f`\n; │┌ @ promotion.jl:410 within `+` @ float.jl:408\n    %3 = fadd double %2, 2.000000e+00\n; └└\n;  @ In[21]:6 within `g`\n; ┌ @ In[18]:1 within `f`\n; │┌ @ promotion.jl:410 within `+`\n; ││┌ @ promotion.jl:381 within `promote`\n; │││┌ @ promotion.jl:358 within `_promote`\n; ││││┌ @ number.jl:7 within `convert`\n; │││││┌ @ float.jl:159 within `Float64`\n        %4 = sitofp i64 %1 to double\n; ││└└└└\n; ││ @ promotion.jl:410 within `+` @ float.jl:408\n    %5 = fadd double %3, %4\n; └└\n  ret double %5\n}\n\n\n(notice how it handles the constant literals 4 and 2: it converted them at compile time to reduce the algorithm to 3 floating point additions).\n\n\n1.2.3 Type Stability\nWhy is the inference algorithm able to infer all of the types of g? It’s because it knows the types coming out of f at compile time. Given an Int and a Float64, f will always output a Float64, and thus it can continue with inference knowing that c, d, and eventually the output is Float64. Thus in order for this to occur, we need that the type of the output on our function is directly inferred from the type of the input. This property is known as type-stability.\nAn example of breaking it is as follows:\n\nfunction h(x, y)\n  out = x + y\n  rand() &lt; 0.5 ? out : Float64(out)\nend\n\nh (generic function with 1 method)\n\n\nHere, on an integer input the output’s type is randomly either Int or Float64, and thus the output is unknown:\n\n@code_warntype h(2, 5)\n\nMethodInstance for h(::Int64, ::Int64)\n  from h(x, y) @ Main In[26]:1\nArguments\n  #self#::Core.Const(h)\n  x::Int64\n  y::Int64\nLocals\n  out::Int64\nBody::Union{Float64, Int64}\n1 ─      (out = x + y)\n│   %2 = Main.rand()::Float64\n│   %3 = (%2 &lt; 0.5)::Bool\n└──      goto #3 if not %3\n2 ─      return out\n3 ─ %6 = Main.Float64(out)::Float64\n└──      return %6\n\n\n\nThis means that its output type is Union{Int,Float64} (Julia uses union types to keep the types still somewhat constrained). Once there are multiple choices, those need to get propagated through the compiler, and all subsequent calculations are the result of either being an Int or a Float64.\n(Note that Julia has small union optimizations, so if this union is of size 4 or less then Julia will still be able to optimize it quite a bit.)\n\n\n1.2.4 Multiple Dispatch\nThe + function on numbers was implemented in Julia, so how were these rules all written down? The answer is multiple dispatch. In Julia, you can tell a function how to act differently on different types by using type assertions on the input values. For example, let’s make a function that computes 2x + y on Int and x/y on Float64:\n\nff(x::Int, y::Int) = 2x + y\nff(x::Float64, y::Float64) = x / y\n@show ff(2, 5)\n@show ff(2.0, 5.0)\n\nff(2, 5) = 9\nff(2.0, 5.0) = 0.4\n\n\n0.4\n\n\nThe + function in Julia is just defined as +(a, b), and we can actually point to that code in the Julia distribution:\n\n@which +(2.0, 5)\n\n+(x::Number, y::Number) in Base at promotion.jl:410\n\n\nTo control at a higher level, Julia uses abstract types. For example, Float64 &lt;: AbstractFloat, meaning Float64s are a subtype of AbstractFloat. We also have that Int &lt;: Integer, while both AbstractFloat &lt;: Number and Integer &lt;: Number.\n\nJulia allows the user to define dispatches at a higher level, and the version that is called is the most strict version that is correct. For example, right now with ff we will get a MethodError if we call it between a Int and a Float64 because no such method exists:\n\nff(2.0, 5)\n\nLoadError: MethodError: no method matching ff(::Float64, ::Int64)\n\n\u001b[0mClosest candidates are:\n\u001b[0m  ff(\u001b[91m::Int64\u001b[39m, ::Int64)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[35mMain\u001b[39m \u001b[90m\u001b[4mIn[28]:1\u001b[24m\u001b[39m\n\u001b[0m  ff(::Float64, \u001b[91m::Float64\u001b[39m)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[35mMain\u001b[39m \u001b[90m\u001b[4mIn[28]:2\u001b[24m\u001b[39m\n\n\nHowever, we can add a fallback method to the function ff for two numbers:\n\nff(x::Number, y::Number) = x + y\nff(2.0, 5)\n\n7.0\n\n\nNotice that the fallback method still specializes on the inputs:\n\n@code_llvm ff(2.0, 5)\n\n;  @ In[31]:1 within `ff`\ndefine double @julia_ff_3211(double %0, i64 signext %1) #0 {\ntop:\n; ┌ @ promotion.jl:410 within `+`\n; │┌ @ promotion.jl:381 within `promote`\n; ││┌ @ promotion.jl:358 within `_promote`\n; │││┌ @ number.jl:7 within `convert`\n; ││││┌ @ float.jl:159 within `Float64`\n       %2 = sitofp i64 %1 to double\n; │└└└└\n; │ @ promotion.jl:410 within `+` @ float.jl:408\n   %3 = fadd double %2, %0\n; └\n  ret double %3\n}\n\n\nIt’s essentially just a template for what functions to possibly try and create given the types that are seen. When it sees Float64 and Int, it knows it should try and create the function that does x+y, and once it knows it’s Float64 plus a Int, it knows it should create the function that converts the Int to a Float64 and then does addition between two Float64s, and that is precisely the generated LLVM IR on this pair of input types.\nAnd that’s essentially Julia’s secret sauce: since it’s always specializing its types on each function, if those functions themselves can infer the output, then the entire function can be inferred and generate optimal code, which is then optimized by the compiler and out comes an efficient function. If types can’t be inferred, Julia falls back to a slower “Python” mode (though with optimizations in cases like small unions). Users then get control over this specialization process through multiple dispatch, which is then Julia’s core feature since it allows adding new options without any runtime cost.\n\n\n1.2.5 Any Fallbacks\nNote that f(x, y) = x + y is equivalent to f(x::Any, y::Any) = x + y, where Any is the maximal supertype of every Julia type. Thus f(x, y) = x + y is essentially a fallback for all possible input values, telling it what to do in the case that no other dispatches exist. However, note that this dispatch itself is not slow, since it will be specialized on the input types.\n\n\n1.2.6 Ambiguities\nThe version that is called is the most strict version that is correct. What happens if it’s impossible to define “the most strict version”? For example,\n\nff(x::Float64, y::Number) = 5x + 2y\nff(x::Number, y::Int) = x - y\n\nff (generic function with 5 methods)\n\n\nWhat should it call on f(2.0, 5) now? ff(x::Float64, y::Number) and ff(x::Number, y::Int) are both more strict than ff(x::Number, y::Number), so one of them should be called, but neither are more strict than each other, and thus you will end up with an ambiguity error:\n\nff(2.0, 5)\n\nLoadError: MethodError: ff(::Float64, ::Int64) is ambiguous.\n\nCandidates:\n  ff(\u001b[90mx\u001b[39m::\u001b[1mFloat64\u001b[22m, \u001b[90my\u001b[39m::\u001b[1mNumber\u001b[22m)\n\u001b[90m    @\u001b[39m \u001b[90mMain\u001b[39m \u001b[90m\u001b[4mIn[33]:1\u001b[24m\u001b[39m\n  ff(\u001b[90mx\u001b[39m::\u001b[1mNumber\u001b[22m, \u001b[90my\u001b[39m::\u001b[1mInt64\u001b[22m)\n\u001b[90m    @\u001b[39m \u001b[90mMain\u001b[39m \u001b[90m\u001b[4mIn[33]:2\u001b[24m\u001b[39m\n\nPossible fix, define\n  ff(::Float64, ::Int64)\n\n\n\n\n1.2.7 Untyped Containers\nOne way to ruin inference is to use an untyped container. For example, the array constructors use type inference themselves to know what their container type will be. Therefore,\n\na = [1.0, 2.0, 3.0]\n\n3-element Vector{Float64}:\n 1.0\n 2.0\n 3.0\n\n\nuses type inference on its inputs to know that it should be something that holds Float64 values, and thus it is a 1-dimensional array of Float64 values, or Array{Float64, 1}. The accesses:\n\na[1]\n\n1.0\n\n\nare then inferred, since this is just the function getindex(a::Array{T}, i) where T which is a function that will produce something of type T, the element type of the array. However, if we tell Julia to make an array with element type Any:\n\nb = [\"1.0\", 2, 2.0]\n\n3-element Vector{Any}:\n  \"1.0\"\n 2\n 2.0\n\n\n(here, Julia falls back to Any because it cannot promote the values to the same type), then the best inference can do on the output is to say it could have any type:\n\nfunction bad_container(a)\n  a[2]\nend\n@code_warntype bad_container(a)\n\nMethodInstance for bad_container(::Vector{Float64})\n  from bad_container(a) @ Main In[38]:1\nArguments\n  #self#::Core.Const(bad_container)\n  a::Vector{Float64}\nBody::Float64\n1 ─ %1 = Base.getindex(a, 2)::Float64\n└──      return %1\n\n\n\n\n@code_warntype bad_container(b)\n\nMethodInstance for bad_container(::Vector{Any})\n  from bad_container(a) @ Main In[38]:1\nArguments\n  #self#::Core.Const(bad_container)\n  a::Vector{Any}\nBody::Any\n1 ─ %1 = Base.getindex(a, 2)::Any\n└──      return %1\n\n\n\nThis is one common way that type inference can breakdown. For example, even if the array is all numbers, we can still break inference:\n\nx = Number[1.0, 3]\nfunction q(x)\n  a = 4\n  b = 2\n  c = f(x[1], a)\n  d = f(b, c)\n  f(d, x[2])\nend\n@code_warntype q(x)\n\nMethodInstance for q(::Vector{Number})\n  from q(x) @ Main In[40]:2\nArguments\n  #self#::Core.Const(q)\n  x::Vector{Number}\nLocals\n  d::Any\n  c::Any\n  b::Int64\n  a::Int64\nBody::Any\n1 ─      (a = 4)\n│        (b = 2)\n│   %3 = Base.getindex(x, 1)::Number\n│        (c = Main.f(%3, a::Core.Const(4)))\n│        (d = Main.f(b::Core.Const(2), c))\n│   %6 = d::Any\n│   %7 = Base.getindex(x, 2)::Number\n│   %8 = Main.f(%6, %7)::Any\n└──      return %8\n\n\n\nHere the type inference algorithm quickly gives up and infers to Any, losing all specialization and automatically switching to Python-style runtime type checking.\n\n\n1.2.8 Type definitions\n\n\n1.2.9 Value types and isbits\nIn Julia, types which can fully inferred and which are composed of primitive or isbits types are value types. This means that, inside of an array, their values are the values of the type itself, and not a pointer to the values.\nYou can check if the type is a value type through isbits:\n\nisbits(1.0)\n\ntrue\n\n\nNote that a Julia struct which holds isbits values is isbits as well, if it’s fully inferred:\n\nstruct MyComplex\n  real::Float64\n  imag::Float64\nend\nisbits(MyComplex(1.0, 1.0))\n\ntrue\n\n\nWe can see that the compiler knows how to use this efficiently since it knows that what comes out is always Float64:\n\nBase.:+(a::MyComplex, b::MyComplex) = MyComplex(a.real + b.real, a.imag + b.imag)\nBase.:+(a::MyComplex, b::Int) = MyComplex(a.real + b, a.imag)\nBase.:+(b::Int, a::MyComplex) = MyComplex(a.real + b, a.imag)\ng(MyComplex(1.0, 1.0),MyComplex(1.0, 1.0))\n\nMyComplex(8.0, 2.0)\n\n\n\n@code_warntype g(MyComplex(1.0, 1.0),MyComplex(1.0, 1.0))\n\nMethodInstance for g(::MyComplex, ::MyComplex)\n  from g(x, y) @ Main In[21]:1\nArguments\n  #self#::Core.Const(g)\n  x::MyComplex\n  y::MyComplex\nLocals\n  d::MyComplex\n  c::MyComplex\n  b::Int64\n  a::Int64\nBody::MyComplex\n1 ─      (a = 4)\n│        (b = 2)\n│        (c = Main.f(x, a::Core.Const(4)))\n│        (d = Main.f(b::Core.Const(2), c))\n│   %5 = Main.f(d, y)::MyComplex\n└──      return %5\n\n\n\n\n@code_llvm g(MyComplex(1.0, 1.0),MyComplex(1.0, 1.0))\n\n;  @ In[21]:1 within `g`\ndefine void @julia_g_3688([2 x double]* noalias nocapture noundef nonnull sret([2 x double]) align 8 dereferenceable(16) %0, [2 x double]* nocapture noundef nonnull readonly align 8 dereferenceable(16) %1, [2 x double]* nocapture noundef nonnull readonly align 8 dereferenceable(16) %2) #0 {\ntop:\n;  @ In[21]:4 within `g`\n; ┌ @ In[18]:1 within `f`\n; │┌ @ In[43]:2 within `+`\n; ││┌ @ Base.jl:37 within `getproperty`\n     %3 = getelementptr inbounds [2 x double], [2 x double]* %1, i64 0, i64 0\n; ││└\n; ││ @ In[43]:2 within `+` @ promotion.jl:410 @ float.jl:408\n    %4 = load double, double* %3, align 8\n    %5 = fadd double %4, 4.000000e+00\n; ││ @ In[43]:2 within `+`\n; ││┌ @ Base.jl:37 within `getproperty`\n     %6 = getelementptr inbounds [2 x double], [2 x double]* %1, i64 0, i64 1\n; └└└\n;  @ In[21]:5 within `g`\n; ┌ @ In[18]:1 within `f`\n; │┌ @ In[43]:3 within `+` @ promotion.jl:410 @ float.jl:408\n    %7 = fadd double %5, 2.000000e+00\n; └└\n;  @ In[21]:6 within `g`\n; ┌ @ In[18]:1 within `f`\n; │┌ @ In[43]:1 within `+` @ float.jl:408\n    %8 = load double, double* %6, align 8\n    %9 = bitcast [2 x double]* %2 to &lt;2 x double&gt;*\n    %10 = load &lt;2 x double&gt;, &lt;2 x double&gt;* %9, align 8\n    %11 = insertelement &lt;2 x double&gt; poison, double %7, i64 0\n    %12 = insertelement &lt;2 x double&gt; %11, double %8, i64 1\n    %13 = fadd &lt;2 x double&gt; %10, %12\n; └└\n  %14 = bitcast [2 x double]* %0 to &lt;2 x double&gt;*\n  store &lt;2 x double&gt; %13, &lt;2 x double&gt;* %14, align 8\n  ret void\n}\n\n\nNote that the compiled code simply works directly on the double pieces. We can also make this be concrete without pre-specifying that the values always have to be Float64 by using a type parameter.\n\nstruct MyParameterizedComplex{T}\n  real::T\n  imag::T\nend\nisbits(MyParameterizedComplex(1.0, 1.0))\n\ntrue\n\n\nNote that MyParameterizedComplex{T} is a concrete type for every T: it is a shorthand form for defining a whole family of types.\n\nBase.:+(a::MyParameterizedComplex, b::MyParameterizedComplex) = MyParameterizedComplex(a.real + b.real, a.imag + b.imag)\nBase.:+(a::MyParameterizedComplex, b::Int) = MyParameterizedComplex(a.real + b, a.imag)\nBase.:+(b::Int, a::MyParameterizedComplex) = MyParameterizedComplex(a.real + b, a.imag)\ng(MyParameterizedComplex(1.0, 1.0), MyParameterizedComplex(1.0, 1.0))\n\nMyParameterizedComplex{Float64}(8.0, 2.0)\n\n\n\n@code_warntype g(MyParameterizedComplex(1.0, 1.0), MyParameterizedComplex(1.0, 1.0))\n\nMethodInstance for g(::MyParameterizedComplex{Float64}, ::MyParameterizedComplex{Float64})\n  from g(x, y) @ Main In[21]:1\nArguments\n  #self#::Core.Const(g)\n  x::MyParameterizedComplex{Float64}\n  y::MyParameterizedComplex{Float64}\nLocals\n  d::MyParameterizedComplex{Float64}\n  c::MyParameterizedComplex{Float64}\n  b::Int64\n  a::Int64\nBody::MyParameterizedComplex{Float64}\n1 ─      (a = 4)\n│        (b = 2)\n│        (c = Main.f(x, a::Core.Const(4)))\n│        (d = Main.f(b::Core.Const(2), c))\n│   %5 = Main.f(d, y)::MyParameterizedComplex{Float64}\n└──      return %5\n\n\n\nSee that this code also automatically works and compiles efficiently for Float32 as well:\n\n@code_warntype g(MyParameterizedComplex(1.0f0, 1.0f0), MyParameterizedComplex(1.0f0, 1.0f0))\n\nMethodInstance for g(::MyParameterizedComplex{Float32}, ::MyParameterizedComplex{Float32})\n  from g(x, y) @ Main In[21]:1\nArguments\n  #self#::Core.Const(g)\n  x::MyParameterizedComplex{Float32}\n  y::MyParameterizedComplex{Float32}\nLocals\n  d::MyParameterizedComplex{Float32}\n  c::MyParameterizedComplex{Float32}\n  b::Int64\n  a::Int64\nBody::MyParameterizedComplex{Float32}\n1 ─      (a = 4)\n│        (b = 2)\n│        (c = Main.f(x, a::Core.Const(4)))\n│        (d = Main.f(b::Core.Const(2), c))\n│   %5 = Main.f(d, y)::MyParameterizedComplex{Float32}\n└──      return %5\n\n\n\n\n@code_llvm g(MyParameterizedComplex(1.0f0, 1.0f0), MyParameterizedComplex(1.0f0, 1.0f0))\n\n;  @ In[21]:1 within `g`\ndefine [2 x float] @julia_g_3750([2 x float]* nocapture noundef nonnull readonly align 4 dereferenceable(8) %0, [2 x float]* nocapture noundef nonnull readonly align 4 dereferenceable(8) %1) #0 {\ntop:\n;  @ In[21]:4 within `g`\n; ┌ @ In[18]:1 within `f`\n; │┌ @ In[47]:2 within `+`\n; ││┌ @ Base.jl:37 within `getproperty`\n     %2 = getelementptr inbounds [2 x float], [2 x float]* %0, i64 0, i64 0\n; ││└\n; ││ @ In[47]:2 within `+` @ promotion.jl:410 @ float.jl:408\n    %3 = load float, float* %2, align 4\n    %4 = fadd float %3, 4.000000e+00\n; ││ @ In[47]:2 within `+`\n; ││┌ @ Base.jl:37 within `getproperty`\n     %5 = getelementptr inbounds [2 x float], [2 x float]* %0, i64 0, i64 1\n; └└└\n;  @ In[21]:5 within `g`\n; ┌ @ In[18]:1 within `f`\n; │┌ @ In[47]:3 within `+` @ promotion.jl:410 @ float.jl:408\n    %6 = fadd float %4, 2.000000e+00\n; └└\n;  @ In[21]:6 within `g`\n; ┌ @ In[18]:1 within `f`\n; │┌ @ In[47]:1 within `+`\n; ││┌ @ Base.jl:37 within `getproperty`\n     %7 = getelementptr inbounds [2 x float], [2 x float]* %1, i64 0, i64 0\n; ││└\n; ││ @ In[47]:1 within `+` @ float.jl:408\n    %8 = load float, float* %7, align 4\n    %9 = fadd float %8, %6\n; ││ @ In[47]:1 within `+`\n; ││┌ @ Base.jl:37 within `getproperty`\n     %10 = getelementptr inbounds [2 x float], [2 x float]* %1, i64 0, i64 1\n; ││└\n; ││ @ In[47]:1 within `+` @ float.jl:408\n    %11 = load float, float* %5, align 4\n    %12 = load float, float* %10, align 4\n    %13 = fadd float %11, %12\n; └└\n  %.fca.0.insert = insertvalue [2 x float] zeroinitializer, float %9, 0\n  %.fca.1.insert = insertvalue [2 x float] %.fca.0.insert, float %13, 1\n  ret [2 x float] %.fca.1.insert\n}\n\n\nIt is important to know that if there is any piece of a type which doesn’t contain type information, then it cannot be isbits because then it would have to be compiled in such a way that the size is not known in advance. For example:\n\nstruct MySlowComplex\n  real\n  imag\nend\nisbits(MySlowComplex(1.0, 1.0))\n\nfalse\n\n\n\nBase.:+(a::MySlowComplex, b::MySlowComplex) = MySlowComplex(a.real + b.real, a.imag + b.imag)\nBase.:+(a::MySlowComplex, b::Int) = MySlowComplex(a.real + b, a.imag)\nBase.:+(b::Int, a::MySlowComplex) = MySlowComplex(a.real + b, a.imag)\ng(MySlowComplex(1.0, 1.0), MySlowComplex(1.0, 1.0))\n\nMySlowComplex(8.0, 2.0)\n\n\n\n@code_warntype g(MySlowComplex(1.0, 1.0), MySlowComplex(1.0, 1.0))\n\nMethodInstance for g(::MySlowComplex, ::MySlowComplex)\n  from g(x, y) @ Main In[21]:1\nArguments\n  #self#::Core.Const(g)\n  x::MySlowComplex\n  y::MySlowComplex\nLocals\n  d::MySlowComplex\n  c::MySlowComplex\n  b::Int64\n  a::Int64\nBody::MySlowComplex\n1 ─      (a = 4)\n│        (b = 2)\n│        (c = Main.f(x, a::Core.Const(4)))\n│        (d = Main.f(b::Core.Const(2), c))\n│   %5 = Main.f(d, y)::MySlowComplex\n└──      return %5\n\n\n\n\n@code_llvm g(MySlowComplex(1.0, 1.0), MySlowComplex(1.0, 1.0))\n\n;  @ In[21]:1 within `g`\ndefine void @julia_g_3819([2 x {}*]* noalias nocapture noundef nonnull sret([2 x {}*]) align 8 dereferenceable(16) %0, [2 x {}*]* nocapture noundef nonnull readonly align 8 dereferenceable(16) %1, [2 x {}*]* nocapture noundef nonnull readonly align 8 dereferenceable(16) %2) #0 {\ntop:\n  %gcframe2 = alloca [8 x {}*], align 16\n  %gcframe2.sub = getelementptr inbounds [8 x {}*], [8 x {}*]* %gcframe2, i64 0, i64 0\n  %3 = bitcast [8 x {}*]* %gcframe2 to i8*\n  call void @llvm.memset.p0i8.i32(i8* noundef nonnull align 16 dereferenceable(64) %3, i8 0, i32 64, i1 false)\n  %4 = getelementptr inbounds [8 x {}*], [8 x {}*]* %gcframe2, i64 0, i64 6\n  %5 = bitcast {}** %4 to [2 x {}*]*\n  %6 = getelementptr inbounds [8 x {}*], [8 x {}*]* %gcframe2, i64 0, i64 4\n  %7 = bitcast {}** %6 to [2 x {}*]*\n  %8 = getelementptr inbounds [8 x {}*], [8 x {}*]* %gcframe2, i64 0, i64 2\n  %9 = bitcast {}** %8 to [2 x {}*]*\n  %thread_ptr = call i8* asm \"movq %fs:0, $0\", \"=r\"() #6\n  %ppgcstack_i8 = getelementptr i8, i8* %thread_ptr, i64 -8\n  %ppgcstack = bitcast i8* %ppgcstack_i8 to {}****\n  %pgcstack = load {}***, {}**** %ppgcstack, align 8\n;  @ In[21]:4 within `g`\n; ┌ @ In[18]:1 within `f`\n   %10 = bitcast [8 x {}*]* %gcframe2 to i64*\n   store i64 24, i64* %10, align 16\n   %11 = getelementptr inbounds [8 x {}*], [8 x {}*]* %gcframe2, i64 0, i64 1\n   %12 = bitcast {}** %11 to {}***\n   %13 = load {}**, {}*** %pgcstack, align 8\n   store {}** %13, {}*** %12, align 8\n   %14 = bitcast {}*** %pgcstack to {}***\n   store {}** %gcframe2.sub, {}*** %14, align 8\n   call void @\"j_+_3821\"([2 x {}*]* noalias nocapture noundef nonnull sret([2 x {}*]) %5, [2 x {}*]* nocapture nonnull readonly %1, i64 signext 4) #0\n; └\n;  @ In[21]:5 within `g`\n; ┌ @ In[18]:1 within `f`\n   call void @\"j_+_3822\"([2 x {}*]* noalias nocapture noundef nonnull sret([2 x {}*]) %7, i64 signext 2, [2 x {}*]* nocapture readonly %5) #0\n; └\n;  @ In[21]:6 within `g`\n; ┌ @ In[18]:1 within `f`\n   call void @\"j_+_3823\"([2 x {}*]* noalias nocapture noundef nonnull sret([2 x {}*]) %9, [2 x {}*]* nocapture readonly %7, [2 x {}*]* nocapture nonnull readonly %2) #0\n; └\n  %15 = bitcast [2 x {}*]* %0 to i8*\n  %16 = bitcast {}** %8 to i8*\n  call void @llvm.memcpy.p0i8.p0i8.i64(i8* noundef nonnull align 8 dereferenceable(16) %15, i8* noundef nonnull align 16 dereferenceable(16) %16, i64 16, i1 false)\n  %17 = load {}*, {}** %11, align 8\n  %18 = bitcast {}*** %pgcstack to {}**\n  store {}* %17, {}** %18, align 8\n  ret void\n}\n\n\n\nstruct MySlowComplex2\n  real::AbstractFloat\n  imag::AbstractFloat\nend\nisbits(MySlowComplex2(1.0,  1.0))\n\nfalse\n\n\n\nBase.:+(a::MySlowComplex2, b::MySlowComplex2) = MySlowComplex2(a.real + b.real, a.imag + b.imag)\nBase.:+(a::MySlowComplex2, b::Int) = MySlowComplex2(a.real + b, a.imag)\nBase.:+(b::Int, a::MySlowComplex2) = MySlowComplex2(a.real + b, a.imag)\ng(MySlowComplex2(1.0, 1.0), MySlowComplex2(1.0, 1.0))\n\nMySlowComplex2(8.0, 2.0)\n\n\nHere’s the timings:\n\na = MyComplex(1.0, 1.0)\nb = MyComplex(2.0, 1.0)\n@btime g(a, b)\n\n  16.782 ns (1 allocation: 32 bytes)\n\n\nMyComplex(9.0, 2.0)\n\n\n\na = MyParameterizedComplex(1.0, 1.0)\nb = MyParameterizedComplex(2.0, 1.0)\n@btime g(a, b)\n\n  16.845 ns (1 allocation: 32 bytes)\n\n\nMyParameterizedComplex{Float64}(9.0, 2.0)\n\n\n\na = MySlowComplex(1.0, 1.0)\nb = MySlowComplex(2.0, 1.0)\n@btime g(a, b)\n\n  85.371 ns (5 allocations: 96 bytes)\n\n\nMySlowComplex(9.0, 2.0)\n\n\n\na = MySlowComplex2(1.0, 1.0)\nb = MySlowComplex2(2.0, 1.0)\n@btime g(a, b)\n\n  667.079 ns (14 allocations: 288 bytes)\n\n\nMySlowComplex2(9.0, 2.0)\n\n\n\n\n1.2.10 Note on Julia\nNote that, because of these type specialization, value types, etc. properties, the number types, even ones such as Int, Float64, and Complex, are all themselves implemented in pure Julia! Thus even basic pieces can be implemented in Julia with full performance, given one uses the features correctly.\n\n\n1.2.11 Note on isbits\nNote that a type which is mutable struct will not be isbits. This means that mutable structs will be a pointer to a heap allocated object, unless it’s shortlived and the compiler can erase its construction. Also, note that isbits compiles down to bit operations from pure Julia, which means that these types can directly compile to GPU kernels through CUDAnative without modification.\n\n\n1.2.12 Function Barriers\nSince functions automatically specialize on their input types in Julia, we can use this to our advantage in order to make an inner loop fully inferred. For example, take the code from above but with a loop:\n\nfunction r(x)\n  a = 4\n  b = 2\n  for i in 1:100\n    c = f(x[1], a)\n    d = f(b, c)\n    a = f(d, x[2])\n  end\n  a\nend\n@btime r(x)\n\n  4.648 μs (300 allocations: 4.69 KiB)\n\n\n604.0\n\n\nIn here, the loop variables are not inferred and thus this is really slow. However, we can force a function call in the middle to end up with specialization and in the inner loop be stable:\n\ns(x) = _s(x[1], x[2])\nfunction _s(x1, x2)\n  a = 4\n  b = 2\n  for i in 1:100\n    c = f(x1, a)\n    d = f(b, c)\n    a = f(d, x2)\n  end\n  a\nend\n@btime s(x)\n\n  304.462 ns (1 allocation: 16 bytes)\n\n\n604.0\n\n\nNotice that this algorithm still doesn’t infer:\n\n@code_warntype s(x)\n\nMethodInstance for s(::Vector{Number})\n  from s(x) @ Main In[62]:1\nArguments\n  #self#::Core.Const(s)\n  x::Vector{Number}\nBody::Any\n1 ─ %1 = Base.getindex(x, 1)::Number\n│   %2 = Base.getindex(x, 2)::Number\n│   %3 = Main._s(%1, %2)::Any\n└──      return %3\n\n\n\nsince the output of _s isn’t inferred, but while it’s in _s it will have specialized on the fact that x[1] is a Float64 while x[2] is a Int, making that inner loop fast. In fact, it will only need to pay one dynamic dispatch, i.e. a multiple dispatch determination that happens at runtime. Notice that whenever functions are inferred, the dispatching is static since the choice of the dispatch is already made and compiled into the LLVM IR.\n\n\n1.2.13 Specialization at Compile Time\nJulia code will specialize at compile time if it can prove something about the result. For example:\n\nfunction fff(x)\n  if x isa Int\n    y = 2\n  else\n    y = 4.0\n  end\n  x + y\nend\n\nfff (generic function with 1 method)\n\n\nYou might think this function has a branch, but in reality Julia can determine whether x is an Int or not at compile time, so it will actually compile it away and just turn it into the function x + 2 or x + 4.0:\n\n@code_llvm fff(5)\n\n;  @ In[64]:1 within `fff`\ndefine i64 @julia_fff_4048(i64 signext %0) #0 {\ntop:\n;  @ In[64]:7 within `fff`\n; ┌ @ int.jl:87 within `+`\n   %1 = add i64 %0, 2\n; └\n  ret i64 %1\n}\n\n\n\n@code_llvm fff(2.0)\n\n;  @ In[64]:1 within `fff`\ndefine double @julia_fff_4050(double %0) #0 {\ntop:\n;  @ In[64]:7 within `fff`\n; ┌ @ float.jl:408 within `+`\n   %1 = fadd double %0, 4.000000e+00\n; └\n  ret double %1\n}\n\n\nThus one does not need to worry about over-optimizing since in the obvious cases the compiler will actually remove all of the extra pieces when it can!\n\n\n1.2.14 Global Scope and Optimizations\nThis discussion shows how Julia’s optimizations all apply during function specialization times. Thus calling Julia functions is fast. But what about when doing something outside of the function, like directly in a module or in the REPL?\n\n@btime for j in 1:100, i in 1:100\n  global A, B, C\n  C[i, j] = A[i, j] + B[i, j]\nend\n\n  641.637 μs (30000 allocations: 468.75 KiB)\n\n\nThis is very slow because the types of A, B, and C cannot be inferred. Why can’t they be inferred? Well, at any time in the dynamic REPL scope I can do something like C = \"haha now a string!\", and thus it cannot specialize on the types currently existing in the REPL (since asynchronous changes could also occur), and therefore it defaults back to doing a type check at every single function which slows it down. Moral of the story, Julia functions are fast but its global scope is too dynamic to be optimized.\n\n\n1.2.15 Summary\n\nJulia is not fast because of its JIT, it’s fast because of function specialization and type inference\nType stable functions allow inference to fully occur\nMultiple dispatch works within the function specialization mechanism to create overhead-free compile time controls\nJulia will specialize the generic functions\nMaking sure values are concretely typed in inner loops is essential for performance"
  },
  {
    "objectID": "optimize.html#overheads-of-individual-operations",
    "href": "optimize.html#overheads-of-individual-operations",
    "title": "1  Optimizing Serial Code",
    "section": "1.3 Overheads of Individual Operations",
    "text": "1.3 Overheads of Individual Operations\nNow let’s dig even a little deeper. Everything the processor does has a cost. A great chart to keep in mind is this classic one. A few things should immediately jump out to you:\n\nSimple arithmetic, like floating point additions, are super cheap. ~1 clock cycle, or a few nanoseconds.\nProcessors do branch prediction on if statements. If the code goes down the predicted route, the if statement costs ~1-2 clock cycles. If it goes down the wrong route, then it will take ~10-20 clock cycles. This means that predictable branches, like ones with clear patterns or usually the same output, are much cheaper (almost free) than unpredictable branches.\nFunction calls are expensive: 15-60 clock cycles!\nRAM reads are very expensive, with lower caches less expensive.\n\n\n1.3.1 Bounds Checking\nLet’s check the LLVM IR on one of our earlier loops:\n\nfunction inner_noalloc!(C, A, B)\n  for j in 1:100, i in 1:100\n    val = A[i, j] + B[i, j]\n    C[i, j] = val[1]\n  end\nend\n@code_llvm inner_noalloc!(C, A, B)\n\n;  @ In[68]:1 within `inner_noalloc!`\ndefine nonnull {}* @\"japi1_inner_noalloc!_4059\"({}* %0, {}** noalias nocapture noundef readonly %1, i32 %2) #0 {\ntop:\n  %3 = alloca {}**, align 8\n  store volatile {}** %1, {}*** %3, align 8\n  %4 = load {}*, {}** %1, align 8\n  %5 = getelementptr inbounds {}*, {}** %1, i64 1\n  %6 = load {}*, {}** %5, align 8\n  %7 = getelementptr inbounds {}*, {}** %1, i64 2\n  %8 = load {}*, {}** %7, align 8\n  %9 = bitcast {}* %6 to {}**\n  %10 = getelementptr inbounds {}*, {}** %9, i64 3\n  %11 = bitcast {}** %10 to i64*\n  %12 = load i64, i64* %11, align 8\n  %13 = getelementptr inbounds {}*, {}** %9, i64 4\n  %14 = bitcast {}** %13 to i64*\n  %15 = bitcast {}* %6 to double**\n  %16 = bitcast {}* %8 to {}**\n  %17 = getelementptr inbounds {}*, {}** %16, i64 3\n  %18 = bitcast {}** %17 to i64*\n  %19 = getelementptr inbounds {}*, {}** %16, i64 4\n  %20 = bitcast {}** %19 to i64*\n  %21 = bitcast {}* %8 to double**\n  %22 = bitcast {}* %4 to {}**\n  %23 = getelementptr inbounds {}*, {}** %22, i64 3\n  %24 = bitcast {}** %23 to i64*\n  %25 = getelementptr inbounds {}*, {}** %22, i64 4\n  %26 = bitcast {}** %25 to i64*\n  %27 = bitcast {}* %4 to double**\n;  @ In[68]:2 within `inner_noalloc!`\n  br label %L2\n\nL2:                                               ; preds = %L25, %top\n  %value_phi = phi i64 [ 1, %top ], [ %61, %L25 ]\n  %28 = add nsw i64 %value_phi, -1\n  %29 = load i64, i64* %14, align 8\n  %30 = icmp ult i64 %28, %29\n  %31 = mul i64 %12, %28\n  %32 = load double*, double** %15, align 8\n  %33 = load i64, i64* %18, align 8\n  %34 = mul i64 %33, %28\n  %35 = load double*, double** %21, align 8\n  %36 = load i64, i64* %24, align 8\n  %37 = load i64, i64* %26, align 8\n  %38 = icmp ult i64 %28, %37\n  %39 = mul i64 %36, %28\n  %40 = load double*, double** %27, align 8\n  br i1 %30, label %L2.split.us, label %oob\n\nL2.split.us:                                      ; preds = %L2\n  %41 = load i64, i64* %20, align 8\n  %42 = icmp ult i64 %28, %41\n  %smin61 = call i64 @llvm.smin.i64(i64 %33, i64 0)\n  %43 = sub i64 %33, %smin61\n  %smax62 = call i64 @llvm.smax.i64(i64 %smin61, i64 -1)\n  %44 = add nsw i64 %smax62, 1\n  %45 = mul nuw nsw i64 %43, %44\n  %umin63 = call i64 @llvm.umin.i64(i64 %12, i64 %45)\n  %smin64 = call i64 @llvm.smin.i64(i64 %36, i64 0)\n  %46 = sub i64 %36, %smin64\n  %smax65 = call i64 @llvm.smax.i64(i64 %smin64, i64 -1)\n  %47 = add nsw i64 %smax65, 1\n  %48 = mul nuw nsw i64 %46, %47\n  %umin66 = call i64 @llvm.umin.i64(i64 %umin63, i64 %48)\n  %exit.mainloop.at68 = call i64 @llvm.umin.i64(i64 %umin66, i64 100)\n  %.not205 = icmp eq i64 %exit.mainloop.at68, 0\n  br i1 %42, label %L2.split.us.split.us, label %L2.split.us.L2.split.us.split_crit_edge\n\nL2.split.us.L2.split.us.split_crit_edge:          ; preds = %L2.split.us\n  %.not203 = icmp eq i64 %12, 0\n  %or.cond = select i1 %.not205, i1 %.not203, i1 false\n  br i1 %or.cond, label %oob, label %oob5\n\nL2.split.us.split.us:                             ; preds = %L2.split.us\n  br i1 %.not205, label %main.pseudo.exit71, label %ib7.us.us\n\nib7.us.us:                                        ; preds = %idxend9.us.us, %L2.split.us.split.us\n  %value_phi2.us.us = phi i64 [ %59, %idxend9.us.us ], [ 1, %L2.split.us.split.us ]\n;  @ In[68]:3 within `inner_noalloc!`\n; ┌ @ essentials.jl:14 within `getindex`\n   %49 = add i64 %value_phi2.us.us, -1\n; └\n;  @ In[68]:4 within `inner_noalloc!`\n; ┌ @ array.jl:971 within `setindex!`\n   br i1 %38, label %idxend9.us.us, label %oob8\n\nidxend9.us.us:                                    ; preds = %ib7.us.us\n; └\n;  @ In[68]:3 within `inner_noalloc!`\n; ┌ @ essentials.jl:14 within `getindex`\n   %50 = add i64 %31, %49\n   %51 = getelementptr inbounds double, double* %32, i64 %50\n   %52 = load double, double* %51, align 8\n   %53 = add i64 %34, %49\n   %54 = getelementptr inbounds double, double\n\n\n* %35, i64 %53\n   %55 = load double, double* %54, align 8\n; └\n; ┌ @ float.jl:408 within `+`\n   %56 = fadd double %52, %55\n; └\n;  @ In[68]:4 within `inner_noalloc!`\n; ┌ @ array.jl:971 within `setindex!`\n   %57 = add i64 %39, %49\n   %58 = getelementptr inbounds double, double* %40, i64 %57\n   store double %56, double* %58, align 8\n; └\n;  @ In[68]:5 within `inner_noalloc!`\n; ┌ @ range.jl:891 within `iterate`\n   %59 = add i64 %value_phi2.us.us, 1\n; └\n  %.not206 = icmp ult i64 %value_phi2.us.us, %exit.mainloop.at68\n  br i1 %.not206, label %ib7.us.us, label %main.exit.selector70\n\nmain.exit.selector70:                             ; preds = %idxend9.us.us\n  %60 = icmp ult i64 %value_phi2.us.us, 100\n  br i1 %60, label %main.pseudo.exit71, label %L25\n\nmain.pseudo.exit71:                               ; preds = %main.exit.selector70, %L2.split.us.split.us\n  %value_phi2.us.us.copy = phi i64 [ 1, %L2.split.us.split.us ], [ %59, %main.exit.selector70 ]\n  br label %L5.us.us.postloop\n\nL25:                                              ; preds = %idxend9.us.us.postloop, %main.exit.selector70\n; ┌ @ range.jl:891 within `iterate`\n; │┌ @ promotion.jl:499 within `==`\n    %.not18 = icmp eq i64 %value_phi, 100\n; │└\n   %61 = add nuw nsw i64 %value_phi, 1\n; └\n  br i1 %.not18, label %L36, label %L2\n\nL36:                                              ; preds = %L25\n  ret {}* inttoptr (i64 140594586316808 to {}*)\n\noob:                                              ; preds = %L5.us.us.postloop, %L2.split.us.L2.split.us.split_crit_edge, %L2\n  %value_phi2.lcssa = phi i64 [ 1, %L2.split.us.L2.split.us.split_crit_edge ], [ %value_phi2.us.us.postloop, %L5.us.us.postloop ], [ 1, %L2 ]\n;  @ In[68]:3 within `inner_noalloc!`\n; ┌ @ essentials.jl:14 within `getindex`\n   %62 = alloca [2 x i64], align 8\n   %.sub = getelementptr inbounds [2 x i64], [2 x i64]* %62, i64 0, i64 0\n   store i64 %value_phi2.lcssa, i64* %.sub, align 8\n   %63 = getelementptr inbounds [2 x i64], [2 x i64]* %62, i64 0, i64 1\n   store i64 %value_phi, i64* %63, align 8\n   call void @ijl_bounds_error_ints({}* %6, i64* nonnull %.sub, i64 2)\n   unreachable\n\noob5:                                             ; preds = %idxend.us.us.postloop, %L2.split.us.L2.split.us.split_crit_edge\n   %value_phi2.lcssa19 = phi i64 [ 1, %L2.split.us.L2.split.us.split_crit_edge ], [ %value_phi2.us.us.postloop, %idxend.us.us.postloop ]\n   %64 = alloca [2 x i64], align 8\n   %.sub16 = getelementptr inbounds [2 x i64], [2 x i64]* %64, i64 0, i64 0\n   store i64 %value_phi2.lcssa19, i64* %.sub16, align 8\n   %65 = getelementptr inbounds [2 x i64], [2 x i64]* %64, i64 0, i64 1\n   store i64 %value_phi, i64* %65, align 8\n   call void @ijl_bounds_error_ints({}* %8, i64* nonnull %.sub16, i64 2)\n   unreachable\n\noob8.loopexit:                                    ; preds = %idxend6.us.us.postloop\n; └\n;  @ In[68]:4 within `inner_noalloc!`\n; ┌ @ array.jl:971 within `setindex!`\n   %value_phi2.us.us.postloop.mux = select i1 %71, i64 %value_phi2.us.us.postloop, i64 %value_phi2.us.us.copy\n   br label %oob8\n\noob8:                                             ; preds = %oob8.loopexit, %ib7.us.us\n   %value_phi2.lcssa20 = phi i64 [ %value_phi2.us.us.postloop.mux, %oob8.loopexit ], [ 1, %ib7.us.us ]\n   %66 = alloca [2 x i64], align 8\n   %.sub17 = getelementptr inbounds [2 x i64], [2 x i64]* %66, i64 0, i64 0\n   store i64 %value_phi2.lcssa20, i64* %.sub17, align 8\n   %67 = getelementptr inbounds [2 x i64], [2 x i64]* %66, i64 0, i64 1\n   store i64 %value_phi, i64* %67, align 8\n   call void @ijl_bounds_error_ints({}* %4, i64* nonnull %.sub17, i64 2)\n   unreachable\n\nL5.us.us.postloop:                                ; preds = %idxend9.us.us.postloop, %main.pseudo.exit71\n   %value_phi2.us.us.postloop = phi i64 [ %value_phi2.us.us.copy, %main.pseudo.exit71 ], [ %81, %idxend9.us.us.postloop ]\n; └\n;  @ In[68]:3 within `inner_noalloc!`\n; ┌ @ essentials.jl:14 within `getindex`\n   %68 = add i64 %value_phi2.us.us.postloop, -1\n   %69 = icmp ult i64 %68, %12\n   br i1 %69, label %idxend.us.us.postloop, label %oob\n\nidxend.us.us.postloop:                            ; preds = %L5.us.us.postloop\n   %70 = icmp ult i64 %68, %33\n   br i1 %70, label %idxend6.us.us.postloop, label %oob5\n\nidxend6.us.us.postloop:                           ; preds = %idxend.us.us.postloop\n; └\n;  @ In[68]:4 within `inner_noalloc!`\n; ┌ @ array.jl:971 within `setindex!`\n   %71 = icmp uge i64 %68, %36\n   %.not208 = xor i1 %38, true\n   %brmerge = select i1 %71, i1 true, i1 %.not208\n   br i1 %brmerge, label %oob8.loopexit, label %idxend9.us.us.postloop\n\nidxend9.us.us.postloop:                           ; preds = %idxend6.us.us.postloop\n; └\n;  @ In[68]:3 within `inner_noalloc!`\n; ┌ @ essentials.jl:14 within `getindex`\n   %72 = add i64 %31\n\n\n, %68\n   %73 = getelementptr inbounds double, double* %32, i64 %72\n   %74 = load double, double* %73, align 8\n   %75 = add i64 %34, %68\n   %76 = getelementptr inbounds double, double* %35, i64 %75\n   %77 = load double, double* %76, align 8\n; └\n; ┌ @ float.jl:408 within `+`\n   %78 = fadd double %74, %77\n; └\n;  @ In[68]:4 within `inner_noalloc!`\n; ┌ @ array.jl:971 within `setindex!`\n   %79 = add i64 %39, %68\n   %80 = getelementptr inbounds double, double* %40, i64 %79\n   store double %78, double* %80, align 8\n; └\n;  @ In[68]:5 within `inner_noalloc!`\n; ┌ @ range.jl:891 within `iterate`\n; │┌ @ promotion.jl:499 within `==`\n    %.not.us.us.postloop = icmp eq i64 %value_phi2.us.us.postloop, 100\n; │└\n   %81 = add nuw nsw i64 %value_phi2.us.us.postloop, 1\n; └\n  br i1 %.not.us.us.postloop, label %L25, label %L5.us.us.postloop\n}\n\n\nNotice that this getelementptr inbounds stuff is bounds checking. Julia, like all other high level languages, enables bounds checking by default in order to not allow the user to index outside of an array. Indexing outside of an array is dangerous: it can quite easily segfault your system if you change some memory that is unknown beyond your actual array. Thus Julia throws an error:\n\nA[101, 1]\n\nLoadError: BoundsError: attempt to access 100×100 Matrix{Float64} at index [101, 1]\n\n\nIn tight inner loops, we can remove this bounds checking process using the @inbounds macro:\n\nfunction inner_noalloc_ib!(C, A, B)\n  @inbounds for j in 1:100, i in 1:100\n    val = A[i, j] + B[i, j]\n    C[i, j] = val[1]\n  end\nend\n@btime inner_noalloc!(C, A, B)\n\n  7.716 μs (0 allocations: 0 bytes)\n\n\n\n@btime inner_noalloc_ib!(C, A, B)\n\n  2.026 μs (0 allocations: 0 bytes)\n\n\n\n\n1.3.2 SIMD\nNow let’s inspect the LLVM IR again:\n\n@code_llvm inner_noalloc_ib!(C, A, B)\n\n;  @ In[70]:1 within `inner_noalloc_ib!`\ndefine nonnull {}* @\"japi1_inner_noalloc_ib!_4098\"({}* %0, {}** noalias nocapture noundef readonly %1, i32 %2) #0 {\ntop:\n  %3 = alloca {}**, align 8\n  store volatile {}** %1, {}*** %3, align 8\n  %4 = load {}*, {}** %1, align 8\n  %5 = getelementptr inbounds {}*, {}** %1, i64 1\n  %6 = load {}*, {}** %5, align 8\n  %7 = getelementptr inbounds {}*, {}** %1, i64 2\n  %8 = load {}*, {}** %7, align 8\n  %9 = bitcast {}* %6 to {}**\n  %10 = getelementptr inbounds {}*, {}** %9, i64 3\n  %11 = bitcast {}** %10 to i64*\n  %12 = load i64, i64* %11, align 8\n  %13 = bitcast {}* %6 to double**\n  %14 = load double*, double** %13, align 8\n  %15 = bitcast {}* %8 to {}**\n  %16 = getelementptr inbounds {}*, {}** %15, i64 3\n  %17 = bitcast {}** %16 to i64*\n  %18 = load i64, i64* %17, align 8\n  %19 = bitcast {}* %8 to double**\n  %20 = load double*, double** %19, align 8\n  %21 = bitcast {}* %4 to {}**\n  %22 = getelementptr inbounds {}*, {}** %21, i64 3\n  %23 = bitcast {}** %22 to i64*\n  %24 = load i64, i64* %23, align 8\n  %25 = bitcast {}* %4 to double**\n  %26 = load double*, double** %25, align 8\n;  @ In[70]:2 within `inner_noalloc_ib!`\n  br label %L2\n\nL2:                                               ; preds = %L25, %top\n  %indvar = phi i64 [ %indvar.next, %L25 ], [ 0, %top ]\n  %value_phi = phi i64 [ %59, %L25 ], [ 1, %top ]\n  %27 = mul i64 %24, %indvar\n  %scevgep = getelementptr double, double* %26, i64 %27\n  %28 = add i64 %27, 100\n  %scevgep12 = getelementptr double, double* %26, i64 %28\n  %29 = mul i64 %12, %indvar\n  %30 = mul i64 %18, %indvar\n  %31 = add nsw i64 %value_phi, -1\n  %32 = mul i64 %12, %31\n  %33 = mul i64 %18, %31\n  %34 = mul i64 %24, %31\n  %35 = add i64 %30, 100\n  %scevgep20 = getelementptr double, double* %20, i64 %35\n  %scevgep18 = getelementptr double, double* %20, i64 %30\n  %36 = add i64 %29, 100\n  %scevgep16 = getelementptr double, double* %14, i64 %36\n  %scevgep14 = getelementptr double, double* %14, i64 %29\n  %bound0 = icmp ult double* %scevgep, %scevgep16\n  %bound1 = icmp ult double* %scevgep14, %scevgep12\n  %found.conflict = and i1 %bound0, %bound1\n  %bound022 = icmp ult double* %scevgep, %scevgep20\n  %bound123 = icmp ult double* %scevgep18, %scevgep12\n  %found.conflict24 = and i1 %bound022, %bound123\n  %conflict.rdx = or i1 %found.conflict, %found.conflict24\n  br i1 %conflict.rdx, label %L5, label %vector.body\n\nvector.body:                                      ; preds = %vector.body, %L2\n  %index = phi i64 [ %index.next, %vector.body ], [ 0, %L2 ]\n;  @ In[70]:3 within `inner_noalloc_ib!`\n; ┌ @ essentials.jl:14 within `getindex`\n   %37 = add i64 %index, %32\n   %38 = getelementptr inbounds double, double* %14, i64 %37\n   %39 = bitcast double* %38 to &lt;4 x double&gt;*\n   %wide.load = load &lt;4 x double&gt;, &lt;4 x double&gt;* %39, align 8\n   %40 = add i64 %index, %33\n   %41 = getelementptr inbounds double, double* %20, i64 %40\n   %42 = bitcast double* %41 to &lt;4 x double&gt;*\n   %wide.load25 = load &lt;4 x double&gt;, &lt;4 x double&gt;* %42, align 8\n; └\n; ┌ @ float.jl:408 within `+`\n   %43 = fadd &lt;4 x double&gt; %wide.load, %wide.load25\n; └\n;  @ In[70]:4 within `inner_noalloc_ib!`\n; ┌ @ array.jl:971 within `setindex!`\n   %44 = add i64 %index, %34\n   %45 = getelementptr inbounds double, double* %26, i64 %44\n   %46 = bitcast double* %45 to &lt;4 x double&gt;*\n   store &lt;4 x double&gt; %43, &lt;4 x double&gt;* %46, align 8\n   %index.next = add nuw i64 %index, 4\n   %47 = icmp eq i64 %index.next, 100\n   br i1 %47, label %L25, label %vector.body\n\nL5:                                               ; preds = %L5, %L2\n   %value_phi2 = phi i64 [ %58, %L5 ], [ 1, %L2 ]\n; └\n;  @ In[70]:3 within `inner_noalloc_ib!`\n; ┌ @ essentials.jl:14 within `getindex`\n   %48 = add nsw i64 %value_phi2, -1\n   %49 = add i64 %48, %32\n   %50 = getelementptr inbounds double, double\n\n\n* %14, i64 %49\n   %51 = load double, double* %50, align 8\n   %52 = add i64 %48, %33\n   %53 = getelementptr inbounds double, double* %20, i64 %52\n   %54 = load double, double* %53, align 8\n; └\n; ┌ @ float.jl:408 within `+`\n   %55 = fadd double %51, %54\n; └\n;  @ In[70]:4 within `inner_noalloc_ib!`\n; ┌ @ array.jl:971 within `setindex!`\n   %56 = add i64 %48, %34\n   %57 = getelementptr inbounds double, double* %26, i64 %56\n   store double %55, double* %57, align 8\n; └\n;  @ In[70]:5 within `inner_noalloc_ib!`\n; ┌ @ range.jl:891 within `iterate`\n; │┌ @ promotion.jl:499 within `==`\n    %.not.not = icmp eq i64 %value_phi2, 100\n; │└\n   %58 = add nuw nsw i64 %value_phi2, 1\n; └\n  br i1 %.not.not, label %L25, label %L5\n\nL25:                                              ; preds = %L5, %vector.body\n; ┌ @ range.jl:891 within `iterate`\n; │┌ @ promotion.jl:499 within `==`\n    %.not.not10 = icmp eq i64 %value_phi, 100\n; │└\n   %59 = add nuw nsw i64 %value_phi, 1\n; └\n  %indvar.next = add i64 %indvar, 1\n  br i1 %.not.not10, label %L36, label %L2\n\nL36:                                              ; preds = %L25\n  ret {}* inttoptr (i64 140594586316808 to {}*)\n}\n\n\nIf you look closely, you will see things like:\n%wide.load24 = load &lt;4 x double&gt;, &lt;4 x double&gt; addrspac(13)* %46, align 8\n; └\n; ┌ @ float.jl:395 within `+'\n%47 = fadd &lt;4 x double&gt; %wide.load, %wide.load24\nWhat this is saying is that it’s loading and adding 4 Float64s at a time! This feature of the processor is known as SIMD: single input multiple data. If certain primitive floating point operations, like + and *, are done in succession (i.e. no inbounds checks between them!), then the processor can lump them together and do multiples at once. Since clock cycles have stopped improving while transistors have gotten smaller, this “lumping” has been a big source of speedups in computational mathematics even though the actual + and * hasn’t gotten faster. Thus to get full speed we want to make sure this is utilized whenever possible, which essentially just amounts to doing type inferred loops with no branches or bounds checks in the way.\n\n\n1.3.3 FMA\nModern processors have a single operation that fuses the multiplication and the addition in the operation x*y+z, known as a fused multiply-add or FMA. Note that FMA has less floating point roundoff error than the two operation form. We can see this intrinsic in the resulting LLVM IR:\n\n@code_llvm fma(2.0, 5.0, 3.0)\n\n;  @ floatfuncs.jl:439 within `fma`\ndefine double @julia_fma_4099(double %0, double %1, double %2) #0 {\ncommon.ret:\n; ┌ @ floatfuncs.jl:434 within `fma_llvm`\n   %3 = call double @llvm.fma.f64(double %0, double %1, double %2)\n; └\n  ret double %3\n}\n\n\nThe Julia function muladd will automatically choose between FMA and the original form depending on the availability of the routine in the processor. The MuladdMacro.jl package has a macro @muladd which pulls apart statements to add muladd expressions. For example, x1 * y1 + x2 * y2 + x3 * y3 can be rewritten as:\nmuladd(x1, y1, muladd(x2, y2, x3 * y3))\nWhich reduces the linear combination to just 3 arithmetic operations. FMA operations can be SIMD’d.\n\n\n1.3.4 Inlining\nAll of this would go to waste if function call costs of 50 clock cycles were interrupting every single +. Fortunately these function calls disappear during the compilation process due to what’s known as inlining. Essentially, if the function call is determined to be “cheap enough”, the actual function call is removed and the code is basically pasted into the function caller. We can force a function call to occur by teling it to not inline:\n\n@noinline fnoinline(x, y) = x + y\nfinline(x, y) = x + y # Can add @inline, but this is automatic here\nfunction qinline(x, y)\n  a = 4\n  b = 2\n  c = finline(x, a)\n  d = finline(b, c)\n  finline(d, y)\nend\nfunction qnoinline(x, y)\n  a = 4\n  b = 2\n  c = fnoinline(x, a)\n  d = fnoinline(b, c)\n  fnoinline(d, y)\nend\n\nqnoinline (generic function with 1 method)\n\n\n\n@code_llvm qinline(1.0, 2.0)\n\n;  @ In[74]:3 within `qinline`\ndefine double @julia_qinline_4125(double %0, double %1) #0 {\ntop:\n;  @ In[74]:6 within `qinline`\n; ┌ @ In[74]:2 within `finline`\n; │┌ @ promotion.jl:410 within `+` @ float.jl:408\n    %2 = fadd double %0, 4.000000e+00\n; └└\n;  @ In[74]:7 within `qinline`\n; ┌ @ In[74]:2 within `finline`\n; │┌ @ promotion.jl:410 within `+` @ float.jl:408\n    %3 = fadd double %2, 2.000000e+00\n; └└\n;  @ In[74]:8 within `qinline`\n; ┌ @ In[74]:2 within `finline`\n; │┌ @ float.jl:408 within `+`\n    %4 = fadd double %3, %1\n; └└\n  ret double %4\n}\n\n\n\n@code_llvm qnoinline(1.0, 2.0)\n\n;  @ In[74]:10 within `qnoinline`\ndefine double @julia_qnoinline_4127(double %0, double %1) #0 {\ntop:\n;  @ In[74]:13 within `qnoinline`\n  %2 = call double @j_fnoinline_4129(double %0, i64 signext 4) #0\n;  @ In[74]:14 within `qnoinline`\n  %3 = call double @j_fnoinline_4130(i64 signext 2, double %2) #0\n;  @ In[74]:15 within `qnoinline`\n  %4 = call double @j_fnoinline_4131(double %3, double %1) #0\n  ret double %4\n}\n\n\nWe can see now that it keeps the function calls:\n%4 = call double @julia_fnoinline_21538(double %3, double %1)\nand this is slower in comparison to what we had before (but it still infers).\n\nx = 1.0\ny = 2.0\n@btime qinline(x, y)\n\n  16.967 ns (1 allocation: 16 bytes)\n\n\n9.0\n\n\n\n@btime qnoinline(x, y)\n\n  20.638 ns (1 allocation: 16 bytes)\n\n\n9.0\n\n\nNote that if we ever want to go the other direction and tell Julia to inline as much as possible, one can use the macro @inline.\n\n\n1.3.5 Summary\n\nScalar operations are super cheap, and if they are cache-aligned then more than one will occur in a clock cycle.\nInlining a function will remove the high function call overhead.\nBranch prediction is pretty good these days, so keep them out of super tight inner loops but don’t worry all too much about them.\nCache misses are quite expensive the further out it goes."
  },
  {
    "objectID": "optimize.html#note-on-benchmarking",
    "href": "optimize.html#note-on-benchmarking",
    "title": "1  Optimizing Serial Code",
    "section": "1.4 Note on Benchmarking",
    "text": "1.4 Note on Benchmarking\nJulia’s compiler is smart. This means that if you don’t try hard enough, Julia’s compiler might get rid of your issues. For example, it can delete branches and directly compute the result if all of the values are known at compile time. So be very careful when benchmarking: your tests may have just compiled away!\nNotice the following:\n\n@btime qinline(1.0, 2.0)\n\n  1.153 ns (0 allocations: 0 bytes)\n\n\n9.0\n\n\nDang, that’s much faster! But if you look into it, Julia’s compiler is actually “cheating” on this benchmark:\n\ncheat() = qinline(1.0, 2.0)\n@code_llvm cheat()\n\n;  @ In[80]:1 within `cheat`\ndefine double @julia_cheat_4159() #0 {\ntop:\n  ret double 9.000000e+00\n}\n\n\nIt realized that 1.0 and 2.0 are constants, so it did what’s known as constant propagation, and then used those constants inside of the function. It realized that the solution is always 9, so it compiled the function that… spits out 9! So it’s fast because it’s not computing anything. So be very careful about propagation of constants and literals. In general this is a very helpful feature, but when benchmarking this can cause some weird behavior. If a micro benchmark is taking less than a nanosecond, check and see if the compiler “fixed” your code!"
  },
  {
    "objectID": "optimize.html#conclusion",
    "href": "optimize.html#conclusion",
    "title": "1  Optimizing Serial Code",
    "section": "1.5 Conclusion",
    "text": "1.5 Conclusion\nOptimize your serial code before you parallelize. There’s a lot to think about."
  },
  {
    "objectID": "sciml.html#youtube-video-link-part-1",
    "href": "sciml.html#youtube-video-link-part-1",
    "title": "2  Introduction to Scientific Machine Learning through Physics-Informed Neural Networks",
    "section": "2.1 Youtube Video Link Part 1",
    "text": "2.1 Youtube Video Link Part 1"
  },
  {
    "objectID": "sciml.html#youtube-video-link-part-2",
    "href": "sciml.html#youtube-video-link-part-2",
    "title": "2  Introduction to Scientific Machine Learning through Physics-Informed Neural Networks",
    "section": "2.2 Youtube Video Link Part 2",
    "text": "2.2 Youtube Video Link Part 2\nHere we will start to dig into what scientific machine learning is all about by looking at physics-informed neural networks. Let’s start by understanding what a neural network really is, why they are used, and what kinds of problems that they solve, and then we will use this understanding of a neural network to see how to solve ordinary differential equations with neural networks. For there, we will use this method to regularize neural networks with physical equations, the aforementioned physics-informed neural network, and see how to define neural network architectures that satisfy physical constraints to improve the training process."
  },
  {
    "objectID": "sciml.html#getting-started-with-machine-learning-adding-flux",
    "href": "sciml.html#getting-started-with-machine-learning-adding-flux",
    "title": "2  Introduction to Scientific Machine Learning through Physics-Informed Neural Networks",
    "section": "2.3 Getting Started with Machine Learning: Adding Flux",
    "text": "2.3 Getting Started with Machine Learning: Adding Flux\nTo add Flux.jl we would do:\n\n]add Flux\n\nTo then use the package we will then use the using command:\n\nusing Flux\n\nIf you prefer to namespace all commands (like is normally done in Python, i.e. Flux.gradient instead of gradient), you can use the command:\n\nimport Flux\n\nNote that the installation and precompilation of these packages will occur at the add and first using phases, so they may take awhile (subsequent uses will utilize the precompiled form and take a lot less time!)"
  },
  {
    "objectID": "sciml.html#what-is-a-neural-network",
    "href": "sciml.html#what-is-a-neural-network",
    "title": "2  Introduction to Scientific Machine Learning through Physics-Informed Neural Networks",
    "section": "2.4 What is a Neural Network?",
    "text": "2.4 What is a Neural Network?\nA neural network is a function:\n\\[\n\\text{NN}(x) = W_3\\sigma_2(W_2\\sigma_1(W_1x + b_1) + b_2) + b_3\n\\]\nwhere we can change the number of layers ((W_i,b_i)) as necessary. Let’s assume we want to approximate some \\(R^{10} \\rightarrow R^5\\) function. To do this we need to make sure that we start with 10 inputs and arrive at 5 outputs. If we want a bigger middle layer for example, we can do something like (10,32,32,5). Size changing occurs at the site of the matrix multiplication, which means that we want a 32x10 matrix, then a 32x32 matrix, and finally a 5x32 matrix. This neural network would look like:\n\nW = [randn(32,10),randn(32,32),randn(5,32)]\nb = [zeros(32),zeros(32),zeros(5)]\n\n3-element Vector{Vector{Float64}}:\n [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0, 0.0, 0.0]\n\n\n\nsimpleNN(x) = W[3]*tanh.(W[2]*tanh.(W[1]*x + b[1]) + b[2]) + b[3]\nsimpleNN(rand(10))\n\n5-element Vector{Float64}:\n -1.7210598671315531\n -9.569085247363393\n  2.6347317388625733\n  4.766292831857366\n  5.229803142178678\n\n\nThis is our direct definition of a neural network. Notice that we choose to use tanh as our activation function between the layers.\n\n2.4.1 Defining Neural Networks with Flux.jl\nOne of the main deep learning libraries in Julia is Flux.jl. Flux is an interesting library for scientific machine learning because it is built on top of language-wide automatic differentiation libraries, giving rise to a programming paradigm known as differentiable programming, which means that one can write a program in a manner that it has easily accessible fast derivatives. However, due to being built on a differentiable programming base, the underlying functionality is simply standard Julia code,\nTo learn how to use the library, consult the documentation. A Google search will bring up the Flux.jl Github repository. From there, the blue link on the README brings you to the package documentation. This is common through Julia so it’s a good habit to learn!\nIn the documentation you will find that the way a neural network is defined is through a Chain of layers. A Dense layer is the kind we defined above, which is given by an input size, an output size, and an activation function. For example, the following recreates the neural network that we had above:\n\nusing Flux\nNN2 = Chain(Dense(10,32,tanh),\n           Dense(32,32,tanh),\n           Dense(32,5))\nNN2(rand(10))\n\n┌ Warning: Layer with Float32 parameters got Float64 input.\n│   The input will be converted, but any earlier layers may be very slow.\n│   layer = Dense(10 =&gt; 32, tanh)  # 352 parameters\n│   summary(x) = \"10-element Vector{Float64}\"\n└ @ Flux ~/.julia/packages/Flux/uCLgc/src/layers/stateless.jl:50\n\n\n5-element Vector{Float32}:\n -0.05766484\n  0.033816747\n -0.2755706\n  0.66105974\n -0.33304453\n\n\nNotice that Flux.jl as a library is written in pure Julia, which means that every piece of this syntax is just sugar over some Julia code that we can specialize ourselves (this is the advantage of having a language fast enough for the implementation of the library and the use of the library!)\nFor example, the activation function is just a scalar Julia function. If we wanted to replace it by something like the quadratic function, we can just use an anonymous function to define the scalar function we would like to use:\n\nNN3 = Chain(Dense(10,32,x-&gt;x^2),\n            Dense(32,32,x-&gt;max(0,x)),\n            Dense(32,5))\nNN3(rand(10))\n\n5-element Vector{Float32}:\n -0.5517122\n  0.29201278\n -0.18773921\n -0.54259044\n -0.056231104\n\n\nThe second activation function there is what’s known as a relu. A relu can be good to use because it’s an exceptionally operation and satisfies a form of the UAT. However, a downside is that its derivative is not continuous, which could impact the numerical properties of some algorithms, and thus it’s widely used throughout standard machine learning but we’ll see reasons why it may be disadvantageous in some cases in scientific machine learning.\n\n\n2.4.2 Digging into the Construction of a Neural Network Library\nAgain, as mentioned before, this neural network NN2 is simply a function:\n\nsimpleNN(x) = W[3]*tanh.(W[2]*tanh.(W[1]*x + b[1]) + b[2]) + b[3]\n\nsimpleNN (generic function with 1 method)\n\n\nLet’s dig into the library and see how that’s represented and really understand the construction of a deep learning library. First, let’s figure out where Dense comes from and what it does.\n\nusing InteractiveUtils\n@which Dense(10,32,tanh)\n\nDense(in::Integer, out::Integer, σ; kw...) in Flux at /home/jacob/.julia/packages/Flux/uCLgc/src/deprecations.jl:63\n\n\nIf we go to that spot of the documentation, we find the following.\n\nstruct Dense{F,S&lt;:AbstractArray,T&lt;:AbstractArray}\n  W::S\n  b::T\n  σ::F\nend\n\nfunction Dense(in::Integer, out::Integer, σ = identity;\n               initW = glorot_uniform, initb = zeros)\n  return Dense(initW(out, in), initb(out), σ)\nend\n\nfunction (a::Dense)(x::AbstractArray)\n  W, b, σ = a.W, a.b, a.σ\n  σ.(W*x .+ b)\nend\n\nFirst, Dense defines a struct in Julia. This struct just holds a weight matrix W, a bias vector b, and an activation function σ. The function called Dense is what’s known as an outer constructor which defines how the Dense type is built. If you give it two integers (and optionally an activation function which defaults to identity), then what it will do is take random initial W and b matrices (according to the glorot_uniform distribution for W and zeros for b), and then it will build the type with those matrices.\nThe last portion might be new. This is known as a callable struct, or a functor. It defines the dispatch for how calls work on the struct. As a quick demonstration, let’s define a type MyCallableStruct with a field x, and then make instances of MyCallableStruct be the function x+y:\n\nstruct MyCallableStruct\n  x\nend\n\n(a::MyCallableStruct)(y) = a.x+y\na = MyCallableStruct(2)\na(3)\n\n5\n\n\nIf you’re familiar with object-oriented programming, this is similar to using an object in a way that references the self, though it’s a bit more general due to allowing dispatching, i.e. this can then dependent on the input types as well.\nSo let’s look at that Dense call with this in mind:\n\nfunction (a::Dense)(x::AbstractArray)\n  W, b, σ = a.W, a.b, a.σ\n  σ.(W*x .+ b)\nend\n\nThis means that Dense is a function that takes in an x and computes σ.(W*x.+b), which is precisely how we defined the layer before! To see that this is just a function, let’s call it directly:\n\nf = Dense(32,32,tanh)\nf(rand(32))\n\n32-element Vector{Float32}:\n  0.39313397\n  0.7670685\n -0.32909486\n -0.32487372\n -0.40816873\n  0.31690586\n  0.05119981\n -0.31287754\n -0.0355136\n -0.51132494\n -0.16828296\n -0.41340718\n  0.68630517\n  ⋮\n  0.16023488\n  0.7362031\n -0.12175784\n  0.39460498\n -0.06559059\n -0.8664172\n  0.17417225\n -0.6453054\n -0.24554409\n  0.70283496\n  0.36583018\n -0.16298659\n\n\nSo okay, Dense objects are just functions that have weight and bias matrices inside of them. Now what does Chain do?\n\n@which Chain(1,2,3)\n\nChain(xs...) in Flux at /home/jacob/.julia/packages/Flux/uCLgc/src/layers/basic.jl:39\n\n\ngives us:\n\nstruct Chain{T&lt;:Tuple}\n  layers::T\n  Chain(xs...) = new{typeof(xs)}(xs)\nend\n\napplychain(::Tuple{}, x) = x\napplychain(fs::Tuple, x) = applychain(tail(fs), first(fs)(x))\n\n(c::Chain)(x) = applychain(c.layers, x)\n\nLet’s now dig into this. The ... is known that the slurp operator, which allows for “slurping up” multiple arguments into a single object xs. For example:\n\nslurper(xs...) = @show xs\nslurper(1,2,3,4,5)\n\nxs = (1, 2, 3, 4, 5)\n\n\n(1, 2, 3, 4, 5)\n\n\nWe see that slurps the inputs up into a Tuple, which is an immutable data store. (Note: Tuples are stack-allocated if inferred and is the internal data store of the compiler itself, and compiler inference can know exactly the size and the type of each individual object, so this does not have an overhead if fully inferred).\nThe function Chain(xs...) = new{typeof(xs)}(xs) is an inner constructor which builds a new instance of Chain where layers is a tuple of the inputs. This means that in our case where we put a bunch of Dense inside of there, layers is a tuple of functions. What does Chain do? Let’s look at its call:\n\n(c::Chain)(x) = applychain(c.layers, x)\n\nThis takes the tuple of functions and then does applychain on it.\n\napplychain(::Tuple{}, x) = x\napplychain(fs::Tuple, x) = applychain(tail(fs), first(fs)(x))\n\napplychain is a recursive function which applies the first element of the tuple onto x, then it calls applychain to call the second function onto x, repeatedly until there are no more functions in which case it returns x. This means that applychain is simply doing h(g(f(x))) on the tuple of functions (f,g,h)! We can thus see that this library function is exactly equivalent to the neural network we defined by hand, just put together in a different form to give a nice user interface.\n\n2.4.2.1 Detail: Recursion?\nBut there’s one more detail… why recursion? If you define a function, look at its type:\n\nff(x,y) = 2x+y\ntypeof(ff)\n\ntypeof(ff) (singleton type of function ff, subtype of Function)\n\n\nNotice that its type is simply typeof(ff) which is unique to the function, i.e. every single function is its own struct. In fact, given what we just learned, it wouldn’t be a surprise to learn that is exactly what a function is in Julia! A function definition lowers at the parser level to something like:\n\nstruct ff2 &lt;: Function end\n(_::ff2)(x,y) = 2x + y\nconst ff = ff2()\n\nThis means that the primitive operation here that everything really comes down to is calls on structs. Why is this done with unique singleton types? (Singleton types are types where every instance is equivalent). Well, if we want the compiler to be able to optimize with respect to which function we are handling inside of another function, then we need “what function we are dealing with” as compile-time information, which necessitates being type information.\nTuples are contravariant and heterogeneously typed with a parameter per internal object. For example:\n\ntup = (1.0,1,\"1\")\ntypeof(tup)\n\nTuple{Float64, Int64, String}\n\n\nThis means that it is possible to infer outputs of a tuple even if it’s heterogeneously typed by making good use of constant literals. For example, the expression tup[1] will be inferred to have the output Float64. However, note that if i is not a compile-time constant, then tup[i] cannot be inferred since, given what the compiler knows, the output could be either a Float64, an Int64, or a String.\nSo now let’s think back to our tuple of functions. By what we described before, tup = (f,g,h) is going to have a different type for each of the functions and thus could not specialize on the inputs if we used tup[i]. Therefore:\n\nfor i in 1:length(tup)\n  x = tup[i](x)\nend\n\nwould be slow (if the function call cost is small compared to the dispatch cost of about 100ns. This is not always the case, but should be considered in many instances!). So how can you get around it? Well, if everything was constant literals then this would specialize:\n\ntup[3](tup[2](tup[1](x)))\n\nwould fully specialize and infer, and the compiler would have full knowledge of the entire call chain as if it were written out as straightline code. Now if we look at the recursion again:\n\napplychain(::Tuple{}, x) = x\napplychain(fs::Tuple, x) = applychain(tail(fs), first(fs)(x))\n\nwe see that, at compile-time, we know that typeof((f,g,h)) = Tuple{typeof(f),typeof(g),typeof(h)}, and so we know that the first first(fs) will be f, and can specialize on this. We know then that tail(fs) has to be the (g,h) and so then we recurse and know that g is first and … This means that this scheme is equivalent to have written out xs[3](xs[2](xs[1](x))) and is thus generating code perfectly specialized to the order and amount of functions we had put into the Chain. This kind of abstraction, an abstraction where all of the overhead compiles away, is known as a zero-cost abstraction.\n(Note that technically, there is a cost since the compiler has to unravel this chain of events.)\n\n\n\n2.4.3 Training Neural Networks\nNow let’s get into training neural networks. “Training” a neural network is simply the process of finding weights that minimize a loss function. For example, let’s say we wanted to make our neural network be the constant function 1 for any input \\(x \\in [0,1]^{10}\\)``$. We can then write the loss function:\n\nNN = Chain(Dense(10,32,tanh),\n           Dense(32,32,tanh),\n           Dense(32,5))\nloss() = sum(abs2,sum(abs2,NN(rand(10)).-1) for i in 1:100)\nloss()\n\n3420.2385f0\n\n\nThis loss function takes 100 random points in [0,1] and then computes the output of the neural network minus 1 on each of the values, and sums up the squared values (abs2). Why the squared values? This means that every computed loss value is positive, and so we know that by decreasing the loss this means that, on average our neural network outputs are closer to 1. What are the weights? Since we’re using the Flux callable struct style from above, the weights are those inside of the NN chain object, which we can inspect:\n\nNN[1].weight # The W matrix of the first layer\n\n32×10 Matrix{Float32}:\n  0.323772     0.28754     -0.200479    …   0.0961688   0.0405403  -0.168476\n -0.0555998   -0.0692148   -0.0758221      -0.0676324   0.077987    0.293891\n -0.02769     -0.0352607    0.169824        0.0992985   0.0200803   0.108126\n  0.0662669   -0.0212718    0.376784       -0.104317   -0.284341    0.305189\n -0.00326861  -0.0874608   -0.0921368       0.0377347   0.109066   -0.0974935\n  0.161767    -0.346361    -0.05864     …   0.070015    0.125056   -0.185534\n  0.367458     0.00724132   0.123174       -0.053565   -0.246679   -0.317091\n  0.212817     0.134729    -0.285708       -0.367694    0.229678   -0.24274\n  0.308518     0.0748213    0.278651        0.317754   -0.228611    0.283016\n -0.295099     0.234957     0.30675         0.200965   -0.253512   -0.152979\n  0.238157    -0.0822664   -0.0547003   …   0.232812    0.0187168  -0.112703\n  0.31287      0.132575     0.0394817      -0.199089   -0.0337447  -0.0929872\n  0.187584    -0.0553328    0.082607        0.0621032   0.207435    0.319334\n  ⋮                                     ⋱                          \n -0.236805     0.22705     -0.226622    …  -0.356825   -0.327173   -0.0536365\n -0.0771046    0.202345    -0.0169084      -0.304076   -0.221288   -0.295218\n  0.0269688    0.0438642    0.0520709      -0.209454   -0.0635821   0.0232181\n -0.241062    -0.22366     -0.165364       -0.102993   -0.199284   -0.34879\n  0.248382    -0.0234489    0.307402       -0.335588   -0.114643    0.206917\n -0.13468      0.0486952    0.368867    …   0.357122    0.144467   -0.0301215\n  0.198812     0.0795738    0.242981        0.0597962  -0.353883    0.118905\n -0.110948     0.119228    -0.00307626     -0.335263   -0.0905137  -0.257492\n -0.0725719    0.177375     0.0436025       0.0504226   0.276618    0.331046\n  0.231007    -0.124937    -0.365684       -0.076196   -0.116909   -0.323134\n -0.352693    -0.271821    -0.327962    …   0.147852   -0.328142    0.236892\n -0.200936     0.35604     -0.0697275       0.258776   -0.114081    0.0189628\n\n\nNow let’s grab all of the parameters together:\n\np = Flux.params(NN)\n\nParams([Float32[0.3237716 0.2875403 … 0.040540285 -0.16847576; -0.055599824 -0.06921484 … 0.07798701 0.29389077; … ; -0.35269296 -0.2718212 … -0.32814246 0.2368922; -0.20093578 0.35604042 … -0.11408131 0.018962817], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.16695854 -0.16079274 … -0.040732373 0.2143003; 0.30143195 0.003008496 … 0.28958726 0.0680239; … ; 0.1567026 0.12379306 … -0.11915479 -0.03273462; -0.03165385 -0.08063845 … -0.050155494 -0.1795024], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.34187138 0.17039119 … -0.17708503 -0.08023286; -0.14614776 -0.19075489 … -0.22762091 -0.0338639; … ; 0.3601323 -0.39468125 … -0.3334246 0.3643005; 0.3074526 -0.12976453 … 0.36062053 0.14320275], Float32[0.0, 0.0, 0.0, 0.0, 0.0]])\n\n\nThat’s a helper function on Chain which recursively gathers all of the defining parameters. Let’s now find the optimal values p which cause the neural network to be the constant 1 function:\n\nFlux.train!(loss, p, Iterators.repeated((), 10000), ADAM(0.1))\n\nNow let’s check the loss:\n\nloss()\n\n7.650448f-6\n\n\nThis means that NN(x) is now a very good function approximator to f(x) = ones(5)!\n\n\n2.4.4 So Why Machine Learning? Why Neural Networks?\nAll we did was find parameters that made NN(x) act like a function f(x). How does that relate to machine learning? Well, in any case where one is acting on data (x,y), the idea is to assume that there exists some underlying mathematical model f(x) = y. If we had perfect knowledge of what f is, then from only the information of x we can then predict what y would be. The inference problem is to then figure out what function f should be. Therefore, machine learning on data is simply this problem of finding an approximator to some unknown function!\nSo why neural networks? Neural networks satisfy two properties. The first of which is known as the Universal Approximation Theorem (UAT), which in simple non-mathematical language means that, for any ϵ of accuracy, if your neural network is large enough (has enough layers, the weight matrices are large enough), then it can approximate any (nice) function f within that ϵ. Therefore, we can reduce the problem of finding missing functions, the problem of machine learning, to a problem of finding the weights of neural networks, which is a well-defined mathematical optimization problem.\nWhy neural networks specifically? That’s a fairly good question, since there are many other functions with this property. For example, you will have learned from analysis that \\(a_0 + a_1 x + a_2 x^2 + \\ldots\\) arbitrary polynomials can be used to approximate any analytic function (this is the Taylor series). Similarly, a Fourier series\n\\[\nf(x) = a_0 + \\sum_k b_k \\cos(kx) + c_k \\sin(kx)\n\\]\ncan approximate any continuous function f (and discontinuous functions also can have convergence, etc. these are the details of a harmonic analysis course).\nThat’s all for one dimension. How about two dimensional functions? It turns out it’s not difficult to prove that tensor products of universal approximators will give higher dimensional universal approximators. So for example, tensoring together two polynomials:\n\\[\na_0 + a_1 x + a_2 y + a_3 x y + a_4 x^2 y + a_5 x y^2 + a_6 x^2 y^2 + \\ldots\n\\]\nwill give a two-dimensional function approximator. But notice how we have to resolve every combination of terms. This means that if we used n coefficients in each dimension d, the total number of coefficients to build a d-dimensional universal approximator from one-dimensional objects would need \\(n^d\\) coefficients. This exponential growth is known as the curse of dimensionality.\nThe second property of neural networks that makes them applicable to machine learning is that they overcome the curse of dimensionality. The proofs in this area can be a little difficult to parse, but what they boil down to is proving in many cases that the growth of neural networks to sufficiently approximate a d-dimensional function grows as a polynomial of d, rather than exponential. This means that there’s some dimensional cutoff where for \\(d&gt;cutoff\\) it is more efficient to use a neural network. This can be problem-specific, but generally it tends to be the case at least by 8 or 10 dimensions.\nNeural networks have a few other properties to consider as well:\n\nThe assumptions of the neural network can be encoded into the neural architectures. A neural network where the last layer has an activation function x-&gt;x^2 is a neural network where all outputs are positive. This means that if you want to find a positive function, you can make the optimization easier by enforcing this constraint. A lot of other constraints can be enforced, like tanh activation functions can make the neural network be a smooth (all derivatives finite) function, or other activations can cause finite numbers of learnable discontinuities.\nGenerating higher dimensional forms from one dimensional forms does not have good symmetry. For example, the two-dimensional tensor Fourier basis does not have a good way to represent \\(sin(xy)\\). This property of the approximator is called (non)isotropy and more detail can be found in this wonderful talk about function approximation for multidimensional integration (cubature). Neural networks are naturally not aligned to a basis.\nNeural networks are “easy” to compute. There’s good software for them, GPU-acceleration, and all other kinds of tooling that make them particularly simple to use.\nThere are proofs that in many scenarios for neural networks the local minima are the global minima, meaning that local optimization is sufficient for training a neural network. Global optimization (which we will cover later in the course) is much more expensive than local methods like gradient descent, and thus this can be a good property to abuse for faster computation.\n\n\n\n2.4.5 From Machine Learning to Scientific Machine Learning: Structure and Science\nThis understanding of a neural network and their libraries directly bridges to the understanding of scientific machine learning and the computation done in the field. In scientific machine learning, neural networks and machine learning are used as the basis to solve problems in scientific computing. Scientific computing, as a discipline also known as Computational Science, is a field of study which focuses on scientific simulation, using tools such as differential equations to investigate physical, biological, and other phenomena.\nWhat we wish to do in scientific machine learning is use these properties of neural networks to improve the way that we investigate our scientific models.\n\n2.4.5.1 Aside: Why Differential Equations?\nWhy do differential equations come up so often in as the model in the scientific context? This is a deep question with quite a simple answer. Essentially, all scientific experiments always have to test how things change. For example, you take a system now, you change it, and your measurement is how the changes you made caused changes in the system. This boils down to gather information about how, for some arbitrary system \\(y = f(x)\\), how \\(\\Delta x\\) is related to \\(\\Delta y\\). Thus what you learn from scientific experiments, what is codified as scientific laws, is not “the answer”, but the answer to how things change. This process of writing down equations by describing how they change precisely gives differential equations."
  },
  {
    "objectID": "sciml.html#solving-odes-with-neural-networks-the-physics-informed-neural-network",
    "href": "sciml.html#solving-odes-with-neural-networks-the-physics-informed-neural-network",
    "title": "2  Introduction to Scientific Machine Learning through Physics-Informed Neural Networks",
    "section": "2.5 Solving ODEs with Neural Networks: The Physics-Informed Neural Network",
    "text": "2.5 Solving ODEs with Neural Networks: The Physics-Informed Neural Network\nNow let’s get to our first true SciML application: solving ordinary differential equations with neural networks. The process of solving a differential equation with a neural network, or using a differential equation as a regularizer in the loss function, is known as a physics-informed neural network, since this allows for physical equations to guide the training of the neural network in circumstances where data might be lacking.\n\n2.5.1 Background: A Method for Solving Ordinary Differential Equations with Neural Networks\nThis is a result first due to Lagaris et. al from 1998. The idea is to solve differential equations using neural networks by representing the solution by a neural network and training the resulting network to satisfy the conditions required by the differential equation.\nLet’s say we want to solve a system of ordinary differential equations\n\\[u' = f(u,t)\\]\nwith \\(t \\in [0,1]\\) and a known initial condition \\(u(0)=u_0\\). To solve this, we approximate the solution by a neural network:\n\\[NN(t) \\approx u(t)\\]\nIf \\(NN(t)\\) was the true solution, then it would hold that \\(NN'(t) = f(NN(t),t)\\) for all \\(t\\). Thus we turn this condition into our loss function. This motivates the loss function:\n\\[L(p) = \\sum_i \\left(\\frac{dNN(t_i)}{dt} - f(NN(t_i),t_i) \\right)^2\\]\nThe choice of \\(t_i\\) could be done in many ways: it can be random, it can be a grid, etc. Anyways, when this loss function is minimized (gradients computed with standard reverse-mode automatic differentiation), then we have that \\(\\frac{dNN(t_i)}{dt} \\approx f(NN(t_i),t_i)\\) and thus \\(NN(t)\\) approximately solves the differential equation.\nNote that we still have to handle the initial condition. One simple way to do this is to add an initial condition term to the cost function. This would look like:\n\\[L(p) = (NN(0) - u_0)^2 + \\sum_i \\left(\\frac{dNN(t_i)}{dt} - f(NN(t_i),t_i) \\right)^2\\]\nWhile that would work, it can be more efficient to encode the initial condition into the function itself so that it’s trivially satisfied for any possible set of parameters. For example, instead of directly using a neural network, we can use:\n\\[g(t) = u_0 + tNN(t)\\]\nas our solution. Notice that \\(g(t)\\) is thus a universal approximator for all continuous functions such that \\(g(0)=u_0\\) (this is a property one should prove!). Since \\(g(t)\\) will always satisfy the initial condition, we can train \\(g(t)\\) to satisfy the derivative function then it will automatically be a solution to the derivative function. In this sense, we can use the loss function:\n\\[L(p) = \\sum_i \\left(\\frac{dg(t_i)}{dt} - f(g(t_i),t_i) \\right)^2\\]\nwhere \\(p\\) are the parameters that define \\(g\\), which in turn are the parameters which define the neural network \\(NN\\) that define \\(g\\). Thus this reduces down, once again, to simply finding weights which minimize a loss function!\n\n\n2.5.2 Coding Up the Method\nNow let’s implement this method with Flux. Let’s define a neural network to be the NN(t) above. To make the problem easier, let’s look at the ODE:\n\\[u' = \\cos 2\\pi t\\]\nand approximate it with the neural network from a scalar to a scalar:\n\nNNODE = Chain(x -&gt; [x], # Take in a scalar and transform it into an array\n           Dense(1,32,tanh),\n           Dense(32,1),\n           first) # Take first value, i.e. return a scalar\nNNODE(1.0)\n\n0.31912556f0\n\n\nInstead of directly approximating the neural network, we will use the transformed equation that is forced to satisfy the boundary conditions. Using u0=1.0, we have the function:\n\ng(t) = t*NNODE(t) + 1f0\n\ng (generic function with 1 method)\n\n\nas our universal approximator. Thus, for this to be a function that satisfies\n\\[g' = \\cos 2\\pi t\\]\nwe would need that:\n\nusing Statistics\nϵ = sqrt(eps(Float32))\nloss() = mean(abs2(((g(t+ϵ)-g(t))/ϵ) - cos(2π*t)) for t in 0:1f-2:1f0)\n\nloss (generic function with 1 method)\n\n\nwould be minimized.\n\nopt = Flux.Descent(0.01)\ndata = Iterators.repeated((), 5000)\niter = 0\ncb = function () #callback function to observe training\n  global iter += 1\n  if iter % 500 == 0\n    display(loss())\n  end\nend\ndisplay(loss())\nFlux.train!(loss, Flux.params(NNODE), data, opt; cb=cb)\n\n0.6410512774437901\n\n\n0.48778666690059225\n\n\n0.4486697563946891\n\n\n0.33379656768224164\n\n\n0.13469618044306436\n\n\n0.0358528783063682\n\n\n0.020953148164407524\n\n\n0.016933953160683413\n\n\n0.015031374010579657\n\n\n0.013995999872137274\n\n\n0.013282741547353878\n\n\nHow well did this do? Well if we take the integral of both sides of our differential equation, we see it’s fairly trivial:\n\\[\n\\int g' = g = \\int \\cos 2\\pi t = C + \\frac{\\sin 2\\pi t}{2\\pi}\n\\]\nwhere we defined \\(C = 1\\). Let’s take a bunch of (input,output) pairs from the neural network and plot it against the analytical solution to the differential equation:\n\nusing Plots\nt = 0:0.001:1.0\nplot(t,g.(t),label=\"NN\")\nplot!(t,1.0 .+ sin.(2π.*t)/2π, label = \"True Solution\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe see that it matches very well, and we can keep improving this fit by increasing the size of the neural network, using more training points, and training for more iterations.\n\n\n2.5.3 Example: Harmonic Oscillator Informed Training\nUsing this idea, differential equations encoding physical laws can be utilized inside of loss functions for terms which we have some basis to believe should approximately follow some physical system. Let’s investigate this last step by looking at how to inform the training of a neural network using the harmonic oscillator.\nLet’s assume that we are taking measurements of (position,force) in some real one-dimensional spring pushing and pulling against a wall.\n\nBut instead of the simple spring, let’s assume we had a more complex spring, for example, let’s say \\(F(x) = -kx + 0.1sin(x)\\) where this extra term is due to some deformities in the medal (assume mass=1). Then by Newton’s law of motion we have a second order ordinary differential equation:\n\\[\nx'' = -kx + 0.1 \\sin(x)\n\\]\nWe can use the DifferentialEquations.jl package to solve this differential equation and see what this system looks like:\n\nusing DifferentialEquations\nk = 1.0\nforce(dx,x,k,t) = -k*x + 0.1sin(x)\nprob = SecondOrderODEProblem(force,1.0,0.0,(0.0,10.0),k)\nsol = solve(prob)\nplot(sol,label=[\"Velocity\" \"Position\"])\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDon’t worry if you don’t understand this sytnax yet: we will go over differential equation solvers and DifferentialEquations.jl in a later lecture.\nLet’s say we want to learn how to predict the force applied on the spring at each point in space, \\(F(x)\\). We want to learn a function, so this is the job for machine learning! However, we only have 6 measurements, which includes the information about (position,velocity,force) at evenly spaced times:\n\nplot_t = 0:0.01:10\ndata_plot = sol(plot_t)\npositions_plot = [state[2] for state in data_plot]\nforce_plot = [force(state[1],state[2],k,t) for state in data_plot]\n\n# Generate the dataset\nt = 0:3.3:10\ndataset = sol(t)\nposition_data = [state[2] for state in sol(t)]\nforce_data = [force(state[1],state[2],k,t) for state in sol(t)]\n\nplot(plot_t,force_plot,xlabel=\"t\",label=\"True Force\")\nscatter!(t,force_data,label=\"Force Measurements\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCan we train a neural network to approximate the expected force at any location for this spring? To see whether this is possible with a standard neural network, let’s just do it. Let’s define a neural network to be \\(F(x)\\) and see if we can learn the force function!\n\nNNForce = Chain(x -&gt; [x],\n           Dense(1,32,tanh),\n           Dense(32,1),\n           first)\n\n\nChain(\n  var\"#33#34\"(),\n  Dense(1 =&gt; 32, tanh),                 # 64 parameters\n  Dense(32 =&gt; 1),                       # 33 parameters\n  first,\n)                   # Total: 4 arrays, 97 parameters, 644 bytes.\n\n\n\nNow our loss function will be to match the force at the (position,force) pairs in the dataset:\n\nloss() = sum(abs2,NNForce(position_data[i]) - force_data[i] for i in 1:length(position_data))\nloss()\n\n0.004744252697395828\n\n\nOur random parameters do not do so well, so let’s train!\n\nopt = Flux.Descent(0.01)\ndata = Iterators.repeated((), 5000)\niter = 0\ncb = function () #callback function to observe training\n  global iter += 1\n  if iter % 500 == 0\n    display(loss())\n  end\nend\ndisplay(loss())\nFlux.train!(loss, Flux.params(NNForce), data, opt; cb=cb)\n\n0.004744252697395828\n\n\n0.0037711061053097295\n\n\n0.003235597533991824\n\n\n0.0027803635782711862\n\n\n0.0023917119575156505\n\n\n0.0020587897645440566\n\n\n0.0017728762825305578\n\n\n0.0015268629723057345\n\n\n0.0013148930814895529\n\n\n0.001132089528630795\n\n\n0.0009743537546267433\n\n\nThe neural network almost exactly matched the dataset, but how well did it actually learn the real force function? Let’s plot it to see:\n\nlearned_force_plot = NNForce.(positions_plot)\n\nplot(plot_t,force_plot,xlabel=\"t\",label=\"True Force\")\nplot!(plot_t,learned_force_plot,label=\"Predicted Force\")\nscatter!(t,force_data,label=\"Force Measurements\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOuch. The problem is that a neural network can approximate any function, so it approximated a function that fits the data, but not the correct function. We somehow need to have more data… but where can we get more data?\nWell, even a first year undergrad in physics will know Hooke’s law, which is that the idealized spring should satisfy \\(F(x) = -kx\\). This is a decent assumption for the evolution of the system:\n\nforce2(dx,x,k,t) = -k*x\nprob_simplified = SecondOrderODEProblem(force2,1.0,0.0,(0.0,10.0),k)\nsol_simplified = solve(prob_simplified)\nplot(sol,label=[\"Velocity\" \"Position\"])\nplot!(sol_simplified,label=[\"Velocity Simplified\" \"Position Simplified\"])\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhile it’s not quite correct, and it definitely drifts near the end, it should be a useful non-data assumption that we can add to improve the fitting. So, assuming we know \\(k\\) (this lab you probably have done before!), we can regularize this fitting by having a term that states our neural network should be the solution to the differential equation.\nThis term looks like what we had done before:\n\nrandom_positions = [2rand()-1 for i in 1:100] # random values in [-1,1]\nloss_ode() = sum(abs2,NNForce(x) - (-k*x) for x in random_positions)\nloss_ode()\n\n22.05418815960023\n\n\nIf this term is zero, then \\(F(x) = -kx\\), which is approximately true. So now let’s put these together:\n\nλ = 0.1\ncomposed_loss() = loss() + λ*loss_ode()\n\ncomposed_loss (generic function with 1 method)\n\n\nwhere \\(\\lambda\\) is some weight factor to control the regularization against the physics assumption. Now we can train the physics-informed neural network:\n\nopt = Flux.Descent(0.01)\ndata = Iterators.repeated((), 5000)\niter = 0\ncb = function () #callback function to observe training\n  global iter += 1\n  if iter % 500 == 0\n    display(composed_loss())\n  end\nend\ndisplay(composed_loss())\nFlux.train!(composed_loss, Flux.params(NNForce), data, opt; cb=cb)\n\nlearned_force_plot = NNForce.(positions_plot)\n\nplot(plot_t,force_plot,xlabel=\"t\",label=\"True Force\")\nplot!(plot_t,learned_force_plot,label=\"Predicted Force\")\nscatter!(t,force_data,label=\"Force Measurements\")\n\n2.2063931697146497\n\n\n0.0007113921180890465\n\n\n0.0006624832803633183\n\n\n0.0006229800305070401\n\n\n0.0005899748514703652\n\n\n0.0005616390753510758\n\n\n0.0005367926080946629\n\n\n0.0005146370373948024\n\n\n0.0004946273184367119\n\n\n0.00047637878645177383\n\n\n0.0004595993978301484\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd there we go: we have used knowledge of physics to help inform our neural network training process!"
  },
  {
    "objectID": "sciml.html#conclusion",
    "href": "sciml.html#conclusion",
    "title": "2  Introduction to Scientific Machine Learning through Physics-Informed Neural Networks",
    "section": "2.6 Conclusion",
    "text": "2.6 Conclusion\nIn this lecture we motivated machine learning not as a process of predicting from data but as a process for learning arbitrary nonlinear functions. Neural networks were just one choice of possible function. We then demonstrated how differential equations could be solved using this function approximation technique and then put together these two domains, solving differential equations and approximating data, into a single process to allow for physical knowledge to be embedded into the training process of a neural network, thus arriving at a physics-informed neural network. This is just one method in scientific machine learning which we will be exploring in more detail, demonstrating how we can utilize scientific knowledge to improve fits and allow for data-efficient machine learning."
  },
  {
    "objectID": "dynamical_systems.html#youtube-video-link-part-1",
    "href": "dynamical_systems.html#youtube-video-link-part-1",
    "title": "3  How Loops Work, An Introduction to Discrete Dynamics",
    "section": "3.1 Youtube Video Link Part 1",
    "text": "3.1 Youtube Video Link Part 1"
  },
  {
    "objectID": "dynamical_systems.html#youtube-video-link-part-2",
    "href": "dynamical_systems.html#youtube-video-link-part-2",
    "title": "3  How Loops Work, An Introduction to Discrete Dynamics",
    "section": "3.2 Youtube Video Link Part 2",
    "text": "3.2 Youtube Video Link Part 2\nAs we saw with the physics-informed neural networks, the basics of most scientific models are dynamical systems. Thus if we want to start to dig into deeper methods, we will need to start looking into the theory and practice of nonlinear dynamical systems. In this lecture we will go over the basic properties of dynamical systems and understand their general behavior through code. We will also learn the idea of stability as an asymptotic property of a mapping, and understand when a system is stable."
  },
  {
    "objectID": "dynamical_systems.html#discrete-dynamical-systems",
    "href": "dynamical_systems.html#discrete-dynamical-systems",
    "title": "3  How Loops Work, An Introduction to Discrete Dynamics",
    "section": "3.3 Discrete Dynamical Systems",
    "text": "3.3 Discrete Dynamical Systems\nA discrete dynamical system is a system which updates through discrete updates:\n\\[u_{n+1} = model(u_n,n)\\]\nThere are many examples of a discrete dynamical system found throughout the scientific literature. For example, many ecological models are discrete dynamical systems, with the most famous being the logistic map:\n\\[u_{n+1} = r u_n (1 - u_n)\\]\ndescribing the growth of a population with a carrying capacity of 1 and a growth rate of r. Another way in which discrete dynamical systems are often encountered is through time series models. These are generally seen in financial forecasting and For example, the autoregressive model AR1 is the following linear dynamical system:\n\\[u_{n+1} = \\alpha u_n + \\epsilon_n\\]\nwhere \\(\\epsilon\\) is a standard normal random number. The AR(k) model allows itself to update using delays as well:\n\\[u_{n+1} = \\sum_{j=0}^{k-1} \\alpha_j u_{n-j} + \\epsilon_n\\]\nThe ARMA model is one that allows using delays on the randomness as well:\n\\[u_{n+1} = \\sum_{j=0}^{k-1} (\\alpha_j u_{n-j}  + \\beta_j \\epsilon_{n-j})\\]\nAnother embodiment of a discrete dynamical system is a Recurrent Neural Network (RNN). In its simplest form, a RNN is a system of the form:\n\\[u_{n+1} = u_n + f(u_n,\\theta)\\]\nwhere \\(f\\) is a neural network parameterized by \\(\\theta\\).\nNote that discrete dynamical systems are even more fundamental than just the ones shown. In any case where a continuous model is discretized to loop on the computer, the resulting algorithm is a discrete dynamical system and thus evolves according to its properties. This fact will be revisited later."
  },
  {
    "objectID": "dynamical_systems.html#properties-of-linear-dynamical-systems",
    "href": "dynamical_systems.html#properties-of-linear-dynamical-systems",
    "title": "3  How Loops Work, An Introduction to Discrete Dynamics",
    "section": "3.4 Properties of Linear Dynamical Systems",
    "text": "3.4 Properties of Linear Dynamical Systems\nFirst let’s take a look at the scalar linear dynamical system:\n\\[u_{n+1} = \\alpha u_{n}\\]\nWe want to ask what the global or geometric behavior of this system is. We can do this by expanding out the system. Notice that if \\(u_0\\) is known, then\n\\[u_{n+1} = \\alpha^n u_0\\]\nThe global behavior can then be categorized as:\n\nIf \\(\\Vert \\alpha \\Vert &lt; 1\\), then \\(u_n \\rightarrow 0\\)\nIf \\(\\Vert \\alpha \\Vert &gt; 1\\), then \\(u_n \\rightarrow \\infty\\)\n\nIf \\(\\Vert \\alpha \\Vert = 1\\), then \\(u_n \\rightarrow u_0\\) if everything is in the real numbers, but more complex dynamics can occur on the complex plane."
  },
  {
    "objectID": "dynamical_systems.html#nonlinear-geometric-dynamics",
    "href": "dynamical_systems.html#nonlinear-geometric-dynamics",
    "title": "3  How Loops Work, An Introduction to Discrete Dynamics",
    "section": "3.5 Nonlinear Geometric Dynamics",
    "text": "3.5 Nonlinear Geometric Dynamics\nThe Geometric Theory of Dynamical Systems is the investigation of their long-term properties and the geometry of the phase space which they occupy. Let’s start looking at this in practical terms: how do nonlinear update equations act as time goes to infinity?\n\n3.5.0.1 Banach Fixed Point Theorem\nThere are surprisingly simple results that we can prove. First let’s recall the Banach Fixed Point Theorem (also known as the Contraction Mapping Theorem). Let \\((X,d)\\) be a metric space (\\(X\\) is the set of points we are thinking of, here the real numbers. \\(d\\) is a distance function). \\(f\\) is a contraction mapping if\n\\[d(f(x),f(y)) \\leq q d(x,y)\\]\nwhere \\(q &lt; 1\\), that is, if applying \\(f\\) always decreases the distance. The theorem then states that if \\(f\\) is a contraction mapping, then there is a unique fixed point (point \\(x^\\ast\\) where \\(f(x^\\ast)=x^\\ast\\)) and a sequence such that \\(x_0 \\rightarrow x^\\ast\\) where\n\\[x_{n+1} = f(x_n)\\]\nThe proof is by induction, showing that the sequence is Cauchy. For some \\(m&gt;n\\) we do by the Triangle Inequality\n\\[d(x_m,x_n) \\leq d(x_{m},x_{m-1}) + \\ldots + d(x_{n+1},x_n)\\]\nthen apply the contraction relation down to the bottom:\n\\[d(x_m,x_n) \\leq q^{m-1} d(x_{1},x_{0}) + \\ldots + q^{n} d(x_{1},x_0)\\]\n\\[d(x_m,x_n) \\leq q^n d(x_{1},x_{0}) \\sum_{k=0}^{m-n-1} q^k\\]\nand since adding more never hurts:\n\\[d(x_m,x_n) \\leq q^n d(x_{1},x_{0}) \\sum_{k=0}^{\\infty} q^k\\]\nBut that summation is just a geometric series now, and since \\(q&lt;1\\) we know it converges to \\(1/(1-q)\\), and so we get:\n\\[d(x_m,x_n) \\leq \\frac{q^n}{1-q} d(x_{1},x_{0})\\]\nThe coefficient converges to zero as \\(n\\) increases, and so the sequence must be Cauchy, which implies there’s a unique fixed point.\n\n\n3.5.0.2 Stability of Linear Discrete Dynamical Systems\nNow let’s take a mapping \\(f\\) which is sufficiently nice (\\(f \\in C^1\\), i.e. the derivative of \\(f\\) exists and is continuous), where\n\\[x_{n+1} = f(x_n)\\]\nAssume that \\(\\Vert f^\\prime (x^\\ast) \\Vert &lt; 1\\) at some point where \\(f(x)=x\\). Then by continuity of the second derivative, it follows that there is a neighborhood where \\(\\Vert f^\\prime (x) \\Vert &lt; 1\\) (). Now recall that this means\n\\[\\frac{df}{dx} \\leq 1\\]\nwhich means that, for any \\(x\\) and \\(y\\) in the neighborhood,\n\\[\\Vert \\frac{f(y)-f(x)}{y-x} \\Vert \\leq 1\\]\nor\n\\[\\Vert f(y)-f(x) \\Vert \\leq \\Vert y-x \\Vert\\]\nThis is essentially another way of saying that a function that is differentiable is Lipschitz, where we can use the derivative as the Lipschitz bound. But notice this means that, in this neighborhood, a function with a derivative less than 1 is a contraction mapping, and thus there is a limiting sequence which goes to the fixed point by the Banach Fixed Point Theorem. Furthermore, the uniqueness guarantees that there is only one fixed point in a sufficiently small neighborhood where the derivative is all less than 1.\nA way to interpret this result is that, any nice enough function \\(f\\) is locally linear. Thus we can understand the global properties of \\(f\\) by looking the linearization of its dynamics, where the best linear approximation is the linear function \\(f^\\prime (x) x\\). This means that we can think of\n\\[x_{n+1} = f(x_n)\\]\nlocally as being approximated by\n\\[x_{n+1} = f^\\prime (x) x_n\\]\nand so if the derivative is less than 1 in some neighborhood of a fixed point, then we locally have a linear dynamical system which looks like the simple \\(x_{n+1} = \\alpha x_n\\) where \\(\\alpha &lt;1\\), and so we get the same convergence property.\nThis is termed “stability” since, if you are a little bit off from the fixed point, you will tend to go right back to it. An unstable fixed point is one where you fall away. And what happens when the derivative is one? There are various forms of semi-stability that can be proved which go beyond the topic of this course.\n\n\n3.5.0.3 Update Form\nNow let’s look at another form:\n\\[x_{n+1} = x_n + f(x_n)\\]\nFor example, this is what we generally see with the recurrent neural network (or, as we will find out later, this is how discretizations of continuous systems tend to look!). In this case, we can say that this is a dynamical system\n\\[x_{n+1} = g(x_n)\\]\nand so if \\(-2 &lt; f^\\prime &lt; 0\\), then \\(\\Vert g^\\prime \\Vert = \\Vert 1 + f^\\prime \\Vert &lt; 1\\) and so we have the same stability idea except now with a condition shifted to zero instead of one."
  },
  {
    "objectID": "dynamical_systems.html#multivariable-systems",
    "href": "dynamical_systems.html#multivariable-systems",
    "title": "3  How Loops Work, An Introduction to Discrete Dynamics",
    "section": "3.6 Multivariable Systems",
    "text": "3.6 Multivariable Systems\nNow let \\(x \\in R^k\\) be a vector, and define discrete mappings:\n\\[x_{n+1} = f(x_n)\\]\nTo visualize this, let’s write out the version for \\(x \\in R^3\\):\n\\[x_{n+1}=\\left[\\begin{array}{c}\na_{n+1}\\\\\nb_{n+1}\\\\\nc_{n+1}\n\\end{array}\\right]=\\left[\\begin{array}{c}\nf_{1}(a_{n},b_{n},c_{n})\\\\\nf_{2}(a_{n},b_{n},c_{n})\\\\\nf_{3}(a_{n},b_{n},c_{n})\n\\end{array}\\right]=f(x_{n})\\]\nThe linear multidimensional discrete dynamical system is:\n\\[x_{n+1} = A x_n\\]\nThe easiest way to analyze a multidimensional system is to turn it into a bunch of single dimension systems. To do this, assume that \\(A\\) is diagonalizable. This means that there exists a diagonalization \\(A =P^{-1}DP\\) where \\(P\\) is the matrix of eigenvectors and \\(D\\) is the diagonal matrix of eigenvalues. We can then decompose the system as follows:\n\\[Px_{n+1} = DPx_n\\]\nand now define new variables \\(z_n = Px_n\\). In these variables,\n\\[z_{n+1} = D z_n\\]\nbut \\(D\\) is diagonal, so this is a system of \\(k\\) independent linear dynamical systems. We know that the linear dynamical system will converge to zero if \\(\\Vert D_i \\Vert &lt; 1\\), and so this means that \\(z_n\\) converges to zero if all of the eigenvalues are within the unit circle. Since \\(P0 = 0\\), this implies that if all of the eigenvalues of \\(A\\) are in the unit circle, then \\(x_n \\rightarrow 0\\).\nA multidimensional version of the contraction mapping theorem is then proven exactly in this manner, meaning that if \\(f(x) = x\\) and all eigenvalues of the Jacobian matrix (the linearization of \\(f\\)) are in the unit circle, then \\(x\\) is a unique fixed point in some neighborhood.\n\n3.6.0.1 Understanding Delayed Systems\nA similar property holds in linear dynamical systems with delays. Take\n\\[x_{n+1} = \\sum_{j=0}^{k-1} \\alpha_j x_{n-j}\\]\nNotice that we can write this as a multidimensional non-delayed system. Let \\(x_n^i\\) be the \\(i\\)th term in the vector of the \\(n\\) time. Then we have:\n\\[x_{n+1}^1 = \\sum_{j=1}^{k-1} \\alpha_{j-1} x_{n}^{j}\\]\nas an equivalent way to write this, where\n\\[x_{n+1}^j = x_n^{j-1}\\]\nfor all of the other terms. Essentially, instead of a system with a delay, we store the memory in other terms of the vector, and keep shifting them down. However, this makes our system much easier to analyze. Instead of a linear delayed dynamical system, this is now a linear multidimensional dynamical system. Its characteristic polynomial is\n\\[\\varphi(x) = 1 - \\sum_{j=0}^{k-1} \\alpha_j x^j\\]\nand so if all of the roots are in the unit circle then this system is stable.\n\n\n3.6.0.2 Stochastic Dynamical Systems\nNow let’s take a look again at the autoregressive process from time series analysis:\n\\[u_{n+1} = \\sum_{j=0}^{k-1} \\alpha_j u_{n-j} + \\epsilon_n\\]\nIn a very quick handwavy way, we can understand such a system by seeing how the perturbations propagate. If \\(u_0 = 0\\), then the starting is just \\(\\epsilon_0\\). If we assume all other \\(\\epsilon_i = 0\\), then this system is the same as a linear dynamical system with delays. If all of the roots are in the unit circle, then it goes to zero, meaning the perturbation is forgotten or squashed over time.\nWe can analyze this more by using the moments. Notice that, by the linearity of the expected value,\n\\[\\mathbb{E}[u_{n+1}] = \\sum_{j=0}^{k-1} \\alpha_j \\mathbb{E}[u_{n-j}]\\]\nis a deterministic linear dynamical system which converges if the roots are in the unit circle. This means that the mean stabilizes over time if all of the roots are in the unit circle. In time series analysis, this is called stationarity of the time series.\nWe can then also look at the stability of the variance as well. Recall that\n\\[\\mathbb{V}[x] = \\mathbb{E}[x^2] - \\mathbb{E}[x]^2\\]\nand so therefore\n\\[\\mathbb{E}[u_{n+1}^2] = \\mathbb{E}[\\sum_{j=0}^{k-1} \\alpha_j u_{n-j}^2]\\]\nand with a bunch of analysis here, working in the same way with the same basic ideas, we can determine conditions on which the variance goes to zero."
  },
  {
    "objectID": "dynamical_systems.html#periodicity-and-chaos",
    "href": "dynamical_systems.html#periodicity-and-chaos",
    "title": "3  How Loops Work, An Introduction to Discrete Dynamics",
    "section": "3.7 Periodicity and Chaos",
    "text": "3.7 Periodicity and Chaos\nStability is the simplest geometric dynamical property, but there are many others. For example, maps can also have periodic orbits, like:\n\\[u_{n+1} = -u_n\\]\nwill bounce back and forth between two values. These periodic orbits themselves have geometric properties, such as whether it’s a stable periodic orbit (points nearby are attracted to the periodic orbit). Periodic orbits have a length as well: this was a periodic orbit of length 2.\nChaos is another interesting property of a discrete dynamical system. It can be interpreted as a periodic orbit where the length is infinity. This can happen if, by changing a parameter, a period 2 orbit becomes a period 4, then a period 8, etc. (a phenomenon known as period doubling), and when it goes beyond the accumulation point the “infinite period orbit” is reached and chaos is found. A homework problem will delve into the properties of chaos as an example of a simple embarrassingly data-parallel problem."
  },
  {
    "objectID": "dynamical_systems.html#efficient-implementation-of-dynamical-systems",
    "href": "dynamical_systems.html#efficient-implementation-of-dynamical-systems",
    "title": "3  How Loops Work, An Introduction to Discrete Dynamics",
    "section": "3.8 Efficient Implementation of Dynamical Systems",
    "text": "3.8 Efficient Implementation of Dynamical Systems\nDynamical systems are just loops, so the implementation is easy to understand. However, there are a few things that one must keep in mind in order to allow for efficient implementations.\n\n3.8.0.1 Higher order functions\nFunctions which compute the solutions to dynamical systems are inherently higher order functions, which means it’s a function which takes in a function as an argument. The following is a quick implementation:\n\n\"\"\"\n`solve_system(f,u0,n)`\n\nSolves the dynamical system\n\n``u_{n+1} = f(u_n)``\n\nfor N steps. Returns the solution at step `n` with parameters `p`.\n\n\"\"\"\nfunction solve_system(f,u0,p,n)\n  u = u0\n  for i in 1:n-1\n    u = f(u,p)\n  end\n  u\nend\n\nsolve_system\n\n\nNotice the \"\"\" before the function: this is a docstring. When the Julia REPL is queried with ?solve_system this will be the description that is displayed.\nNow, is this function going to be efficient? Recall from the earlier discussion that:\n\nType-stability is necessary for inference to carry forward type information.\nJulia auto-specializes on input types.\nInlining can occur automatically for sufficiently small functions.\n\nFrom this information, we know that in order for this to be efficient, we require that the type of f(u) is inferred. But if f is a variable, how can that be inferred? In order for that to occur, we would have to know what f is, since not all functions will give the same output type. Additionally, in order to inline the function, we will have to know what the function is at compile-time. So, is it possible to make this implementation efficient?\nIt turns out that this does optimize due to one fact: every function is given by its own type. We can verify this by defining a function and checking its type:\n\nf(u,p) = u^2 - p*u\ntypeof(f)\n\ntypeof(f) (singleton type of function f, subtype of Function)\n\n\nIt displays typeof(f) to indicate that the function f is its own type, and thus at automatic specialization time the compiler knows all of the information about the function and thus inlines and performs inference correctly.\nNote that this does mean that the function will need to recompile for every new f. This is similar to statically compiling a function for use in a C/Fortran library. What is the equivalent to using a function like a shared library or a shared object? This is given by FunctionWrappers.jl. This directly stores the function pointer in an object that can have shared type information in order to keep every function as the same type. However, the wrapped function has more information about the pointer… what’s necessary?\nThe answer is that FunctionWrappers.jl allows for specifying the input and output types, in order for the wrapper to do the right assertions for inference to carry forward type stability, since in this case inference is not able to step through the function pointer.\n\n\n3.8.0.2 Quick Check\nWhat will approximately be the value of this dynamical system after 1000 steps if you start at 1.0 with parameter p=0.25? Can you guess without solving the system? Think about steady states and stability.\n\nsolve_system(f,1.0,0.25,1000)\n\n0.0\n\n\nThe answer is that it goes to zero. The steady states are the zeros of the polynomial, which are 0 and p+1. It’s reasonable to believe that it either goes to one of those 2 values or infinity. In the first step, 1^2 - 0.25 = 0.75 &lt; 1 which suggests (but doesn’t confirm!) that it’s a contraction. Notice that the derivative is 2u-p, and so u=p+1=1.25 is not a stable steady state, and thus we go to zero. In fact, we can check a few values:\n\nsolve_system(f,1.1,0.25,1000)\n\n0.0\n\n\n\nsolve_system(f,1.22,0.25,1000)\n\n0.0\n\n\n\nsolve_system(f,1.25,0.25,1000)\n\n1.25\n\n\n\nsolve_system(f,1.251,0.25,20)\n\nInf\n\n\nNotice that the moment we go above the steady state p+1, we exponentially grow to infinity.\nJust to double check the implementation:\n\nsolve_system(f,1.251,0.25,10)\nsolve_system(f,1.251,0.25,100)\nsolve_system(f,1.251,0.25,1000)\n\nNaN\n\n\nThose allocations are just the output, and notice it’s independent of the loop count.\n\n\n3.8.0.3 Multidimensional System Implementations\nWhen we go to multidimensional systems, some care needs to be taken to decrease the number of allocations which are occurring . One of the ways to do this is to utilize statically sized arrays. For example, let’s look at a discretization of the Lorenz system:\n\nfunction lorenz(u,p)\n  α,σ,ρ,β = p\n  du1 = u[1] + α*(σ*(u[2]-u[1]))\n  du2 = u[2] + α*(u[1]*(ρ-u[3]) - u[2])\n  du3 = u[3] + α*(u[1]*u[2] - β*u[3])\n  [du1,du2,du3]\nend\np = (0.02,10.0,28.0,8/3)\nsolve_system(lorenz,[1.0,0.0,0.0],p,1000)\n\n3-element Vector{Float64}:\n  1.4744010677851374\n  0.8530017039412324\n 20.62004063423844\n\n\nLet’s see what this gives us by saving:\n\nfunction solve_system_save(f,u0,p,n)\n  u = Vector{typeof(u0)}(undef,n)\n  u[1] = u0\n  for i in 1:n-1\n    u[i+1] = f(u[i],p)\n  end\n  u\nend\nto_plot = solve_system_save(lorenz,[1.0,0.0,0.0],p,1000)\n\n1000-element Vector{Vector{Float64}}:\n [1.0, 0.0, 0.0]\n [0.8, 0.56, 0.0]\n [0.752, 0.9968000000000001, 0.008960000000000001]\n [0.80096, 1.3978492416000001, 0.023474005333333336]\n [0.92033784832, 1.8180538219817644, 0.04461448495326095]\n [1.099881043052353, 2.296260732619613, 0.07569952060880669]\n [1.339156980965805, 2.864603692722823, 0.12217448583728006]\n [1.6442463233172087, 3.5539673118971193, 0.19238159391549564]\n [2.026190521033191, 4.397339452147425, 0.2989931959555302]\n [2.5004203072560376, 5.431943011293093, 0.4612438424853632]\n [3.0867248480634486, 6.700473453723668, 0.7082869831520391]\n [3.8094745691954923, 8.25130415895562, 1.0841620354518975]\n [4.697840487147518, 10.136982080467158, 1.655002727352565]\n ⋮\n [10.49730559336705, 4.660822889495989, 35.336831929448614]\n [9.330009052592839, 3.0272670946941713, 34.430722536963344]\n [8.069460661013105, 1.766747763108672, 33.159305922954225]\n [6.8089180814322185, 0.8987564841782779, 31.6759436385101]\n [5.6268857619814305, 0.3801973723631693, 30.108951163308078]\n [4.577548084057778, 0.13525687944525802, 28.545926978224173]\n [3.6890898431352737, 0.08257160199224252, 27.035860436772758]\n [2.9677861949066675, 0.15205611935372762, 25.600040161309696]\n [2.4046401797960795, 0.2914663505185634, 24.24373008707723]\n [1.9820054139405763, 0.46628657468365653, 22.964748583050085]\n [1.6788616460891923, 0.6565587545689172, 21.758445642263496]\n [1.4744010677851374, 0.8530017039412324, 20.62004063423844]\n\n\n\nusing Plots\nx = [to_plot[i][1] for i in 1:length(to_plot)]\ny = [to_plot[i][2] for i in 1:length(to_plot)]\nz = [to_plot[i][3] for i in 1:length(to_plot)]\nplot(x,y,z)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is the chaotic Lorenz attractor plotted in phase space, i.e. the values of the variables against each other.\nLet’s look at the implementation a little bit more. u = Vector{typeof(u0)}(undef,n) is type-generic, meaning any u0 can be used with that code. However, as a vector of vectors, it is a vector of pointers to contiguous memory, instead of being contiguous itself. Note that that means there is not much of a cost by not pre-specifying the size up front:\n\nfunction solve_system_save_push(f,u0,p,n)\n  u = Vector{typeof(u0)}(undef,1)\n  u[1] = u0\n  for i in 1:n-1\n    push!(u,f(u[i],p))\n  end\n  u\nend\n@time solve_system_save(lorenz,[1.0,0.0,0.0],p,1000)\n\n  0.000058 seconds (1.00 k allocations: 86.062 KiB)\n\n\n1000-element Vector{Vector{Float64}}:\n [1.0, 0.0, 0.0]\n [0.8, 0.56, 0.0]\n [0.752, 0.9968000000000001, 0.008960000000000001]\n [0.80096, 1.3978492416000001, 0.023474005333333336]\n [0.92033784832, 1.8180538219817644, 0.04461448495326095]\n [1.099881043052353, 2.296260732619613, 0.07569952060880669]\n [1.339156980965805, 2.864603692722823, 0.12217448583728006]\n [1.6442463233172087, 3.5539673118971193, 0.19238159391549564]\n [2.026190521033191, 4.397339452147425, 0.2989931959555302]\n [2.5004203072560376, 5.431943011293093, 0.4612438424853632]\n [3.0867248480634486, 6.700473453723668, 0.7082869831520391]\n [3.8094745691954923, 8.25130415895562, 1.0841620354518975]\n [4.697840487147518, 10.136982080467158, 1.655002727352565]\n ⋮\n [10.49730559336705, 4.660822889495989, 35.336831929448614]\n [9.330009052592839, 3.0272670946941713, 34.430722536963344]\n [8.069460661013105, 1.766747763108672, 33.159305922954225]\n [6.8089180814322185, 0.8987564841782779, 31.6759436385101]\n [5.6268857619814305, 0.3801973723631693, 30.108951163308078]\n [4.577548084057778, 0.13525687944525802, 28.545926978224173]\n [3.6890898431352737, 0.08257160199224252, 27.035860436772758]\n [2.9677861949066675, 0.15205611935372762, 25.600040161309696]\n [2.4046401797960795, 0.2914663505185634, 24.24373008707723]\n [1.9820054139405763, 0.46628657468365653, 22.964748583050085]\n [1.6788616460891923, 0.6565587545689172, 21.758445642263496]\n [1.4744010677851374, 0.8530017039412324, 20.62004063423844]\n\n\n\n@time solve_system_save_push(lorenz,[1.0,0.0,0.0],p,1000)\n\n  0.012756 seconds (7.48 k allocations: 538.761 KiB, 99.50% compilation time)\n\n\n1000-element Vector{Vector{Float64}}:\n [1.0, 0.0, 0.0]\n [0.8, 0.56, 0.0]\n [0.752, 0.9968000000000001, 0.008960000000000001]\n [0.80096, 1.3978492416000001, 0.023474005333333336]\n [0.92033784832, 1.8180538219817644, 0.04461448495326095]\n [1.099881043052353, 2.296260732619613, 0.07569952060880669]\n [1.339156980965805, 2.864603692722823, 0.12217448583728006]\n [1.6442463233172087, 3.5539673118971193, 0.19238159391549564]\n [2.026190521033191, 4.397339452147425, 0.2989931959555302]\n [2.5004203072560376, 5.431943011293093, 0.4612438424853632]\n [3.0867248480634486, 6.700473453723668, 0.7082869831520391]\n [3.8094745691954923, 8.25130415895562, 1.0841620354518975]\n [4.697840487147518, 10.136982080467158, 1.655002727352565]\n ⋮\n [10.49730559336705, 4.660822889495989, 35.336831929448614]\n [9.330009052592839, 3.0272670946941713, 34.430722536963344]\n [8.069460661013105, 1.766747763108672, 33.159305922954225]\n [6.8089180814322185, 0.8987564841782779, 31.6759436385101]\n [5.6268857619814305, 0.3801973723631693, 30.108951163308078]\n [4.577548084057778, 0.13525687944525802, 28.545926978224173]\n [3.6890898431352737, 0.08257160199224252, 27.035860436772758]\n [2.9677861949066675, 0.15205611935372762, 25.600040161309696]\n [2.4046401797960795, 0.2914663505185634, 24.24373008707723]\n [1.9820054139405763, 0.46628657468365653, 22.964748583050085]\n [1.6788616460891923, 0.6565587545689172, 21.758445642263496]\n [1.4744010677851374, 0.8530017039412324, 20.62004063423844]\n\n\nThe first time Julia compiles the function, and the second is a straight call.\n\n@time solve_system_save_push(lorenz,[1.0,0.0,0.0],p,1000)\n\n  0.000048 seconds (1.01 k allocations: 99.984 KiB)\n\n\n1000-element Vector{Vector{Float64}}:\n [1.0, 0.0, 0.0]\n [0.8, 0.56, 0.0]\n [0.752, 0.9968000000000001, 0.008960000000000001]\n [0.80096, 1.3978492416000001, 0.023474005333333336]\n [0.92033784832, 1.8180538219817644, 0.04461448495326095]\n [1.099881043052353, 2.296260732619613, 0.07569952060880669]\n [1.339156980965805, 2.864603692722823, 0.12217448583728006]\n [1.6442463233172087, 3.5539673118971193, 0.19238159391549564]\n [2.026190521033191, 4.397339452147425, 0.2989931959555302]\n [2.5004203072560376, 5.431943011293093, 0.4612438424853632]\n [3.0867248480634486, 6.700473453723668, 0.7082869831520391]\n [3.8094745691954923, 8.25130415895562, 1.0841620354518975]\n [4.697840487147518, 10.136982080467158, 1.655002727352565]\n ⋮\n [10.49730559336705, 4.660822889495989, 35.336831929448614]\n [9.330009052592839, 3.0272670946941713, 34.430722536963344]\n [8.069460661013105, 1.766747763108672, 33.159305922954225]\n [6.8089180814322185, 0.8987564841782779, 31.6759436385101]\n [5.6268857619814305, 0.3801973723631693, 30.108951163308078]\n [4.577548084057778, 0.13525687944525802, 28.545926978224173]\n [3.6890898431352737, 0.08257160199224252, 27.035860436772758]\n [2.9677861949066675, 0.15205611935372762, 25.600040161309696]\n [2.4046401797960795, 0.2914663505185634, 24.24373008707723]\n [1.9820054139405763, 0.46628657468365653, 22.964748583050085]\n [1.6788616460891923, 0.6565587545689172, 21.758445642263496]\n [1.4744010677851374, 0.8530017039412324, 20.62004063423844]\n\n\nor we can use @btime:\n\nusing BenchmarkTools\n@btime solve_system_save(lorenz,[1.0,0.0,0.0],p,1000)\n\n  32.372 μs (1001 allocations: 86.06 KiB)\n\n\n1000-element Vector{Vector{Float64}}:\n [1.0, 0.0, 0.0]\n [0.8, 0.56, 0.0]\n [0.752, 0.9968000000000001, 0.008960000000000001]\n [0.80096, 1.3978492416000001, 0.023474005333333336]\n [0.92033784832, 1.8180538219817644, 0.04461448495326095]\n [1.099881043052353, 2.296260732619613, 0.07569952060880669]\n [1.339156980965805, 2.864603692722823, 0.12217448583728006]\n [1.6442463233172087, 3.5539673118971193, 0.19238159391549564]\n [2.026190521033191, 4.397339452147425, 0.2989931959555302]\n [2.5004203072560376, 5.431943011293093, 0.4612438424853632]\n [3.0867248480634486, 6.700473453723668, 0.7082869831520391]\n [3.8094745691954923, 8.25130415895562, 1.0841620354518975]\n [4.697840487147518, 10.136982080467158, 1.655002727352565]\n ⋮\n [10.49730559336705, 4.660822889495989, 35.336831929448614]\n [9.330009052592839, 3.0272670946941713, 34.430722536963344]\n [8.069460661013105, 1.766747763108672, 33.159305922954225]\n [6.8089180814322185, 0.8987564841782779, 31.6759436385101]\n [5.6268857619814305, 0.3801973723631693, 30.108951163308078]\n [4.577548084057778, 0.13525687944525802, 28.545926978224173]\n [3.6890898431352737, 0.08257160199224252, 27.035860436772758]\n [2.9677861949066675, 0.15205611935372762, 25.600040161309696]\n [2.4046401797960795, 0.2914663505185634, 24.24373008707723]\n [1.9820054139405763, 0.46628657468365653, 22.964748583050085]\n [1.6788616460891923, 0.6565587545689172, 21.758445642263496]\n [1.4744010677851374, 0.8530017039412324, 20.62004063423844]\n\n\n\n@btime solve_system_save_push(lorenz,[1.0,0.0,0.0],p,1000)\n\n  37.062 μs (1006 allocations: 99.98 KiB)\n\n\n1000-element Vector{Vector{Float64}}:\n [1.0, 0.0, 0.0]\n [0.8, 0.56, 0.0]\n [0.752, 0.9968000000000001, 0.008960000000000001]\n [0.80096, 1.3978492416000001, 0.023474005333333336]\n [0.92033784832, 1.8180538219817644, 0.04461448495326095]\n [1.099881043052353, 2.296260732619613, 0.07569952060880669]\n [1.339156980965805, 2.864603692722823, 0.12217448583728006]\n [1.6442463233172087, 3.5539673118971193, 0.19238159391549564]\n [2.026190521033191, 4.397339452147425, 0.2989931959555302]\n [2.5004203072560376, 5.431943011293093, 0.4612438424853632]\n [3.0867248480634486, 6.700473453723668, 0.7082869831520391]\n [3.8094745691954923, 8.25130415895562, 1.0841620354518975]\n [4.697840487147518, 10.136982080467158, 1.655002727352565]\n ⋮\n [10.49730559336705, 4.660822889495989, 35.336831929448614]\n [9.330009052592839, 3.0272670946941713, 34.430722536963344]\n [8.069460661013105, 1.766747763108672, 33.159305922954225]\n [6.8089180814322185, 0.8987564841782779, 31.6759436385101]\n [5.6268857619814305, 0.3801973723631693, 30.108951163308078]\n [4.577548084057778, 0.13525687944525802, 28.545926978224173]\n [3.6890898431352737, 0.08257160199224252, 27.035860436772758]\n [2.9677861949066675, 0.15205611935372762, 25.600040161309696]\n [2.4046401797960795, 0.2914663505185634, 24.24373008707723]\n [1.9820054139405763, 0.46628657468365653, 22.964748583050085]\n [1.6788616460891923, 0.6565587545689172, 21.758445642263496]\n [1.4744010677851374, 0.8530017039412324, 20.62004063423844]\n\n\nThis is because growth costs are amortized, meaning that when pushing, the size isn’t increasing by one each time, but rather it’s doing something like doubling, so that it’s averaging O(1) cost to keep growing (in theory the best is to do Golden ratio resizing, and long discussions can be had on this topic).\nWe can also look at what happens if we use matrices:\n\nfunction solve_system_save_matrix(f,u0,p,n)\n  u = Matrix{eltype(u0)}(undef,length(u0),n)\n  u[:,1] = u0\n  for i in 1:n-1\n    u[:,i+1] = f(u[:,i],p)\n  end\n  u\nend\n@btime solve_system_save_matrix(lorenz,[1.0,0.0,0.0],p,1000)\n\n  67.775 μs (2001 allocations: 179.66 KiB)\n\n\n3×1000 Matrix{Float64}:\n 1.0  0.8   0.752    0.80096   0.920338   …   1.98201    1.67886    1.4744\n 0.0  0.56  0.9968   1.39785   1.81805        0.466287   0.656559   0.853002\n 0.0  0.0   0.00896  0.023474  0.0446145     22.9647    21.7584    20.62\n\n\nWhere is this cost coming from? A large portion of the cost is due to the slicing on the u, which we can fix with a view:\n\nfunction solve_system_save_matrix_view(f,u0,p,n)\n  u = Matrix{eltype(u0)}(undef,length(u0),n)\n  u[:,1] = u0\n  for i in 1:n-1\n    u[:,i+1] = f(@view(u[:,i]),p)\n  end\n  u\nend\n@btime solve_system_save_matrix_view(lorenz,[1.0,0.0,0.0],p,1000)\n\n  38.414 μs (1002 allocations: 101.61 KiB)\n\n\n3×1000 Matrix{Float64}:\n 1.0  0.8   0.752    0.80096   0.920338   …   1.98201    1.67886    1.4744\n 0.0  0.56  0.9968   1.39785   1.81805        0.466287   0.656559   0.853002\n 0.0  0.0   0.00896  0.023474  0.0446145     22.9647    21.7584    20.62\n\n\nSince we are only ever using single columns as a unit, notice that there isn’t any benefit to keeping the whole thing contiguous, and in fact there are some downsides (cache is harder to optimize because the longer cache lines are unnecessary, the views need to be used). Also, growing the matrix adaptively is not a very good idea since every growth requires both allocating memory and copying over the old values:\n\nfunction solve_system_save_matrix_resize(f,u0,p,n)\n  u = Matrix{eltype(u0)}(undef,length(u0),1)\n  u[:,1] = u0\n  for i in 1:n-1\n    u = hcat(u,f(@view(u[:,i]),p))\n  end\n  u\nend\n@btime solve_system_save_matrix_resize(lorenz,[1.0,0.0,0.0],p,1000)\n\n  897.386 μs (2318 allocations: 11.65 MiB)\n\n\n3×1000 Matrix{Float64}:\n 1.0  0.8   0.752    0.80096   0.920338   …   1.98201    1.67886    1.4744\n 0.0  0.56  0.9968   1.39785   1.81805        0.466287   0.656559   0.853002\n 0.0  0.0   0.00896  0.023474  0.0446145     22.9647    21.7584    20.62\n\n\nSo for now let’s go back to the Vector of Arrays approach. One way to reduce the number of allocations is to require that the user provides an in-place non-allocating function. For example:\n\nfunction lorenz(du,u,p)\n  α,σ,ρ,β = p\n  du[1] = u[1] + α*(σ*(u[2]-u[1]))\n  du[2] = u[2] + α*(u[1]*(ρ-u[3]) - u[2])\n  du[3] = u[3] + α*(u[1]*u[2] - β*u[3])\nend\np = (0.02,10.0,28.0,8/3)\nfunction solve_system_save(f,u0,p,n)\n  u = Vector{typeof(u0)}(undef,n)\n  du = similar(u0)\n  u[1] = u0\n  for i in 1:n-1\n    f(du,u[i],p)\n    u[i+1] = du\n  end\n  u\nend\nsolve_system_save(lorenz,[1.0,0.0,0.0],p,1000)\n\n1000-element Vector{Vector{Float64}}:\n [1.0, 0.0, 0.0]\n [-6.762970638284242, -11.790601841925698, 16.051639908100938]\n [-6.762970638284242, -11.790601841925698, 16.051639908100938]\n [-6.762970638284242, -11.790601841925698, 16.051639908100938]\n [-6.762970638284242, -11.790601841925698, 16.051639908100938]\n [-6.762970638284242, -11.790601841925698, 16.051639908100938]\n [-6.762970638284242, -11.790601841925698, 16.051639908100938]\n [-6.762970638284242, -11.790601841925698, 16.051639908100938]\n [-6.762970638284242, -11.790601841925698, 16.051639908100938]\n [-6.762970638284242, -11.790601841925698, 16.051639908100938]\n [-6.762970638284242, -11.790601841925698, 16.051639908100938]\n [-6.762970638284242, -11.790601841925698, 16.051639908100938]\n [-6.762970638284242, -11.790601841925698, 16.051639908100938]\n ⋮\n [-6.762970638284242, -11.790601841925698, 16.051639908100938]\n [-6.762970638284242, -11.790601841925698, 16.051639908100938]\n [-6.762970638284242, -11.790601841925698, 16.051639908100938]\n [-6.762970638284242, -11.790601841925698, 16.051639908100938]\n [-6.762970638284242, -11.790601841925698, 16.051639908100938]\n [-6.762970638284242, -11.790601841925698, 16.051639908100938]\n [-6.762970638284242, -11.790601841925698, 16.051639908100938]\n [-6.762970638284242, -11.790601841925698, 16.051639908100938]\n [-6.762970638284242, -11.790601841925698, 16.051639908100938]\n [-6.762970638284242, -11.790601841925698, 16.051639908100938]\n [-6.762970638284242, -11.790601841925698, 16.051639908100938]\n [-6.762970638284242, -11.790601841925698, 16.051639908100938]\n\n\nOh no, all of the outputs are the same! What happened? The problem is in the line u[i+1] = du. What we had done is set the save vector to the same pointer as du, effectively linking all of the pointers. The moral of the story is, you cannot get around allocating for all of these outputs if you’re going to give the user all of the outputs! It’s impossible to not make all of these arrays, so if this is the case then you’d have to:\n\nfunction solve_system_save_copy(f,u0,p,n)\n  u = Vector{typeof(u0)}(undef,n)\n  du = similar(u0)\n  u[1] = u0\n  for i in 1:n-1\n    f(du,u[i],p)\n    u[i+1] = copy(du)\n  end\n  u\nend\nsolve_system_save_copy(lorenz,[1.0,0.0,0.0],p,1000)\n\n1000-element Vector{Vector{Float64}}:\n [1.0, 0.0, 0.0]\n [0.8, 0.56, 0.0]\n [0.752, 0.9968000000000001, 0.008960000000000001]\n [0.80096, 1.3978492416000001, 0.023474005333333336]\n [0.92033784832, 1.8180538219817644, 0.04461448495326095]\n [1.099881043052353, 2.296260732619613, 0.07569952060880669]\n [1.339156980965805, 2.864603692722823, 0.12217448583728006]\n [1.6442463233172087, 3.5539673118971193, 0.19238159391549564]\n [2.026190521033191, 4.397339452147425, 0.2989931959555302]\n [2.5004203072560376, 5.431943011293093, 0.4612438424853632]\n [3.0867248480634486, 6.700473453723668, 0.7082869831520391]\n [3.8094745691954923, 8.25130415895562, 1.0841620354518975]\n [4.697840487147518, 10.136982080467158, 1.655002727352565]\n ⋮\n [10.49730559336705, 4.660822889495989, 35.336831929448614]\n [9.330009052592839, 3.0272670946941713, 34.430722536963344]\n [8.069460661013105, 1.766747763108672, 33.159305922954225]\n [6.8089180814322185, 0.8987564841782779, 31.6759436385101]\n [5.6268857619814305, 0.3801973723631693, 30.108951163308078]\n [4.577548084057778, 0.13525687944525802, 28.545926978224173]\n [3.6890898431352737, 0.08257160199224252, 27.035860436772758]\n [2.9677861949066675, 0.15205611935372762, 25.600040161309696]\n [2.4046401797960795, 0.2914663505185634, 24.24373008707723]\n [1.9820054139405763, 0.46628657468365653, 22.964748583050085]\n [1.6788616460891923, 0.6565587545689172, 21.758445642263496]\n [1.4744010677851374, 0.8530017039412324, 20.62004063423844]\n\n\nwhich nullifies the advantage of the non-allocating approach. However, if only the end point is necessary, then the reduced allocation approach is helpful:\n\nfunction solve_system_mutate(f,u0,p,n)\n  # create work buffers\n  du = similar(u0); u = copy(u0)\n  # non-allocating loop\n  for i in 1:n-1\n    f(du,u,p)\n    u,du = du,u\n  end\n  u\nend\nsolve_system_mutate(lorenz,[1.0,0.0,0.0],p,1000)\n\n3-element Vector{Float64}:\n  1.4744010677851374\n  0.8530017039412324\n 20.62004063423844\n\n\nHere we see a little trick: the line u,du = du,u is swapping the pointer of u with the pointer of du (since the value of the array is its reference). An alternative way to write the loop is:\n\nfor i in 1:n-1\n  f(du,u,p)\n  u .= du\nend\n\nwhich would compute f and then take the values of du and update u with them, but that’s 3 extra operations than required, whereas u,du = du,u will change u to be a pointer to the updated memory and now du is an “empty” cache array that we can refill (this decreases the computational cost by ~33%). Let’s see what the cost is with this newest version:\n\n@btime solve_system(lorenz,[1.0,0.0,0.0],p,1000)\n\n  30.028 μs (1000 allocations: 78.12 KiB)\n\n\n3-element Vector{Float64}:\n  1.4744010677851374\n  0.8530017039412324\n 20.62004063423844\n\n\n\n@btime solve_system_mutate(lorenz,[1.0,0.0,0.0],p,1000)\n\n  6.684 μs (3 allocations: 240 bytes)\n\n\n3-element Vector{Float64}:\n  1.4744010677851374\n  0.8530017039412324\n 20.62004063423844\n\n\nOne last change that we could do is make use of StaticArrays. To do this, we need to go back to non-mutating, like:\n\nusing StaticArrays\nfunction lorenz(u,p)\n  α,σ,ρ,β = p\n  du1 = u[1] + α*(σ*(u[2]-u[1]))\n  du2 = u[2] + α*(u[1]*(ρ-u[3]) - u[2])\n  du3 = u[3] + α*(u[1]*u[2] - β*u[3])\n  @SVector [du1,du2,du3]\nend\np = (0.02,10.0,28.0,8/3)\nfunction solve_system_save(f,u0,p,n)\n  u = Vector{typeof(u0)}(undef,n)\n  u[1] = u0\n  for i in 1:n-1\n    u[i+1] = f(u[i],p)\n  end\n  u\nend\nsolve_system_save(lorenz,@SVector[1.0,0.0,0.0],p,1000)\n\n1000-element Vector{SVector{3, Float64}}:\n [1.0, 0.0, 0.0]\n [0.8, 0.56, 0.0]\n [0.752, 0.9968000000000001, 0.008960000000000001]\n [0.80096, 1.3978492416000001, 0.023474005333333336]\n [0.92033784832, 1.8180538219817644, 0.04461448495326095]\n [1.099881043052353, 2.296260732619613, 0.07569952060880669]\n [1.339156980965805, 2.864603692722823, 0.12217448583728006]\n [1.6442463233172087, 3.5539673118971193, 0.19238159391549564]\n [2.026190521033191, 4.397339452147425, 0.2989931959555302]\n [2.5004203072560376, 5.431943011293093, 0.4612438424853632]\n [3.0867248480634486, 6.700473453723668, 0.7082869831520391]\n [3.8094745691954923, 8.25130415895562, 1.0841620354518975]\n [4.697840487147518, 10.136982080467158, 1.655002727352565]\n ⋮\n [10.49730559336705, 4.660822889495989, 35.336831929448614]\n [9.330009052592839, 3.0272670946941713, 34.430722536963344]\n [8.069460661013105, 1.766747763108672, 33.159305922954225]\n [6.8089180814322185, 0.8987564841782779, 31.6759436385101]\n [5.6268857619814305, 0.3801973723631693, 30.108951163308078]\n [4.577548084057778, 0.13525687944525802, 28.545926978224173]\n [3.6890898431352737, 0.08257160199224252, 27.035860436772758]\n [2.9677861949066675, 0.15205611935372762, 25.600040161309696]\n [2.4046401797960795, 0.2914663505185634, 24.24373008707723]\n [1.9820054139405763, 0.46628657468365653, 22.964748583050085]\n [1.6788616460891923, 0.6565587545689172, 21.758445642263496]\n [1.4744010677851374, 0.8530017039412324, 20.62004063423844]\n\n\n\n@btime solve_system_save(lorenz,@SVector[1.0,0.0,0.0],p,1000)\n\n  6.234 μs (2 allocations: 23.48 KiB)\n\n\n1000-element Vector{SVector{3, Float64}}:\n [1.0, 0.0, 0.0]\n [0.8, 0.56, 0.0]\n [0.752, 0.9968000000000001, 0.008960000000000001]\n [0.80096, 1.3978492416000001, 0.023474005333333336]\n [0.92033784832, 1.8180538219817644, 0.04461448495326095]\n [1.099881043052353, 2.296260732619613, 0.07569952060880669]\n [1.339156980965805, 2.864603692722823, 0.12217448583728006]\n [1.6442463233172087, 3.5539673118971193, 0.19238159391549564]\n [2.026190521033191, 4.397339452147425, 0.2989931959555302]\n [2.5004203072560376, 5.431943011293093, 0.4612438424853632]\n [3.0867248480634486, 6.700473453723668, 0.7082869831520391]\n [3.8094745691954923, 8.25130415895562, 1.0841620354518975]\n [4.697840487147518, 10.136982080467158, 1.655002727352565]\n ⋮\n [10.49730559336705, 4.660822889495989, 35.336831929448614]\n [9.330009052592839, 3.0272670946941713, 34.430722536963344]\n [8.069460661013105, 1.766747763108672, 33.159305922954225]\n [6.8089180814322185, 0.8987564841782779, 31.6759436385101]\n [5.6268857619814305, 0.3801973723631693, 30.108951163308078]\n [4.577548084057778, 0.13525687944525802, 28.545926978224173]\n [3.6890898431352737, 0.08257160199224252, 27.035860436772758]\n [2.9677861949066675, 0.15205611935372762, 25.600040161309696]\n [2.4046401797960795, 0.2914663505185634, 24.24373008707723]\n [1.9820054139405763, 0.46628657468365653, 22.964748583050085]\n [1.6788616460891923, 0.6565587545689172, 21.758445642263496]\n [1.4744010677851374, 0.8530017039412324, 20.62004063423844]\n\n\nThis is utilizing a lot more optimizations, like SIMD, automatically, which is helpful. Let’s also remove the bounds checks:\n\nfunction lorenz(u,p)\n  α,σ,ρ,β = p\n  @inbounds begin\n    du1 = u[1] + α*(σ*(u[2]-u[1]))\n    du2 = u[2] + α*(u[1]*(ρ-u[3]) - u[2])\n    du3 = u[3] + α*(u[1]*u[2] - β*u[3])\n  end\n  @SVector [du1,du2,du3]\nend\nfunction solve_system_save(f,u0,p,n)\n  u = Vector{typeof(u0)}(undef,n)\n  @inbounds u[1] = u0\n  @inbounds for i in 1:n-1\n    u[i+1] = f(u[i],p)\n  end\n  u\nend\nsolve_system_save(lorenz,@SVector[1.0,0.0,0.0],p,1000)\n\n1000-element Vector{SVector{3, Float64}}:\n [1.0, 0.0, 0.0]\n [0.8, 0.56, 0.0]\n [0.752, 0.9968000000000001, 0.008960000000000001]\n [0.80096, 1.3978492416000001, 0.023474005333333336]\n [0.92033784832, 1.8180538219817644, 0.04461448495326095]\n [1.099881043052353, 2.296260732619613, 0.07569952060880669]\n [1.339156980965805, 2.864603692722823, 0.12217448583728006]\n [1.6442463233172087, 3.5539673118971193, 0.19238159391549564]\n [2.026190521033191, 4.397339452147425, 0.2989931959555302]\n [2.5004203072560376, 5.431943011293093, 0.4612438424853632]\n [3.0867248480634486, 6.700473453723668, 0.7082869831520391]\n [3.8094745691954923, 8.25130415895562, 1.0841620354518975]\n [4.697840487147518, 10.136982080467158, 1.655002727352565]\n ⋮\n [10.49730559336705, 4.660822889495989, 35.336831929448614]\n [9.330009052592839, 3.0272670946941713, 34.430722536963344]\n [8.069460661013105, 1.766747763108672, 33.159305922954225]\n [6.8089180814322185, 0.8987564841782779, 31.6759436385101]\n [5.6268857619814305, 0.3801973723631693, 30.108951163308078]\n [4.577548084057778, 0.13525687944525802, 28.545926978224173]\n [3.6890898431352737, 0.08257160199224252, 27.035860436772758]\n [2.9677861949066675, 0.15205611935372762, 25.600040161309696]\n [2.4046401797960795, 0.2914663505185634, 24.24373008707723]\n [1.9820054139405763, 0.46628657468365653, 22.964748583050085]\n [1.6788616460891923, 0.6565587545689172, 21.758445642263496]\n [1.4744010677851374, 0.8530017039412324, 20.62004063423844]\n\n\n\n@btime solve_system_save(lorenz,@SVector[1.0,0.0,0.0],p,1000)\n\n  5.546 μs (2 allocations: 23.48 KiB)\n\n\n1000-element Vector{SVector{3, Float64}}:\n [1.0, 0.0, 0.0]\n [0.8, 0.56, 0.0]\n [0.752, 0.9968000000000001, 0.008960000000000001]\n [0.80096, 1.3978492416000001, 0.023474005333333336]\n [0.92033784832, 1.8180538219817644, 0.04461448495326095]\n [1.099881043052353, 2.296260732619613, 0.07569952060880669]\n [1.339156980965805, 2.864603692722823, 0.12217448583728006]\n [1.6442463233172087, 3.5539673118971193, 0.19238159391549564]\n [2.026190521033191, 4.397339452147425, 0.2989931959555302]\n [2.5004203072560376, 5.431943011293093, 0.4612438424853632]\n [3.0867248480634486, 6.700473453723668, 0.7082869831520391]\n [3.8094745691954923, 8.25130415895562, 1.0841620354518975]\n [4.697840487147518, 10.136982080467158, 1.655002727352565]\n ⋮\n [10.49730559336705, 4.660822889495989, 35.336831929448614]\n [9.330009052592839, 3.0272670946941713, 34.430722536963344]\n [8.069460661013105, 1.766747763108672, 33.159305922954225]\n [6.8089180814322185, 0.8987564841782779, 31.6759436385101]\n [5.6268857619814305, 0.3801973723631693, 30.108951163308078]\n [4.577548084057778, 0.13525687944525802, 28.545926978224173]\n [3.6890898431352737, 0.08257160199224252, 27.035860436772758]\n [2.9677861949066675, 0.15205611935372762, 25.600040161309696]\n [2.4046401797960795, 0.2914663505185634, 24.24373008707723]\n [1.9820054139405763, 0.46628657468365653, 22.964748583050085]\n [1.6788616460891923, 0.6565587545689172, 21.758445642263496]\n [1.4744010677851374, 0.8530017039412324, 20.62004063423844]\n\n\nAnd we can get down to non-allocating for the loop:\n\n@btime solve_system(lorenz,@SVector([1.0,0.0,0.0]),p,1000)\n\n  5.528 μs (1 allocation: 32 bytes)\n\n\n3-element SVector{3, Float64} with indices SOneTo(3):\n  1.4744010677851374\n  0.8530017039412324\n 20.62004063423844\n\n\nNotice that the single allocation is the output.\nWe can lastly make the saving version completely non-allocating if we hoist the allocation out to the higher level:\n\nfunction solve_system_save!(u,f,u0,p,n)\n  @inbounds u[1] = u0\n  @inbounds for i in 1:length(u)-1\n    u[i+1] = f(u[i],p)\n  end\n  u\nend\nu = Vector{typeof(@SVector([1.0,0.0,0.0]))}(undef,1000)\n@btime solve_system_save!(u,lorenz,@SVector([1.0,0.0,0.0]),p,1000)\n\n  5.875 μs (0 allocations: 0 bytes)\n\n\n1000-element Vector{SVector{3, Float64}}:\n [1.0, 0.0, 0.0]\n [0.8, 0.56, 0.0]\n [0.752, 0.9968000000000001, 0.008960000000000001]\n [0.80096, 1.3978492416000001, 0.023474005333333336]\n [0.92033784832, 1.8180538219817644, 0.04461448495326095]\n [1.099881043052353, 2.296260732619613, 0.07569952060880669]\n [1.339156980965805, 2.864603692722823, 0.12217448583728006]\n [1.6442463233172087, 3.5539673118971193, 0.19238159391549564]\n [2.026190521033191, 4.397339452147425, 0.2989931959555302]\n [2.5004203072560376, 5.431943011293093, 0.4612438424853632]\n [3.0867248480634486, 6.700473453723668, 0.7082869831520391]\n [3.8094745691954923, 8.25130415895562, 1.0841620354518975]\n [4.697840487147518, 10.136982080467158, 1.655002727352565]\n ⋮\n [10.49730559336705, 4.660822889495989, 35.336831929448614]\n [9.330009052592839, 3.0272670946941713, 34.430722536963344]\n [8.069460661013105, 1.766747763108672, 33.159305922954225]\n [6.8089180814322185, 0.8987564841782779, 31.6759436385101]\n [5.6268857619814305, 0.3801973723631693, 30.108951163308078]\n [4.577548084057778, 0.13525687944525802, 28.545926978224173]\n [3.6890898431352737, 0.08257160199224252, 27.035860436772758]\n [2.9677861949066675, 0.15205611935372762, 25.600040161309696]\n [2.4046401797960795, 0.2914663505185634, 24.24373008707723]\n [1.9820054139405763, 0.46628657468365653, 22.964748583050085]\n [1.6788616460891923, 0.6565587545689172, 21.758445642263496]\n [1.4744010677851374, 0.8530017039412324, 20.62004063423844]\n\n\nIt is important to note that this single allocation does not seem to effect the timing of the result in this case, when run serially. However, when parallelism or embedded applications get involved, this can be a significant effect."
  },
  {
    "objectID": "dynamical_systems.html#discussion-questions",
    "href": "dynamical_systems.html#discussion-questions",
    "title": "3  How Loops Work, An Introduction to Discrete Dynamics",
    "section": "3.9 Discussion Questions",
    "text": "3.9 Discussion Questions\n\nWhat are some ways to compute steady states? Periodic orbits?\nWhen using the mutating algorithms, what are the data dependencies between different solves if they were to happen simultaneously?\nWe saw that there is a connection between delayed systems and multivariable systems. How deep does that go? Is every delayed system also a multivariable system and vice versa? Is this a useful idea to explore?"
  },
  {
    "objectID": "parallelism_overview.html#youtube-video-link",
    "href": "parallelism_overview.html#youtube-video-link",
    "title": "4  The Basics of Single Node Parallel Computing",
    "section": "4.1 Youtube Video Link",
    "text": "4.1 Youtube Video Link\nMoore’s law was the idea that computers double in efficiency at fixed time points, leading to exponentially more computing power over time. This was true for a very long time.\n\nHowever, sometime in the last decade, computer cores have stopped getting faster.\n\nThe technology that promises to keep Moore’s Law going after 2013 is known as extreme ultraviolet (EUV) lithography. It uses light to write a pattern into a chemical layer on top of a silicon wafer, which is then chemically etched into the silicon to make chip components. EUV lithography uses very high energy ultraviolet light rays that are closer to X-rays than visible light. That’s attractive because EUV light has a short wavelength—around 13 nanometers—which allows for making smaller details than the 193-nanometer ultraviolet light used in lithography today. But EUV has proved surprisingly difficult to perfect.\n\n-MIT Technology Review\nThe answer to the “end of Moore’s Law” is Parallel Computing. However, programs need to be specifically designed in order to adequately use parallelism. This lecture will describe at a very high level the forms of parallelism and when they are appropriate. We will then proceed to use shared-memory multithreading to parallelize the simulation of the discrete dynamical system."
  },
  {
    "objectID": "parallelism_overview.html#managing-threads",
    "href": "parallelism_overview.html#managing-threads",
    "title": "4  The Basics of Single Node Parallel Computing",
    "section": "4.2 Managing Threads",
    "text": "4.2 Managing Threads\n\n4.2.1 Concurrency vs Parallelism and Green Threads\nThere is a difference between concurrency and parallelism. In a nutshell:\n\nConcurrency: Interruptability\nParallelism: Independentability\n\n\n\nTo start thinking about concurrency, we need to distinguish between a process and a thread. A process is discrete running instance of a computer program. It has allocated memory for the program’s code, its data, a heap, etc. Each process can have many compute threads. These threads are the unit of execution that needs to be done. On each task is its own stack and a virtual CPU (virtual CPU since it’s not the true CPU, since that would require that the task is ON the CPU, which it might not be because the task can be temporarily haulted). The kernel of the operating systems then schedules tasks, which runs them. In order to keep the computer running smooth, context switching, i.e. changing the task that is actually running, happens all the time. This is independent of whether tasks are actually scheduled in parallel or not.\n\n\n\nEach thread has its own stack associated with it.\n\n\nThis is an important distinction because many tasks may need to be ran concurrently but without parallelism. Examples of this are input/output (I/O). For example, in a game you may want to be updating the graphics, but the moment a user clicks you want to handle that event. You do not necessarily need to have these running in parallel, but you need the event handling task to be running concurrently to the processing of the game.\n\nData handling is the key area of scientific computing where green threads (concurrent non-parallel threads) show up. For data handling, one may need to send a signal that causes a message to start being passed. Alternative hardware take over at that point. This alternative hardware is a processor specific for an I/O bus, like the controller for the SSD, modem, GPU, or Infiniband. It will be polled, then it will execute the command, and give the result. There are two variants:\n\nNon-Blocking vs Blocking: Whether the thread will periodically poll for whether that task is complete, or whether it should wait for the task to complete before doing anything else\nSynchronous vs Asynchronus: Whether to execute the operation as initiated by the program or as a response to an event from the kernel.\n\nI/O operations cause a privileged context switch, allowing the task which is handling the I/O to directly be switched to in order to continue actions.\n\n4.2.1.1 The Main Event Loop\nJulia, along with other languages with a runtime (Javascript, Go, etc.) at its core is a single process running an event loop. This event loop has is the main thread, and “Julia program” or “script” that one is running is actually ran in a green thread that is controlled by the main event loop. The event loop takes over to look for other work whenever the program hits a yield point. More yield points allows for more aggressive task switching, while it also means more switches to the event loop which suspends the numerical task, i.e. making it slower. Thus yielding shouldn’t interrupt the main loop!\nThis is one area where languages can wildly differ in implementation. Languages structured for lots of I/O and input handling, like Javascript, have yield points at every line (it’s an interpreted language and therefore the interpreter can always take control). In Julia, the yield points are minimized. The common yield points are allocations and I/O (println). This means that a tight non-allocating inner loop will not have any yield points and will be a thread that is not interruptible. While this is great for numerical performance, it is something to be aware of.\nSide effect: if you run a long tight loop and wish to exit it, you may try Ctrl + C and see that it doesn’t work. This is because interrupts are handled by the event loop. The event loop is never re-entered until after your tight numerical loop, and therefore you have the waiting occur. If you hit Ctrl + C multiple times, you will escalate the interruption until the OS takes over and then this is handled by the signal handling of the OS’s event loop, which sends a higher level interrupt which Julia handles the moment the safety locks says it’s okay (these locks occur during memory allocations to ensure that memory is not corrupted).\n\n\n4.2.1.2 Asynchronous Calling Example\nThis example will become more clear when we get to distributed computing, but for think of remotecall_fetch as a way to run a command on a different computer. What we want to do is start all of the commands at once, and then wait for all the results before finishing the loop. We will use @async to make the call to remotecall_fetch be non-blocking, i.e. it’ll start the job and only poll infrequently to find out when the other machine has completed the job and returned the result. We then add @sync to the loop, which will only continue the loop after all of the green threads have fetched the result. Otherwise, it’s possible that a[idx] may not be filled yet, since the thread may not have fetched the result!\n\n@time begin\n    a = Vector{Any}(undef,nworkers())\n    @sync for (idx, pid) in enumerate(workers())\n        @async a[idx] = remotecall_fetch(sleep, pid, 2)\n    end\nend\n\nThe same can be done for writing to the disk. @async is a quick shorthand for spawning a green thread which will handle that I/O operation, and the main event loop will keep switching between them until they are all handled. @sync encodes that the program will not continue until all green threads are handled. This could be done more manually with Task and Channels, which will be something we touch on in the future.\n\n\n\n4.2.2 Examples of the Differences\nSynchronous = Thread will complete an action\nBlocking = Thread will wait until action is completed\n\nAsynchronous + Non-Blocking: I/O\nAsynchronous + Blocking: Threaded atomics (demonstrated next lecture)\nSynchronous + Blocking: Standard computing, @sync\nSynchronous + Non-Blocking: Webservers where an I/O operation can be performed, but one never checks if the operation is completed.\n\n\n\n4.2.3 Multithreading\nIf your threads are independent, then it may make sense to run them in parallel. This is the form of parallelism known as multithreading. To understand the data that is available in a multithreaded setup, let’s look at the picture of threads again:\n\nEach thread has its own call stack, but it’s the process that holds the heap. This means that dynamically-sized heap allocated objects are shared between threads with no cost, a setup known as shared-memory computing.\n\n4.2.3.1 Loop-Based Multithreading with (threads?)\nLet’s look back at our Lorenz dynamical system from before:\n\nusing StaticArrays, BenchmarkTools\nfunction lorenz(u,p)\n  α,σ,ρ,β = p\n  @inbounds begin\n    du1 = u[1] + α*(σ*(u[2]-u[1]))\n    du2 = u[2] + α*(u[1]*(ρ-u[3]) - u[2])\n    du3 = u[3] + α*(u[1]*u[2] - β*u[3])\n  end\n  @SVector [du1,du2,du3]\nend\nfunction solve_system_save!(u,f,u0,p,n)\n  @inbounds u[1] = u0\n  @inbounds for i in 1:length(u)-1\n    u[i+1] = f(u[i],p)\n  end\n  u\nend\np = (0.02,10.0,28.0,8/3)\nu = Vector{typeof(@SVector([1.0,0.0,0.0]))}(undef,1000)\n@btime solve_system_save!(u,lorenz,@SVector([1.0,0.0,0.0]),p,1000)\n\n  5.086 μs (0 allocations: 0 bytes)\n\n\n1000-element Vector{SVector{3, Float64}}:\n [1.0, 0.0, 0.0]\n [0.8, 0.56, 0.0]\n [0.752, 0.9968000000000001, 0.008960000000000001]\n [0.80096, 1.3978492416000001, 0.023474005333333336]\n [0.92033784832, 1.8180538219817644, 0.04461448495326095]\n [1.099881043052353, 2.296260732619613, 0.07569952060880669]\n [1.339156980965805, 2.864603692722823, 0.12217448583728006]\n [1.6442463233172087, 3.5539673118971193, 0.19238159391549564]\n [2.026190521033191, 4.397339452147425, 0.2989931959555302]\n [2.5004203072560376, 5.431943011293093, 0.4612438424853632]\n [3.0867248480634486, 6.700473453723668, 0.7082869831520391]\n [3.8094745691954923, 8.25130415895562, 1.0841620354518975]\n [4.697840487147518, 10.136982080467158, 1.655002727352565]\n ⋮\n [10.49730559336705, 4.660822889495989, 35.336831929448614]\n [9.330009052592839, 3.0272670946941713, 34.430722536963344]\n [8.069460661013105, 1.766747763108672, 33.159305922954225]\n [6.8089180814322185, 0.8987564841782779, 31.6759436385101]\n [5.6268857619814305, 0.3801973723631693, 30.108951163308078]\n [4.577548084057778, 0.13525687944525802, 28.545926978224173]\n [3.6890898431352737, 0.08257160199224252, 27.035860436772758]\n [2.9677861949066675, 0.15205611935372762, 25.600040161309696]\n [2.4046401797960795, 0.2914663505185634, 24.24373008707723]\n [1.9820054139405763, 0.46628657468365653, 22.964748583050085]\n [1.6788616460891923, 0.6565587545689172, 21.758445642263496]\n [1.4744010677851374, 0.8530017039412324, 20.62004063423844]\n\n\nIn order to use multithreading on this code, we need to take a look at the dependency graph and see what items can be calculated independently of each other. Notice that\nσ*(u[2]-u[1])\nρ-u[3]\nu[1]*u[2]\nβ*u[3]\nare all independent operations, so in theory we could split those off to different threads, move up, etc.\nOr we can have three threads:\nu[1] + α*(σ*(u[2]-u[1]))\nu[2] + α*(u[1]*(ρ-u[3]) - u[2])\nu[3] + α*(u[1]*u[2] - β*u[3])\nall don’t depend on the output of each other, so these tasks can be run in parallel. We can do this by using Julia’s Threads.@threads macro which puts each of the computations of a loop in a different thread. The threaded loops do not allow you to return a value, so how do you build up the values for the @SVector?\n…?\n…?\n…?\nIt’s not possible! To understand why, let’s look at the picture again:\n\nThere is a shared heap, but the stacks are thread local. This means that a value cannot be stack allocated in one thread and magically appear when re-entering the main thread: it needs to go on the heap somewhere. But if it needs to go onto the heap, then it makes sense for us to have preallocated its location. But if we want to preallocate du[1], du[2], and du[3], then it makes sense to use the fully non-allocating update form:\n\nfunction lorenz!(du,u,p)\n  α,σ,ρ,β = p\n  @inbounds begin\n    du[1] = u[1] + α*(σ*(u[2]-u[1]))\n    du[2] = u[2] + α*(u[1]*(ρ-u[3]) - u[2])\n    du[3] = u[3] + α*(u[1]*u[2] - β*u[3])\n  end\nend\nfunction solve_system_save_iip!(u,f,u0,p,n)\n  @inbounds u[1] = u0\n  @inbounds for i in 1:length(u)-1\n    f(u[i+1],u[i],p)\n  end\n  u\nend\np = (0.02,10.0,28.0,8/3)\nu = [Vector{Float64}(undef,3) for i in 1:1000]\n@btime solve_system_save_iip!(u,lorenz!,[1.0,0.0,0.0],p,1000)\n\n  5.982 μs (1 allocation: 80 bytes)\n\n\n1000-element Vector{Vector{Float64}}:\n [1.0, 0.0, 0.0]\n [0.8, 0.56, 0.0]\n [0.752, 0.9968000000000001, 0.008960000000000001]\n [0.80096, 1.3978492416000001, 0.023474005333333336]\n [0.92033784832, 1.8180538219817644, 0.04461448495326095]\n [1.099881043052353, 2.296260732619613, 0.07569952060880669]\n [1.339156980965805, 2.864603692722823, 0.12217448583728006]\n [1.6442463233172087, 3.5539673118971193, 0.19238159391549564]\n [2.026190521033191, 4.397339452147425, 0.2989931959555302]\n [2.5004203072560376, 5.431943011293093, 0.4612438424853632]\n [3.0867248480634486, 6.700473453723668, 0.7082869831520391]\n [3.8094745691954923, 8.25130415895562, 1.0841620354518975]\n [4.697840487147518, 10.136982080467158, 1.655002727352565]\n ⋮\n [10.49730559336705, 4.660822889495989, 35.336831929448614]\n [9.330009052592839, 3.0272670946941713, 34.430722536963344]\n [8.069460661013105, 1.766747763108672, 33.159305922954225]\n [6.8089180814322185, 0.8987564841782779, 31.6759436385101]\n [5.6268857619814305, 0.3801973723631693, 30.108951163308078]\n [4.577548084057778, 0.13525687944525802, 28.545926978224173]\n [3.6890898431352737, 0.08257160199224252, 27.035860436772758]\n [2.9677861949066675, 0.15205611935372762, 25.600040161309696]\n [2.4046401797960795, 0.2914663505185634, 24.24373008707723]\n [1.9820054139405763, 0.46628657468365653, 22.964748583050085]\n [1.6788616460891923, 0.6565587545689172, 21.758445642263496]\n [1.4744010677851374, 0.8530017039412324, 20.62004063423844]\n\n\nand now we multithread:\n\nusing Base.Threads\nfunction lorenz_mt!(du,u,p)\n  α,σ,ρ,β = p\n  let du=du, u=u, p=p\n    Threads.@threads for i in 1:3\n      @inbounds begin\n        if i == 1\n          du[1] = u[1] + α*(σ*(u[2]-u[1]))\n        elseif i == 2\n          du[2] = u[2] + α*(u[1]*(ρ-u[3]) - u[2])\n        else\n          du[3] = u[3] + α*(u[1]*u[2] - β*u[3])\n        end\n        nothing\n      end\n    end\n  end\n  nothing\nend\nfunction solve_system_save_iip!(u,f,u0,p,n)\n  @inbounds u[1] = u0\n  @inbounds for i in 1:length(u)-1\n    f(u[i+1],u[i],p)\n  end\n  u\nend\np = (0.02,10.0,28.0,8/3)\nu = [Vector{Float64}(undef,3) for i in 1:1000]\n@btime solve_system_save_iip!(u,lorenz_mt!,[1.0,0.0,0.0],p,1000);\n\n  1.332 ms (6994 allocations: 671.28 KiB)\n\n\nParallelism doesn’t always make things faster. There are two costs associated with this code. For one, we had to go to the slower heap+mutation version, so its implementation starting point is slower. But secondly, and more importantly, the cost of spinning a new thread is non-negligible. In fact, here we can see that it even needs to make a small allocation for the new context. The total cost is on the order of It’s on the order of 50ns: not huge, but something to take note of. So what we’ve done is taken almost free calculations and made them ~50ns by making each in a different thread, instead of just having it be one thread with one call stack.\nThe moral of the story is that you need to make sure that there’s enough work per thread in order to effectively accelerate a program with parallelism.\n\n\n\n4.2.4 Data-Parallel Problems\nSo not every setup is amenable to parallelism. Dynamical systems are notorious for being quite difficult to parallelize because the dependency of the future time step on the previous time step is clear, meaning that one cannot easily “parallelize through time” (though it is possible, which we will study later).\nHowever, one common way that these systems are generally parallelized is in their inputs. The following questions allow for independent simulations:\n\nWhat steady state does an input u0 go to for some list/region of initial conditions?\nHow does the solution very when I use a different p?\n\nThe problem has a few descriptions. For one, it’s called an embarrassingly parallel problem since the problem can remain largely intact to solve the parallelism problem. To solve this, we can use the exact same solve_system_save_iip!, and just change how we are calling it. Secondly, this is called a data parallel problem, since it parallelized by splitting up the input data (here, the possible u0 or ps) and acting on them independently.\n\n4.2.4.1 Multithreaded Parameter Searches\nNow let’s multithread our parameter search. Let’s say we wanted to compute the mean of the values in the trajectory. For a single input pair, we can compute that like:\n\nusing Statistics\nfunction compute_trajectory_mean(u0,p)\n  u = Vector{typeof(@SVector([1.0,0.0,0.0]))}(undef,1000)\n  solve_system_save!(u,lorenz,u0,p,1000);\n  mean(u)\nend\n@btime compute_trajectory_mean(@SVector([1.0,0.0,0.0]),p)\n\n  5.755 μs (3 allocations: 23.52 KiB)\n\n\n3-element SVector{3, Float64} with indices SOneTo(3):\n -0.3114996234648468\n -0.30974901748976497\n 26.02460355858298\n\n\nWe can make this faster by preallocating the cache vector u. For example, we can globalize it:\n\nu = Vector{typeof(@SVector([1.0,0.0,0.0]))}(undef,1000)\nfunction compute_trajectory_mean2(u0,p)\n  # u is automatically captured\n  solve_system_save!(u,lorenz,u0,p,1000);\n  mean(u)\nend\n@btime compute_trajectory_mean2(@SVector([1.0,0.0,0.0]),p)\n\n  5.594 μs (3 allocations: 112 bytes)\n\n\n3-element SVector{3, Float64} with indices SOneTo(3):\n -0.3114996234648468\n -0.30974901748976497\n 26.02460355858298\n\n\nBut this is still allocating? The issue with this code is that u is a global, and captured globals cannot be inferred because their type can change at any time. Thus what we can do instead is capture a constant:\n\nconst _u_cache = Vector{typeof(@SVector([1.0,0.0,0.0]))}(undef,1000)\nfunction compute_trajectory_mean3(u0,p)\n  # u is automatically captured\n  solve_system_save!(_u_cache,lorenz,u0,p,1000);\n  mean(_u_cache)\nend\n@btime compute_trajectory_mean3(@SVector([1.0,0.0,0.0]),p)\n\n  5.437 μs (1 allocation: 32 bytes)\n\n\n3-element SVector{3, Float64} with indices SOneTo(3):\n -0.3114996234648468\n -0.30974901748976497\n 26.02460355858298\n\n\nNow it’s just allocating the output. The other way to do this is to use a closure which encapsulates the cache data:\n\nfunction _compute_trajectory_mean4(u,u0,p)\n  solve_system_save!(u,lorenz,u0,p,1000);\n  mean(u)\nend\ncompute_trajectory_mean4(u0,p) = _compute_trajectory_mean4(_u_cache,u0,p)\n@btime compute_trajectory_mean4(@SVector([1.0,0.0,0.0]),p)\n\n  5.437 μs (1 allocation: 32 bytes)\n\n\n3-element SVector{3, Float64} with indices SOneTo(3):\n -0.3114996234648468\n -0.30974901748976497\n 26.02460355858298\n\n\nThis is the same, but a bit more explicit. Now let’s create our parameter search function. Let’s take a sample of parameters:\n\nps = [(0.02,10.0,28.0,8/3) .* (1.0,rand(3)...) for i in 1:1000]\n\n1000-element Vector{NTuple{4, Float64}}:\n (0.02, 2.906028730747081, 25.815448683561712, 1.12275960676391)\n (0.02, 9.113888679754554, 23.396345437832256, 0.7087562836042887)\n (0.02, 5.624281003968744, 13.659741289695551, 0.4208787410209084)\n (0.02, 8.984206004881038, 0.6852615071967283, 1.0301245930254095)\n (0.02, 4.2694439608822154, 4.9471629140946405, 2.3050770138904744)\n (0.02, 7.663425652277925, 18.64683941659189, 0.7363023476241628)\n (0.02, 4.709434330622436, 26.792899558395884, 0.37439480474450804)\n (0.02, 8.831670441492465, 16.84462530625224, 0.021743723916777626)\n (0.02, 8.553449211066816, 9.804064155048039, 0.7496459573762918)\n (0.02, 0.4565144731650128, 9.349563328382146, 2.2635468681224284)\n (0.02, 8.42290976678019, 14.966673067788223, 1.1960051176820123)\n (0.02, 0.5457585911274876, 24.90136619584543, 1.3109669223727853)\n (0.02, 0.5359945718426307, 14.307891013960985, 1.9546348854959956)\n ⋮\n (0.02, 1.7204623896087134, 25.287843081510555, 0.3252568576015632)\n (0.02, 7.844463934360588, 4.757745483226857, 1.781725005525443)\n (0.02, 2.8711699406052205, 4.087444438946797, 1.4811591925337435)\n (0.02, 3.902751304527816, 6.238336076607061, 1.2705785287011333)\n (0.02, 9.968337747303584, 5.293795065732924, 1.303275829403436)\n (0.02, 7.708774005658567, 5.735602222665191, 2.0789871634786685)\n (0.02, 2.12164254935724, 8.860777750306905, 1.289277966433394)\n (0.02, 9.569874092656468, 6.865041625393758, 2.5301436950975873)\n (0.02, 8.009180587249597, 10.593322579474947, 2.362216817235268)\n (0.02, 8.666632542262393, 8.944711266322507, 0.02674328422848197)\n (0.02, 7.308376816435307, 2.573780328389054, 0.9360601003642579)\n (0.02, 0.5885533401797627, 5.544287667215732, 0.2478493970596694)\n\n\nAnd let’s get the mean of the trajectory for each of the parameters.\n\nserial_out = map(p -&gt; compute_trajectory_mean4(@SVector([1.0,0.0,0.0]),p),ps)\n\n1000-element Vector{SVector{3, Float64}}:\n [1.2870613149922208, 1.1438117377304484, 22.669697978635234]\n [-0.097034701516529, -0.08504573089716216, 22.37001310300846]\n [0.27821116678919566, 0.27002118086184895, 12.235995564476475]\n [0.017555205468589588, 0.011991074160098073, 0.0005694144205018892]\n [2.9341064618083235, 2.957720277786962, 3.7631308833438184]\n [0.08355705930996428, 0.09868312584495192, 16.708323298061178]\n [0.1439809343709656, 0.1335020515226761, 26.897349357733248]\n [0.1645878209199906, 0.15892637778403598, 29.25441870590682]\n [0.023896881157504005, -0.013675090924136792, 7.4843300243334365]\n [4.106839222388996, 4.473461790124109, 7.9214466035819635]\n [-0.47550108258884005, -0.4860137926531155, 12.734635921351629]\n [5.440942288045614, 5.862161117104718, 23.130221311332342]\n [4.892880857532143, 5.275366330814495, 12.792226972706137]\n ⋮\n [0.6423401153616347, 0.7359944325332999, 23.732878946460538]\n [2.519234311063701, 2.529353415928587, 3.5827775800975536]\n [2.088308258334852, 2.1081397117252565, 2.9312902609659215]\n [2.442287584724364, 2.462780413293451, 4.902743083455716]\n [2.2888836655888816, 2.2957369635098184, 4.09734744761817]\n [3.0525482059827014, 3.0664148208970974, 4.531257717050107]\n [3.0198041858344693, 3.0717061281815505, 7.337327433546069]\n [3.7455104424898713, 3.7604129809004503, 5.631139370594428]\n [-3.7532028322749538, -3.783303266679652, 8.669842002760918]\n [0.16487278105552758, 0.15910352757025786, 12.675401664157413]\n [1.1817629127943725, 1.1832218915995645, 1.4615938649753042]\n [1.0866052697001025, 1.09106295335647, 4.324846322028256]\n\n\nNow let’s do this with multithreading:\n\nfunction tmap(f,ps)\n  out = Vector{typeof(@SVector([1.0,0.0,0.0]))}(undef,1000)\n  Threads.@threads for i in 1:1000\n    # each loop part is using a different part of the data\n    out[i] = f(ps[i])\n  end\n  out\nend\nthreaded_out = tmap(p -&gt; compute_trajectory_mean4(@SVector([1.0,0.0,0.0]),p),ps)\n\n1000-element Vector{SVector{3, Float64}}:\n [1.2870613149922208, 1.1438117377304484, 22.669697978635234]\n [-0.097034701516529, -0.08504573089716216, 22.37001310300846]\n [0.27821116678919566, 0.27002118086184895, 12.235995564476475]\n [0.017555205468589588, 0.011991074160098073, 0.0005694144205018892]\n [2.9341064618083235, 2.957720277786962, 3.7631308833438184]\n [0.08355705930996428, 0.09868312584495192, 16.708323298061178]\n [0.1439809343709656, 0.1335020515226761, 26.897349357733248]\n [0.1645878209199906, 0.15892637778403598, 29.25441870590682]\n [0.023896881157504005, -0.013675090924136792, 7.4843300243334365]\n [4.106839222388996, 4.473461790124109, 7.9214466035819635]\n [-0.47550108258884005, -0.4860137926531155, 12.734635921351629]\n [5.440942288045614, 5.862161117104718, 23.130221311332342]\n [4.892880857532143, 5.275366330814495, 12.792226972706137]\n ⋮\n [0.6423401153616347, 0.7359944325332999, 23.732878946460538]\n [2.519234311063701, 2.529353415928587, 3.5827775800975536]\n [2.088308258334852, 2.1081397117252565, 2.9312902609659215]\n [2.442287584724364, 2.462780413293451, 4.902743083455716]\n [2.2888836655888816, 2.2957369635098184, 4.09734744761817]\n [3.0525482059827014, 3.0664148208970974, 4.531257717050107]\n [3.0198041858344693, 3.0717061281815505, 7.337327433546069]\n [3.7455104424898713, 3.7604129809004503, 5.631139370594428]\n [-3.7532028322749538, -3.783303266679652, 8.669842002760918]\n [0.16487278105552758, 0.15910352757025786, 12.675401664157413]\n [1.1817629127943725, 1.1832218915995645, 1.4615938649753042]\n [1.0866052697001025, 1.09106295335647, 4.324846322028256]\n\n\nLet’s check the output:\n\nserial_out - threaded_out\n\n1000-element Vector{SVector{3, Float64}}:\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n ⋮\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n\n\nOh no, we don’t get the same answer! What happened?\nThe answer is the caching. Every single thread is using _u_cache as the cache, and so while one is writing into it the other is reading out of it, and thus is getting the value written to it from the wrong cache!\nTo fix this, what we need is a different heap per thread:\n\nconst _u_cache_threads = [Vector{typeof(@SVector([1.0,0.0,0.0]))}(undef,1000) for i in 1:Threads.nthreads()]\nfunction compute_trajectory_mean5(u0,p)\n  # u is automatically captured\n  solve_system_save!(_u_cache_threads[Threads.threadid()],lorenz,u0,p,1000);\n  mean(_u_cache_threads[Threads.threadid()])\nend\n@btime compute_trajectory_mean5(@SVector([1.0,0.0,0.0]),p)\n\n  5.441 μs (1 allocation: 32 bytes)\n\n\n3-element SVector{3, Float64} with indices SOneTo(3):\n -0.3114996234648468\n -0.30974901748976497\n 26.02460355858298\n\n\n\nserial_out = map(p -&gt; compute_trajectory_mean5(@SVector([1.0,0.0,0.0]),p),ps)\nthreaded_out = tmap(p -&gt; compute_trajectory_mean5(@SVector([1.0,0.0,0.0]),p),ps)\nserial_out - threaded_out\n\n1000-element Vector{SVector{3, Float64}}:\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n ⋮\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n [0.0, 0.0, 0.0]\n\n\n\n@btime serial_out = map(p -&gt; compute_trajectory_mean5(@SVector([1.0,0.0,0.0]),p),ps)\n\n  5.478 ms (3 allocations: 23.50 KiB)\n\n\n1000-element Vector{SVector{3, Float64}}:\n [1.2870613149922208, 1.1438117377304484, 22.669697978635234]\n [-0.097034701516529, -0.08504573089716216, 22.37001310300846]\n [0.27821116678919566, 0.27002118086184895, 12.235995564476475]\n [0.017555205468589588, 0.011991074160098073, 0.0005694144205018892]\n [2.9341064618083235, 2.957720277786962, 3.7631308833438184]\n [0.08355705930996428, 0.09868312584495192, 16.708323298061178]\n [0.1439809343709656, 0.1335020515226761, 26.897349357733248]\n [0.1645878209199906, 0.15892637778403598, 29.25441870590682]\n [0.023896881157504005, -0.013675090924136792, 7.4843300243334365]\n [4.106839222388996, 4.473461790124109, 7.9214466035819635]\n [-0.47550108258884005, -0.4860137926531155, 12.734635921351629]\n [5.440942288045614, 5.862161117104718, 23.130221311332342]\n [4.892880857532143, 5.275366330814495, 12.792226972706137]\n ⋮\n [0.6423401153616347, 0.7359944325332999, 23.732878946460538]\n [2.519234311063701, 2.529353415928587, 3.5827775800975536]\n [2.088308258334852, 2.1081397117252565, 2.9312902609659215]\n [2.442287584724364, 2.462780413293451, 4.902743083455716]\n [2.2888836655888816, 2.2957369635098184, 4.09734744761817]\n [3.0525482059827014, 3.0664148208970974, 4.531257717050107]\n [3.0198041858344693, 3.0717061281815505, 7.337327433546069]\n [3.7455104424898713, 3.7604129809004503, 5.631139370594428]\n [-3.7532028322749538, -3.783303266679652, 8.669842002760918]\n [0.16487278105552758, 0.15910352757025786, 12.675401664157413]\n [1.1817629127943725, 1.1832218915995645, 1.4615938649753042]\n [1.0866052697001025, 1.09106295335647, 4.324846322028256]\n\n\n\n@btime threaded_out = tmap(p -&gt; compute_trajectory_mean5(@SVector([1.0,0.0,0.0]),p),ps)\n\n  5.438 ms (9 allocations: 24.12 KiB)\n\n\n1000-element Vector{SVector{3, Float64}}:\n [1.2870613149922208, 1.1438117377304484, 22.669697978635234]\n [-0.097034701516529, -0.08504573089716216, 22.37001310300846]\n [0.27821116678919566, 0.27002118086184895, 12.235995564476475]\n [0.017555205468589588, 0.011991074160098073, 0.0005694144205018892]\n [2.9341064618083235, 2.957720277786962, 3.7631308833438184]\n [0.08355705930996428, 0.09868312584495192, 16.708323298061178]\n [0.1439809343709656, 0.1335020515226761, 26.897349357733248]\n [0.1645878209199906, 0.15892637778403598, 29.25441870590682]\n [0.023896881157504005, -0.013675090924136792, 7.4843300243334365]\n [4.106839222388996, 4.473461790124109, 7.9214466035819635]\n [-0.47550108258884005, -0.4860137926531155, 12.734635921351629]\n [5.440942288045614, 5.862161117104718, 23.130221311332342]\n [4.892880857532143, 5.275366330814495, 12.792226972706137]\n ⋮\n [0.6423401153616347, 0.7359944325332999, 23.732878946460538]\n [2.519234311063701, 2.529353415928587, 3.5827775800975536]\n [2.088308258334852, 2.1081397117252565, 2.9312902609659215]\n [2.442287584724364, 2.462780413293451, 4.902743083455716]\n [2.2888836655888816, 2.2957369635098184, 4.09734744761817]\n [3.0525482059827014, 3.0664148208970974, 4.531257717050107]\n [3.0198041858344693, 3.0717061281815505, 7.337327433546069]\n [3.7455104424898713, 3.7604129809004503, 5.631139370594428]\n [-3.7532028322749538, -3.783303266679652, 8.669842002760918]\n [0.16487278105552758, 0.15910352757025786, 12.675401664157413]\n [1.1817629127943725, 1.1832218915995645, 1.4615938649753042]\n [1.0866052697001025, 1.09106295335647, 4.324846322028256]\n\n\n\n\n\n4.2.5 Hierarchical Task-Based Multithreading and Dynamic Scheduling\nThe major change in Julia v1.3 is that Julia’s Tasks, which are traditionally its green threads interface, are now the basis of its multithreading infrastructure. This means that all independent threads are parallelized, and a new interface for multithreading will exist that works by spawning threads.\nThis implementation follows Go’s goroutines and the classic multithreading interface of Cilk. There is a Julia-level scheduler that handles the multithreading to put different tasks on different vCPU threads. A benefit from this is hierarchical multithreading. Since Julia’s tasks can spawn tasks, what can happen is a task can create tasks which create tasks which etc. In Julia (/Go/Cilk), this is then seen as a single pool of tasks which it can schedule, and thus it will still make sure only N are running at a time (as opposed to the naive implementation where the total number of running threads is equal then multiplied). This is essential for numerical performance because running multiple compute threads on a single CPU thread requires constant context switching between the threads, which will slow down the computations.\nTo directly use the task-based interface, simply use Threads.@spawn to spawn new tasks. For example:\n\nfunction tmap2(f,ps)\n  tasks = [Threads.@spawn f(ps[i]) for i in 1:1000]\n  out = [fetch(t) for t in tasks]\nend\nthreaded_out = tmap2(p -&gt; compute_trajectory_mean5(@SVector([1.0,0.0,0.0]),p),ps)\n\n1000-element Vector{SVector{3, Float64}}:\n [1.2870613149922208, 1.1438117377304484, 22.669697978635234]\n [-0.097034701516529, -0.08504573089716216, 22.37001310300846]\n [0.27821116678919566, 0.27002118086184895, 12.235995564476475]\n [0.017555205468589588, 0.011991074160098073, 0.0005694144205018892]\n [2.9341064618083235, 2.957720277786962, 3.7631308833438184]\n [0.08355705930996428, 0.09868312584495192, 16.708323298061178]\n [0.1439809343709656, 0.1335020515226761, 26.897349357733248]\n [0.1645878209199906, 0.15892637778403598, 29.25441870590682]\n [0.023896881157504005, -0.013675090924136792, 7.4843300243334365]\n [4.106839222388996, 4.473461790124109, 7.9214466035819635]\n [-0.47550108258884005, -0.4860137926531155, 12.734635921351629]\n [5.440942288045614, 5.862161117104718, 23.130221311332342]\n [4.892880857532143, 5.275366330814495, 12.792226972706137]\n ⋮\n [0.6423401153616347, 0.7359944325332999, 23.732878946460538]\n [2.519234311063701, 2.529353415928587, 3.5827775800975536]\n [2.088308258334852, 2.1081397117252565, 2.9312902609659215]\n [2.442287584724364, 2.462780413293451, 4.902743083455716]\n [2.2888836655888816, 2.2957369635098184, 4.09734744761817]\n [3.0525482059827014, 3.0664148208970974, 4.531257717050107]\n [3.0198041858344693, 3.0717061281815505, 7.337327433546069]\n [3.7455104424898713, 3.7604129809004503, 5.631139370594428]\n [-3.7532028322749538, -3.783303266679652, 8.669842002760918]\n [0.16487278105552758, 0.15910352757025786, 12.675401664157413]\n [1.1817629127943725, 1.1832218915995645, 1.4615938649753042]\n [1.0866052697001025, 1.09106295335647, 4.324846322028256]\n\n\nHowever, if we check the timing we see:\n\n@btime tmap2(p -&gt; compute_trajectory_mean5(@SVector([1.0,0.0,0.0]),p),ps)\n\n  5.763 ms (6005 allocations: 562.70 KiB)\n\n\n1000-element Vector{SVector{3, Float64}}:\n [1.2870613149922208, 1.1438117377304484, 22.669697978635234]\n [-0.097034701516529, -0.08504573089716216, 22.37001310300846]\n [0.27821116678919566, 0.27002118086184895, 12.235995564476475]\n [0.017555205468589588, 0.011991074160098073, 0.0005694144205018892]\n [2.9341064618083235, 2.957720277786962, 3.7631308833438184]\n [0.08355705930996428, 0.09868312584495192, 16.708323298061178]\n [0.1439809343709656, 0.1335020515226761, 26.897349357733248]\n [0.1645878209199906, 0.15892637778403598, 29.25441870590682]\n [0.023896881157504005, -0.013675090924136792, 7.4843300243334365]\n [4.106839222388996, 4.473461790124109, 7.9214466035819635]\n [-0.47550108258884005, -0.4860137926531155, 12.734635921351629]\n [5.440942288045614, 5.862161117104718, 23.130221311332342]\n [4.892880857532143, 5.275366330814495, 12.792226972706137]\n ⋮\n [0.6423401153616347, 0.7359944325332999, 23.732878946460538]\n [2.519234311063701, 2.529353415928587, 3.5827775800975536]\n [2.088308258334852, 2.1081397117252565, 2.9312902609659215]\n [2.442287584724364, 2.462780413293451, 4.902743083455716]\n [2.2888836655888816, 2.2957369635098184, 4.09734744761817]\n [3.0525482059827014, 3.0664148208970974, 4.531257717050107]\n [3.0198041858344693, 3.0717061281815505, 7.337327433546069]\n [3.7455104424898713, 3.7604129809004503, 5.631139370594428]\n [-3.7532028322749538, -3.783303266679652, 8.669842002760918]\n [0.16487278105552758, 0.15910352757025786, 12.675401664157413]\n [1.1817629127943725, 1.1832218915995645, 1.4615938649753042]\n [1.0866052697001025, 1.09106295335647, 4.324846322028256]\n\n\nThreads.@threads is built on the same multithreading infrastructure, so why is this so much slower? The reason is because Threads.@threads employs static scheduling while Threads.@spawn is using dynamic scheduling. Dynamic scheduling is the model of allowing the runtime to determine the ordering and scheduling of processes, i.e. what tasks will run run where and when. Julia’s task-based multithreading system has a thread scheduler which will automatically do this for you in the background, but because this is done at runtime it will have overhead. Static scheduling is the model of pre-determining where and when tasks will run, instead of allowing this to be determined at runtime. Threads.@threads is “quasi-static” in the sense that it cuts the loop so that it spawns only as many tasks as there are threads, essentially assigning one thread for even chunks of the input data.\nDoes this lack of runtime overhead mean that static scheduling is “better”? No, it simply has trade-offs. Static scheduling assumes that the runtime of each block is the same. For this specific case where there are fixed number of loop iterations for the dynamical systems, we know that every compute_trajectory_mean5 costs exactly the same, and thus this will be more efficient. However, There are many cases where this might not be efficient. For example:\n\nfunction sleepmap_static()\n  out = Vector{Int}(undef,24)\n  Threads.@threads for i in 1:24\n    sleep(i/10)\n    out[i] = i\n  end\n  out\nend\nisleep(i) = (sleep(i/10);i)\nfunction sleepmap_spawn()\n  tasks = [Threads.@spawn(isleep(i)) for i in 1:24]\n  out = [fetch(t) for t in tasks]\nend\n\n@btime sleepmap_static()\n@btime sleepmap_spawn()\n\n  30.032 s (1052 allocations: 30.67 KiB)\n\n\n  2.402 s (313 allocations: 17.55 KiB)\n\n\n24-element Vector{Int64}:\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n 10\n 11\n 12\n 13\n 14\n 15\n 16\n 17\n 18\n 19\n 20\n 21\n 22\n 23\n 24\n\n\nThe reason why this occurs is because of how the static scheduling had chunked my calculation. On my computer:\n\nThreads.nthreads()\n\n1\n\n\nThis means that there are 6 tasks that are created by Threads.@threads. The first takes:\n\nsum(i/10 for i in 1:4)\n\n1.0\n\n\n1 second, while the next group takes longer, then the next, etc. while the last takes:\n\nsum(i/10 for i in 21:24)\n\n9.0\n\n\n9 seconds (which is precisely the result!). Thus by unevenly distributing the runtime, we run as fast as the slowest thread. However, dynamic scheduling allows new tasks to immediately run when another is finished, meaning that the in that case the shorter tasks tend to be piled together, causing a faster execution. Thus whether dynamic or static scheduling is beneficial is dependent on the problem and the implementation of the static schedule.\n\n4.2.5.1 Possible Project\nNote that this can extend to external library calls as well. FFTW.jl recently gained support for this. A possible final project would be to do a similar change to OpenBLAS."
  },
  {
    "objectID": "parallelism_overview.html#a-teaser-for-alternative-parallelism-models",
    "href": "parallelism_overview.html#a-teaser-for-alternative-parallelism-models",
    "title": "4  The Basics of Single Node Parallel Computing",
    "section": "4.3 A Teaser for Alternative Parallelism Models",
    "text": "4.3 A Teaser for Alternative Parallelism Models\n\n4.3.1 Simplest Parallel Code\n\nA = rand(10000,10000)\nB = rand(10000,10000)\nA*B\n\n10000×10000 Matrix{Float64}:\n 2529.02  2525.04  2525.5   2523.41  …  2502.55  2491.57  2532.05  2527.85\n 2490.83  2481.74  2493.91  2512.91     2482.09  2488.14  2486.98  2471.21\n 2515.04  2502.64  2514.29  2511.42     2488.54  2488.75  2507.64  2512.36\n 2503.85  2486.43  2481.52  2469.86     2485.5   2462.59  2491.61  2476.54\n 2517.15  2497.31  2502.47  2514.01     2494.42  2475.74  2499.98  2503.78\n 2518.94  2502.0   2504.1   2513.73  …  2503.59  2480.16  2513.11  2505.43\n 2536.13  2531.77  2531.96  2560.98     2531.99  2503.31  2541.91  2520.26\n 2524.15  2502.71  2527.27  2516.97     2493.25  2498.01  2500.6   2510.92\n 2465.45  2468.05  2486.37  2477.11     2467.8   2470.96  2482.67  2461.55\n 2484.35  2479.11  2476.05  2510.38     2474.17  2464.76  2477.68  2475.19\n 2492.32  2485.88  2483.0   2490.94  …  2482.6   2472.82  2487.66  2486.47\n 2528.0   2501.62  2510.88  2528.93     2496.21  2486.49  2508.73  2504.82\n 2490.97  2482.01  2483.6   2476.63     2481.24  2452.17  2471.32  2469.44\n    ⋮                                ⋱                             \n 2494.77  2492.79  2500.69  2495.52     2473.33  2469.06  2503.87  2479.53\n 2504.33  2484.09  2487.61  2500.92     2494.83  2468.28  2501.73  2487.18\n 2512.03  2501.31  2507.81  2516.98  …  2493.84  2471.47  2506.51  2501.31\n 2525.0   2495.28  2497.91  2510.05     2476.66  2475.31  2503.12  2483.54\n 2498.65  2487.66  2484.11  2485.84     2482.62  2475.31  2483.04  2470.14\n 2484.89  2461.14  2480.86  2476.08     2464.63  2445.52  2479.57  2482.56\n 2480.27  2481.26  2493.57  2487.54     2483.09  2458.73  2501.45  2485.35\n 2525.02  2514.33  2519.78  2516.64  …  2511.22  2480.74  2530.18  2505.86\n 2545.12  2534.92  2533.85  2532.71     2526.29  2514.33  2528.7   2521.14\n 2512.11  2516.87  2513.09  2512.33     2479.93  2491.69  2501.4   2504.96\n 2505.0   2505.1   2492.63  2511.32     2493.35  2474.13  2509.69  2495.74\n 2518.78  2500.49  2507.42  2525.53     2496.14  2493.69  2506.43  2509.91\n\n\nIf you are using a computer that has N cores, then this will use N cores. Try it and look at your resource usage!\n\n\n4.3.2 Array-Based Parallelism\nThe simplest form of parallelism is array-based parallelism. The idea is that you use some construction of an array whose operations are already designed to be parallel under the hood. In Julia, some examples of this are:\n\nDistributedArrays (Distributed Computing)\nElemental\nMPIArrays\nCuArrays (GPUs)\n\nThis is not a Julia specific idea either.\n\n\n4.3.3 BLAS and Standard Libraries\nThe basic linear algebra calls are all handled by a set of libraries which follow the same interface known as BLAS (Basic Linear Algebra Subroutines). It’s divided into 3 portions:\n\nBLAS1: Element-wise operations (O(n))\nBLAS2: Matrix-vector operations (O(n^2))\nBLAS3: Matrix-matrix operations (O(n^3))\n\nBLAS implementations are highly optimized, like OpenBLAS and Intel MKL, so every numerical language and library essentially uses similar underlying BLAS implementations. Extensions to these, known as LAPACK, include operations like factorizations, and are included in these standard libraries. These are all multithreaded. The reason why this is a location to target is because the operation count is high enough that parallelism can be made efficient even when only targeting this level: a matrix multiplication can take on the order of seconds, minutes, hours, or even days, and these are all highly parallel operations. This means you can get away with a bunch just by parallelizing at this level, which happens to be a bottleneck for a lot scientific computing codes.\nThis is also commonly the level at which GPU computing occurs in machine learning libraries for reasons which we will explain later.\n\n\n4.3.4 MPI\nWell, this is a big topic and we’ll address this one later!"
  },
  {
    "objectID": "parallelism_overview.html#conclusion",
    "href": "parallelism_overview.html#conclusion",
    "title": "4  The Basics of Single Node Parallel Computing",
    "section": "4.4 Conclusion",
    "text": "4.4 Conclusion\nThe easiest forms of parallelism are:\n\nEmbarrassingly parallel\nArray-level parallelism (built into linear algebra)\n\nExploit these when possible."
  },
  {
    "objectID": "styles_of_parallelism.html#youtube-video-link",
    "href": "styles_of_parallelism.html#youtube-video-link",
    "title": "5  The Different Flavors of Parallelism",
    "section": "5.1 Youtube Video Link",
    "text": "5.1 Youtube Video Link\nNow that you are aware of the basics of parallel computing, let’s give a high level overview of the differences between different modes of parallelism."
  },
  {
    "objectID": "styles_of_parallelism.html#lowest-level-simd",
    "href": "styles_of_parallelism.html#lowest-level-simd",
    "title": "5  The Different Flavors of Parallelism",
    "section": "5.2 Lowest Level: SIMD",
    "text": "5.2 Lowest Level: SIMD\nRecall SIMD, the idea that processors can run multiple commands simultaneously on specially structured data. “Single Instruction Multiple Data”. SIMD is parallelism within a single core.\n\n5.2.1 High Level Idea of SIMD\nCalculations can occur in parallel in the processor if there is sufficient structure in the computation.\n\n\n5.2.2 How to do SIMD\nThe simplest way to do SIMD is simply to make sure that your values are aligned. If they are, then great, LLVM’s autovectorizer pass has a good chance of automatic vectorization (in the world of computing, “SIMD” is synonymous with vectorization since it is taking specific values and instead computing on small vectors. That is not to be confused with “vectorization” in the sense of Python/R/MATLAB, which is a programming style which prefers using C-defined primitive functions, like broadcast or matrix multiplication).\nYou can check for auto-vectorization inside of the LLVM IR by looking for statements like:\n%wide.load24 = load &lt;4 x double&gt;, &lt;4 x double&gt; addrspac(13)* %46, align 8\n; └\n; ┌ @ float.jl:395 within `+'\n%47 = fadd &lt;4 x double&gt; %wide.load, %wide.load24\nwhich means that 4 additions are happening simultaneously. The amount of vectorization is heavily dependent on your architecture. The ancient form of SIMD, the SSE(2) instructions, required that your data was aligned. Now there’s a bit more leeway, but generally it holds that making your the data you’re trying to SIMD over is aligned. Thus there can be major differences in computing using a struct of array format instead of an arrays of structs format. For example:\n\nstruct MyComplex\n  real::Float64\n  imag::Float64\nend\narr = [MyComplex(rand(),rand()) for i in 1:100]\n\n100-element Vector{MyComplex}:\n MyComplex(0.5171425799729418, 0.5389833329835383)\n MyComplex(0.9273852992839166, 0.6711676516426572)\n MyComplex(0.9188839358734163, 0.69566633863532)\n MyComplex(0.5149805563839515, 0.34684634193884556)\n MyComplex(0.5025762934009232, 0.9317528981610566)\n MyComplex(0.6182965305758257, 0.4714678558437191)\n MyComplex(0.9637130781038232, 0.35381659885390127)\n MyComplex(0.05303934651739051, 0.5283571896609601)\n MyComplex(0.7640379612452078, 0.030918641696845883)\n MyComplex(0.7879963381227947, 0.877727975786527)\n MyComplex(0.40923630065090066, 0.2841032996700653)\n MyComplex(0.5310039504202523, 0.3809548581888238)\n MyComplex(0.28644668740025714, 0.040403772416303)\n ⋮\n MyComplex(0.966889263377449, 0.225967114853175)\n MyComplex(0.7583992061643928, 0.8376264040502798)\n MyComplex(0.4950710312486035, 0.38216405552956967)\n MyComplex(0.5577584208258319, 0.03476798870449893)\n MyComplex(0.2612858158174869, 0.9293738045669759)\n MyComplex(0.16877190046116053, 0.6768799829602348)\n MyComplex(0.610591500686496, 0.1714927381851986)\n MyComplex(0.025248616089485476, 0.8845033232694497)\n MyComplex(0.6623917047313289, 0.45970378019654157)\n MyComplex(0.22731260398681374, 0.38286776865734873)\n MyComplex(0.01214026543643354, 0.15850002116286088)\n MyComplex(0.17568792291939672, 0.7317199034423749)\n\n\nis represented in memory as\n[real1,imag1,real2,imag2,...]\nwhile the struct of array formats are\n\nstruct MyComplexes\n  real::Vector{Float64}\n  imag::Vector{Float64}\nend\narr2 = MyComplexes(rand(100),rand(100))\n\nMyComplexes([0.5240456439259698, 0.7641990890595322, 0.3272012223563314, 0.32665635557581374, 0.6208235049962002, 0.017636574146488226, 0.6754741671723067, 0.6606017902042781, 0.9031998261731529, 0.7511761900676958  …  0.35946892646270157, 0.8366647043642217, 0.80495314181679, 0.17012580793194387, 0.4266440935361443, 0.8248150023004187, 0.7475526145122053, 0.6682331655066719, 0.7488365584042757, 0.6216662994614262], [0.5147671464058595, 0.4030123955856699, 0.8925325487199312, 0.8974490320038392, 0.26696452517979885, 0.15703223245821918, 0.5753006191470085, 0.18369838826811646, 0.1616695840779412, 0.9874418694199811  …  0.9053886752455029, 0.23022308636435107, 0.6651321483834207, 0.6417777842027098, 0.030151577749300107, 0.03298300488010508, 0.58854566946127, 0.7105727582002626, 0.9149637304123415, 0.5216242759285099])\n\n\nNow let’s check what happens when we perform a reduction:\n\nusing InteractiveUtils\nBase.:+(x::MyComplex,y::MyComplex) = MyComplex(x.real+y.real,x.imag+y.imag)\nBase.:/(x::MyComplex,y::Int) = MyComplex(x.real/y,x.imag/y)\naverage(x::Vector{MyComplex}) = sum(x)/length(x)\n@code_llvm average(arr)\n\n;  @ In[5]:4 within `average`\ndefine \n\n\nvoid @julia_average_1306([2 x double]* noalias nocapture noundef nonnull sret([2 x double]) align 8 dereferenceable(16) %0, {}* noundef nonnull align 16 dereferenceable(40) %1) #0 {\ntop:\n  %2 = alloca [4 x {}*], align 8\n  %3 = alloca &lt;2 x double&gt;, align 16\n  %tmpcast = bitcast &lt;2 x double&gt;* %3 to [2 x double]*\n  %.sub = getelementptr inbounds [4 x {}*], [4 x {}*]* %2, i64 0, i64 0\n; ┌ @ reducedim.jl:994 within `sum`\n; │┌ @ reducedim.jl:994 within `#sum#808`\n; ││┌ @ reducedim.jl:998 within `_sum`\n; │││┌ @ reducedim.jl:998 within `#_sum#810`\n; ││││┌ @ reducedim.jl:999 within `_sum`\n; │││││┌ @ reducedim.jl:999 within `#_sum#811`\n; ││││││┌ @ reducedim.jl:357 within `mapreduce`\n; │││││││┌ @ reducedim.jl:357 within `#mapreduce#801`\n; ││││││││┌ @ reducedim.jl:365 within `_mapreduce_dim`\n; │││││││││┌ @ reduce.jl:424 within `_mapreduce`\n; ││││││││││┌ @ indices.jl:486 within `LinearIndices`\n; │││││││││││┌ @ abstractarray.jl:98 within `axes`\n; ││││││││││││┌ @ array.jl:149 within `size`\n               %4 = bitcast {}* %1 to { i8*, i64, i16, i16, i32 }*\n               %5 = getelementptr inbounds { i8*, i64, i16, i16, i32 }, { i8*, i64, i16, i16, i32 }* %4, i64 0, i32 1\n               %6 = load i64, i64* %5, align 8\n; ││││││││││└└└\n; ││││││││││ @ reduce.jl:426 within `_mapreduce`\n            switch i64 %6, label %L14 [\n    i64 0, label %L8\n    i64 1, label %L12\n  ]\n\nL8:                                               ; preds = %top\n; ││││││││││ @ reduce.jl:427 within `_mapreduce`\n            store {}* inttoptr (i64 140224218384336 to {}*), {}** %.sub, align 8\n            %7 = getelementptr inbounds [4 x {}*], [4 x {}*]* %2, i64 0, i64 1\n            store {}* inttoptr (i64 140224214304000 to {}*), {}** %7, align 8\n            %8 = getelementptr inbounds [4 x {}*], [4 x {}*]* %2, i64 0, i64 2\n            store {}* %1, {}** %8, align 8\n            %9 = getelementptr inbounds [4 x {}*], [4 x {}*]* %2, i64 0, i64 3\n            store {}* inttoptr (i64 140224235746528 to {}*), {}** %9, align 8\n            %10 = call nonnull {}* @ijl_invoke({}* inttoptr (i64 140224233214240 to {}*), {}** nonnull %.sub, i32 4, {}* inttoptr (i64 140224414853696 to {}*))\n            call void @llvm.trap()\n            unreachable\n\nL12:                                              ; preds = %top\n; ││││││││││ @ reduce.jl:429 within `_mapreduce`\n; ││││││││││┌ @ essentials.jl:13 within `getindex`\n             %11 = bitcast {}* %1 to &lt;2 x double&gt;**\n             %12 = load &lt;2 x double&gt;*, &lt;2 x double&gt;** %11, align 8\n             %13 = load &lt;2 x double&gt;, &lt;2 x double&gt;* %12, align 1\n             br label %L46\n\nL14:                                              ; preds = %top\n; ││││││││││└\n; ││││││││││ @ reduce.jl:431 within `_mapreduce`\n; ││││││││││┌ @ int.jl:83 within `&lt;`\n             %14 = icmp ugt i64 %6, 15\n; ││││││││││└\n            br i1 %14, label %L42, label %L16\n\nL16:                                              ; preds = %L14\n; ││││││││││ @ reduce.jl:433 within `_mapreduce`\n; ││││││││││┌ @ essentials.jl:13 within `getindex`\n             %15 = bitcast {}* %1 to [2 x double]**\n             %16 = load [2 x double]*, [2 x double]** %15, align 8\n             %17 = bitcast [2 x double]* %16 to &lt;2 x double&gt;*\n             %18 = load &lt;2 x double&gt;, &lt;2 x double&gt;* %17, align 1\n; ││││││││││└\n; ││││││││││ @ reduce.jl:434 within `_mapreduce`\n; ││││││││││┌ @ essentials.jl:13 within `getindex`\n             %.sroa.028.0..sroa_idx = getelementptr inbounds [2 x double], [2 x double]* %16, i64 1, i64 0\n             %19 = bitcast double* %.sroa.028.0..sroa_idx to &lt;2 x double&gt;*\n             %20 = load &lt;2 x double&gt;, &lt;2 x double&gt;* %19, align 1\n; ││││││││││└\n; ││││││││││ @ reduce.jl:435 within `_mapreduce`\n; ││││││││││┌ @ reduce.jl:24 within `add_sum`\n; │││││││││││┌ @ In[5]:2 within `+` @ float.jl:408\n              %21 = fadd &lt;2 x double&gt; %18, %20\n; ││││││││││└└\n; ││││││││││ @ reduce.jl:436 within `_mapreduce`\n; ││││││││││┌ @ int.jl:83 within `&lt;`\n             %.not6482 = icmp ugt i64 %6, 2\n; ││││││││││└\n\n\n\n            br i1 %.not6482, label %L33, label %L46\n\nL33:                                              ; preds = %L33, %L16\n            %value_phi285 = phi i64 [ %23, %L33 ], [ 2, %L16 ]\n            %22 = phi &lt;2 x double&gt; [ %26, %L33 ], [ %21, %L16 ]\n; ││││││││││ @ reduce.jl:437 within `_mapreduce`\n; ││││││││││┌ @ int.jl:87 within `+`\n             %23 = add nuw nsw i64 %value_phi285, 1\n; ││││││││││└\n; ││││││││││┌ @ essentials.jl:13 within `getindex`\n             %.sroa.0.0..sroa_idx = getelementptr inbounds [2 x double], [2 x double]* %16, i64 %value_phi285, i64 0\n             %24 = bitcast double* %.sroa.0.0..sroa_idx to &lt;2 x double&gt;*\n             %25 = load &lt;2 x double&gt;, &lt;2 x double&gt;* %24, align 1\n; ││││││││││└\n; ││││││││││ @ reduce.jl:438 within `_mapreduce`\n; ││││││││││┌ @ reduce.jl:24 within `add_sum`\n; │││││││││││┌ @ In[5]:2 within `+` @ float.jl:408\n              %26 = fadd &lt;2 x double&gt; %22, %25\n; ││││││││││└└\n; ││││││││││ @ reduce.jl:436 within `_mapreduce`\n; ││││││││││┌ @ int.jl:83 within `&lt;`\n             %exitcond.not = icmp eq i64 %23, %6\n; ││││││││││└\n            br i1 %exitcond.not, label %L46, label %L33\n\nL42:                                              ; preds = %L14\n; ││││││││││ @ reduce.jl:442 within `_mapreduce`\n; ││││││││││┌ @ reduce.jl:272 within `mapreduce_impl`\n             call void @j_mapreduce_impl_1308([2 x double]* noalias nocapture noundef nonnull sret([2 x double]) %tmpcast, {}* nonnull %1, i64 signext 1, i64 signext %6, i64 signext 1024) #0\n; └└└└└└└└└└└\n  %27 = load &lt;2 x double&gt;, &lt;2 x double&gt;* %3, align 16\n; ┌ @ essentials.jl:10 within `length`\n   %.pre = load i64, i64* %5, align 8\n   br label %L46\n\nL46:                                              ; preds = %L42, %L33, %L16, %L12\n   %28 = phi i64 [ %.pre, %L42 ], [ 1, %L12 ], [ %6, %L16 ], [ %6, %L33 ]\n   %29 = phi &lt;2 x double&gt; [ %27, %L42 ], [ %13, %L12 ], [ %21, %L16 ], [ %26, %L33 ]\n; └\n; ┌ @ In[5]:3 within `/` @ promotion.jl:413\n; │┌ @ promotion.jl:381 within `promote`\n; ││┌ @ promotion.jl:358 within `_promote`\n; │││┌ @ number.jl:7 within `convert`\n; ││││┌ @ float.jl:159 within `Float64`\n       %30 = sitofp i64 %28 to double\n; │└└└└\n; │ @ In[5]:3 within `/` @ promotion.jl:413 @ float.jl:411\n   %31 = insertelement &lt;2 x double&gt; poison, double %30, i64 0\n   %32 = shufflevector &lt;2 x double&gt; %31, &lt;2 x double&gt; poison, &lt;2 x i32&gt; zeroinitializer\n   %33 = fdiv &lt;2 x double&gt; %29, %32\n; └\n  %34 = bitcast [2 x double]* %0 to &lt;2 x double&gt;*\n  store &lt;2 x double&gt; %33, &lt;2 x double&gt;* %34, align 8\n  ret void\n}\n\n\nWhat this is doing is creating small little vectors and then parallelizing the operations of those vectors by calling specific vector-parallel instructions. Keep this in mind.\n\n\n5.2.3 Explicit SIMD\nThe following was all a form of loop-level parallelism known as loop vectorization. It’s simply easier for compilers to reason at the array level, prove iterates are independent, and automatically generate SIMD code from that. This is not necessary, and compilers can produce SIMD code from non-looping code through a process known as SLP supervectorization, but the results are far from optimal and the compiler requires a lot of time to do this calculation, meaning that it’s usually not a pass used by default.\nIf you want to pack the vectors yourself, then primitives for doing so from within Julia are available in SIMD.jl. This is for “real” performance warriors. This looks like for example:\n\nusing SIMD\nv = Vec{4,Float64}((1,2,3,4))\n@show v+v # basic arithmetic is supported\n@show sum(v) # basic reductions are supported\n\nv + v = &lt;4 x Float64&gt;[2.0, 4.0, 6.0, 8.0]\nsum(v) = 10.0\n\n\n10.0\n\n\nUsing this you can pull apart code and force the usage of SIMD vectors. One library which makes great use of this is LoopVectorization.jl. However, one word of “caution”:\nMost performance optimization is not trying to do something really good for performance. Most performance optimization is trying to not do something that is actively bad for performance.\n\n\n5.2.4 Summary of SIMD\n\nCommunication in SIMD is due to locality: if things are local the processor can automatically setup the operations.\nThere’s no real worry about “getting it wrong”: you cannot overwrite pieces from different parts of the arithmetic unit, and if SIMD is unsafe then it just won’t auto-vectorize.\nSuitable for operations measured in ns."
  },
  {
    "objectID": "styles_of_parallelism.html#next-level-up-multithreading",
    "href": "styles_of_parallelism.html#next-level-up-multithreading",
    "title": "5  The Different Flavors of Parallelism",
    "section": "5.3 Next Level Up: Multithreading",
    "text": "5.3 Next Level Up: Multithreading\nLast time we briefly went over multithreading and described how every process has multiple threads which share a single heap, and when multiple threads are executed simultaneously we have multithreaded parallelism. Note that you can have multiple threads which aren’t executed simultaneously, like in the case of I/O operations, and this is an example of concurrency without parallelism and is commonly referred to as green threads.\n\nLast time we described a simple multithreaded program and noticed that multithreading has an overhead cost of around 50ns-100ns. This is due to the construction of the new stack (among other things) each time a new computational thread is spun up. This means that, unlike SIMD, some thought needs to be put in as to when to perform multithreading: it’s not always a good idea. It needs to be high enough on the cost for this to be counter-balanced.\nOne abstraction that was glossed over was the memory access style. Before, we were considering a single heap, or an UMA style:\n\nHowever, this is the case for all shared memory devices. For example, compute nodes on the HPC tend to be “dual Xeon” or “quad Xeon”, where each Xeon processor is itself a multicore processor. But each processor on its own accesses its own local caches, and thus one has to be aware that this is setup in a NUMA (non-uniform memory access) manner:\n\nwhere there is a cache that is closer to the processor and a cache that is further away. Care should be taken in this to localize the computation per thread, otherwise a cost associated with the memory sharing will be hit (but all sharing will still be automatic).\nIn this sense, interthread communication is naturally done through the heap: if you want other threads to be able to touch a value, then you can simply place it on the heap and then it’ll be available. We saw this last time by how overlapping computations can re-use the same heap-based caches, meaning that care needs to be taken with how one writes into a dynamically-allocated array.\nA simple example that demonstrates this is. First, let’s make sure we have multithreading enabled:\n\nusing Base.Threads\nThreads.nthreads() # should not be 1\n\n1\n\n\n\nusing BenchmarkTools\nacc = 0\n@threads for i in 1:10_000\n    global acc\n    acc += 1\nend\nacc\n\n10000\n\n\nThe reason for this behavior is that there is a difference between the reading and the writing step to an array. Here, values are being read while other threads are writing, meaning that they see a lower value than when they are attempting to write into it. The result is that the total summation is lower than the true value because of this clashing. We can prevent this by only allowing one thread to utilize the heap-allocated variable at a time. One abstraction for doing this is atomics:\n\nacc = Atomic{Int64}(0)\n@threads for i in 1:10_000\n    atomic_add!(acc, 1)\nend\nacc\n\nAtomic{Int64}(10000)\n\n\nWhen an atomic add is being done, all other threads wishing to do the same computation are blocked. This of course can have a massive effect on performance since atomic computations are not parallel.\nJulia also exposes a lower level of heap control in threading using locks\n\nconst acc_lock = Ref{Int64}(0)\nconst splock = SpinLock()\nfunction f1()\n    @threads for i in 1:10_000\n        lock(splock)\n        acc_lock[] += 1\n        unlock(splock)\n    end\nend\nconst rsplock = ReentrantLock()\nfunction f2()\n    @threads for i in 1:10_000\n        lock(rsplock)\n        acc_lock[] += 1\n        unlock(rsplock)\n    end\nend\nacc2 = Atomic{Int64}(0)\nfunction g()\n  @threads for i in 1:10_000\n      atomic_add!(acc2, 1)\n  end\nend\nconst acc_s = Ref{Int64}(0)\nfunction h()\n  global acc_s\n  for i in 1:10_000\n      acc_s[] += 1\n  end\nend\n@btime f1()\n\n  157.285 μs (7 allocations: 640 bytes)\n\n\nSpinLock is non-reentrant, i.e. it will block itself if a thread that calls a lock does another lock. Therefore it has to be used with caution (every lock goes with one unlock), but it’s fast. ReentrantLock alleviates those concerns, but trades off a bit of performance:\n\n@btime f2()\n\n  166.921 μs (7 allocations: 640 bytes)\n\n\nBut if you can use atomics, they will be faster:\n\n@btime g()\n\n  46.660 μs (7 allocations: 640 bytes)\n\n\nand if your computation is actually serial, then use serial code:\n\n@btime h()\n\n  2.329 ns (0 allocations: 0 bytes)\n\n\nWhy is this so fast? Check the code:\n\n@code_llvm h()\n\n;  @ In[10]:25 within `h`\ndefine void @julia_h_1791() #0 {\ntop:\n  %.promoted = load i64, i64* inttoptr (i64 140224426130176 to i64*), align 256\n;  @ In[10]:27 within `h`\n  %0 = add i64 %.promoted, 10000\n;  @ In[10]:28 within `h`\n; ┌ @ Base.jl within `setproperty!`\n   store i64 %0, i64* inttoptr (i64 140224426130176 to i64*), align 256\n; └\n;  @ In[10]:29 within `h`\n  ret void\n}\n\n\nIt just knows to add 10000. So to get a proper timing let’s make the size mutable:\n\nconst len = Ref{Int}(10_000)\nfunction h2()\n  global acc_s\n  global len\n  for i in 1:len[]\n      acc_s[] += 1\n  end\nend\n@btime h2()\n\n  2.393 ns (0 allocations: 0 bytes)\n\n\n\n@code_llvm h2()\n\n;  @ In[15]:2 within `h2`\ndefine void @julia_h2_1798() #0 {\ntop:\n;  @ In[15]:5 within `h2`\n; ┌ @ refvalue.jl:56 within `getindex`\n; │┌ @ Base.jl:37 within `getproperty`\n    %0 = load i64, i64* inttoptr (i64 140224426639520 to i64*), align 32\n; └└\n; ┌ @ range.jl:5 within `Colon`\n; │┌ @ range.jl:397 within `UnitRange`\n; ││┌ @ range.jl:404 within `unitrange_last`\n     %.inv = icmp sgt i64 %0, 0\n; └└└\n  br i1 %.inv, label %L18.preheader, label %L34\n\nL18.preheader:                                    ; preds = %top\n  %.promoted = load i64, i64* inttoptr (i64 140224426130176 to i64*), align 256\n;  @ In[15]:7 within `h2`\n  %1 = add i64 %.promoted, %0\n;  @ In[15]:6 within `h2`\n; ┌ @ Base.jl within `setproperty!`\n   store i64 %1, i64* inttoptr (i64 140224426130176 to i64*), align 256\n; └\n;  @ In[15]:7 within `h2`\n  br label %L34\n\nL34:                                              ; preds = %L18.preheader, %top\n  ret void\n}\n\n\nIt’s still optimizing it!\n\nnon_const_len = 10000\nfunction h3()\n  global acc_s\n  global non_const_len\n  len2::Int = non_const_len\n  for i in 1:len2\n      acc_s[] += 1\n  end\nend\n@btime h3()\n\n  99.019 ns (0 allocations: 0 bytes)\n\n\nNote that what is shown here is a type-declaration. a::T = ... forces a to be of type T throughout the whole function. By giving the compiler this information, I am able to use the non-constant global in a type-stable manner.\nOne last thing to note about multithreaded computations, and parallel computations, is that one cannot assume that the parallelized computation is computed in any given order. For example, the following will has a quasi-random ordering:\n\nconst a2 = zeros(nthreads()*10)\nconst acc_lock2 = Ref{Int64}(0)\nconst splock2 = SpinLock()\nfunction f_order()\n    @threads for i in 1:length(a2)\n        lock(splock2)\n        acc_lock2[] += 1\n        a2[i] = acc_lock2[]\n        unlock(splock2)\n    end\nend\nf_order()\na2\n\n10-element Vector{Float64}:\n  1.0\n  2.0\n  3.0\n  4.0\n  5.0\n  6.0\n  7.0\n  8.0\n  9.0\n 10.0\n\n\nNote that here we can see that Julia 1.5 is dividing up the work into groups of 10 for each thread, and then one thread dominates the computation at a time, but which thread dominates is random.\n\n5.3.1 The Dining Philosophers Problem\nA classic tale in parallel computing is the dining philosophers problem. In this case, there are N philosophers at a table who all want to eat at the same time, following all of the same rules. Each philosopher must alternatively think and then eat. They need both their left and right fork to start eating, but cannot start eating until they have both forks. The problem is how to setup a concurrent algorithm that will not cause any philosophers to starve.\nThe difficulty is a situation known as deadlock. For example, if each philosopher was told to grab the right fork when it’s available, and then the left fork, and put down the fork after eating, then they will all grab the right fork and none will ever eat because they will all be waiting on the left fork. This is analogous to two blocked computations which are waiting on the other to finish. Thus, when using blocking structures, one needs to be careful about deadlock!\n\n\n5.3.2 Two Programming Models: Loop-Level Parallelism and Task-Based Parallelism\nAs described in the previous lecture, one can also use Threads.@spawn to do multithreading in Julia v1.3+. The same factors all applay: how to do locks and Mutex etc. This is a case of a parallelism construct having two alternative programming models. Threads.@spawn represents task-based parallelism, while Threads.@threads represents Loop-Level Parallelism or a parallel iterator model. Loop-based parallelization models are very high level and, assuming every iteration is independent, almost requires no code change. Task-based parallelism is a more expressive parallelism model, but usually requires modifying the code to be explicitly written as a set of parallelizable tasks. Note that in the case of Julia, Threads.@threads is implemented using Threads.@spawn’s model.\n\n\n5.3.3 Summary of Multithreading\n\nCommunication in multithreading is done on the heap. Locks and atomics allow for a form of safe message passing.\n50ns-100ns of overhead. Suitable for 1μs calculations.\nBe careful of ordering and heap-allocated values."
  },
  {
    "objectID": "styles_of_parallelism.html#gpu-computing",
    "href": "styles_of_parallelism.html#gpu-computing",
    "title": "5  The Different Flavors of Parallelism",
    "section": "5.4 GPU Computing",
    "text": "5.4 GPU Computing\nGPUs are not fast. In fact, the problem with GPUs is that each processor is slow. However, GPUs have a lot of cores… like thousands.\n\nAn RTX2080, a standard “gaming” GPU (not even the ones in the cluster), has 2944 cores. However, not only are GPUs slow, but they also need to be programmed in a style that is SPMD, which standard for Single Program Multiple Data. This means that every single thread must be running the same program but on different pieces of data. Exactly the same program. If you have\n\nif a &gt; 1\n  # Do something\nelse\n  # Do something else\nend\n\nwhere some of the data goes on one branch and other data goes on the other branch, every single thread will run both branches (performing “fake” computations while on the other branch). This means that GPU tasks should be “very parallel” with as few conditionals as possible.\n\n5.4.1 GPU Memory\nGPUs themselves are shared memory devices, meaning they have a heap that is shared amongst all threads. However, GPUs are heavily in the NUMA camp, where different blocks of the GPU have much faster access to certain parts of the memory. Additionally, this heap is disconnected from the standard processor, so data must be passed to the GPU and data must be returned.\nGPU memory size is relatively small compared to CPUs. Example: the RTX2080Ti has 8GB of RAM. Thus one needs to be doing computations that are memory compact (such as matrix multiplications, which are O(n^3) making the computation time scale quicker than the memory cost).\n\n\n5.4.2 Note on GPU Hardware\nStandard GPU hardware “for gaming”, like RTX2070, is just as fast as higher end GPU hardware for Float32. Higher end hardware, like the Tesla, add more memory, memory safety, and Float64 support. However, these require being in a server since they have alternative cooling strategies, making them a higher end product.\n\n\n5.4.3 SPMD Kernel Generation GPU Computing Models\nThe core programming models for GPU computing are SPMD kernel compilers, of which the most well-known is CUDA. CUDA is a C++-like programming language which compiles to .ptx kernels, and GPU execution on NVIDIA GPUs is done by “all steams” of a GPU doing concurrent execution of the kernel (generally, without going into more details, you can of “all streams” as just meaning “all cores”. More detailed views of GPU execution will come later).\n.ptx CUDA kernels can be compiled from LLVM IR, and thus since Julia is a programming language which emits LLVM IR for all of its operations, native Julia programs are compatible with compilation to CUDA. The helper functions to enable this separate compilation path is CUDA.jl. Let’s take a look at a basic CUDA.jl kernel generating example:\n\nusing CUDA\n\nN = 2^20\nx_d = CUDA.fill(1.0f0, N)  # a vector stored on the GPU filled with 1.0 (Float32)\ny_d = CUDA.fill(2.0f0, N)  # a vector stored on the GPU filled with 2.0\n\nfunction gpu_add2!(y, x)\n    index = threadIdx().x    # this example only requires linear indexing, so just use `x`\n    stride = blockDim().x\n    for i = index:stride:length(y)\n        @inbounds y[i] += x[i]\n    end\n    return nothing\nend\n\nfill!(y_d, 2)\n@cuda threads=256 gpu_add2!(y_d, x_d)\nall(Array(y_d) .== 3.0f0)\n\ntrue\n\n\nThe key to understanding the SPMD kernel approach is the index = threadIdx().x and stride = blockDim().x portions.\n\nThe way kernels are expected to run in parallel is that they are given a specific block of the computation and are expected to write a kernel which only on that small block of the input. This kernel is then called on every separate thread on the GPU, making each CUDA core simultaneously compute each block. Thus as a user in such a SPMD programming model, you never specify the computation globally but instead simply specify how chunks should behave, giving the compiler the leeway to determine the optimal global execution.\n\n\n5.4.4 Array-Based GPU Computing Models\nThe simplest version of GPU computing is the array-based programming model.\n\nA = rand(100,100); B = rand(100,100)\nusing CUDA\n# Pass to the GPU\ncuA = cu(A); cuB = cu(B)\ncuC = cuA*cuB\n# Pass to the CPU\nC = Array(cuC)\n\n100×100 Matrix{Float32}:\n 25.0564  28.1081  24.5791  24.2118  …  29.1066  25.3427  26.785   26.8159\n 23.8365  26.9323  22.1268  24.394      25.1406  24.2087  23.0141  24.5436\n 23.8291  27.5819  20.8153  24.5719     26.0579  24.7479  24.5362  24.5483\n 22.3662  25.2902  20.4605  22.7104     23.514   22.1021  23.3403  23.0844\n 24.8252  29.7588  22.9885  28.0034     28.7318  26.3997  26.019   27.2803\n 23.8504  25.0349  20.1403  22.2076  …  24.6679  21.7806  21.7851  22.3799\n 20.0263  22.023   17.9829  20.6622     21.3617  19.9235  21.8351  21.5375\n 22.3813  25.5924  19.3125  22.7719     24.4725  23.848   23.5627  23.3131\n 25.0231  29.8407  23.5434  25.3326     26.7342  24.6092  25.2959  24.4861\n 22.874   26.0299  20.9581  23.9912     25.9096  23.7     23.8586  24.6916\n 22.7543  26.2432  20.4097  25.0279  …  27.3809  24.721   24.4355  26.0748\n 25.4256  30.0091  22.1195  25.6505     26.6411  24.233   25.3694  25.9158\n 24.6519  27.243   21.2237  24.1523     26.9574  25.4418  23.3858  24.1972\n  ⋮                                  ⋱                             \n 22.2877  27.4741  21.8779  25.6035     26.5338  23.4227  25.7808  24.7436\n 25.7797  28.7837  23.0472  26.992      26.7406  25.9313  27.3336  27.8026\n 23.9616  25.2329  20.9843  23.1032  …  25.5     24.5971  23.6548  26.0905\n 23.9671  28.4682  21.4784  26.8163     26.3495  25.1586  25.6829  25.9901\n 24.8729  29.1563  20.8062  25.7028     28.3692  25.0772  23.9394  28.236\n 24.1732  29.4735  21.1834  25.9207     25.0506  25.0781  24.6209  25.716\n 24.6369  28.7742  22.0081  26.1276     28.1415  27.08    26.6875  26.4206\n 25.5436  26.6702  21.8379  24.6884  …  25.2922  24.5365  24.3202  25.257\n 23.8022  27.3092  21.9259  23.1253     26.5923  23.4442  23.6898  24.4027\n 25.0953  28.9557  23.7193  25.9754     28.8757  25.4649  25.8137  27.0875\n 22.7922  26.9324  20.2291  23.9017     26.5135  22.9873  23.975   24.1223\n 21.9951  26.0003  21.9184  21.6673     23.6457  22.287   22.3392  24.2492\n\n\nLet’s see the transfer times:\n\n@btime cu(A)\n\n  8.144 μs (8 allocations: 39.30 KiB)\n\n\n100×100 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n 0.795979   0.820715   0.804492   …  0.173238   0.602618  0.325205\n 0.248617   0.817344   0.31537       0.99921    0.353251  0.357586\n 0.562645   0.590865   0.612691      0.835189   0.317652  0.613724\n 0.0302437  0.108246   0.134883      0.756453   0.844707  0.275566\n 0.803271   0.454746   0.0568371     0.476437   0.292629  0.436834\n 0.528203   0.965217   0.3534     …  0.284256   0.51717   0.149346\n 0.0373348  0.664591   0.305146      0.37815    0.915737  0.26873\n 0.161136   0.870413   0.747152      0.950921   0.273694  0.240076\n 0.948914   0.538506   0.163413      0.087484   0.387942  0.0266194\n 0.232321   0.729338   0.874976      0.342558   0.171119  0.669174\n 0.749575   0.989877   0.585643   …  0.841039   0.372072  0.480976\n 0.13611    0.0182909  0.993907      0.772716   0.208189  0.724729\n 0.738871   0.168995   0.370926      0.902453   0.496412  0.624321\n ⋮                                ⋱                       \n 0.850467   0.865298   0.566511      0.484169   0.133182  0.745015\n 0.424948   0.767975   0.0608416     0.381281   0.962849  0.0890435\n 0.163066   0.660003   0.823241   …  0.491438   0.353012  0.819618\n 0.989283   0.663811   0.23109       0.359999   0.718979  0.0412983\n 0.658782   0.359017   0.475921      0.643057   0.38225   0.922345\n 0.60349    0.128136   0.304796      0.0785167  0.960574  0.403708\n 0.904782   0.669525   0.633114      0.594589   0.381224  0.483661\n 0.716692   0.930138   0.359974   …  0.739195   0.365376  0.108808\n 0.253205   0.974106   0.696407      0.876903   0.518735  0.241752\n 0.730507   0.961051   0.208908      0.767898   0.723701  0.709793\n 0.803944   0.900472   0.60208       0.988069   0.471301  0.870057\n 0.331299   0.979528   0.293971      0.121811   0.331233  0.993614\n\n\n\n@btime Array(cuC)\n\n  12.888 μs (3 allocations: 39.12 KiB)\n\n\n100×100 Matrix{Float32}:\n 25.0564  28.1081  24.5791  24.2118  …  29.1066  25.3427  26.785   26.8159\n 23.8365  26.9323  22.1268  24.394      25.1406  24.2087  23.0141  24.5436\n 23.8291  27.5819  20.8153  24.5719     26.0579  24.7479  24.5362  24.5483\n 22.3662  25.2902  20.4605  22.7104     23.514   22.1021  23.3403  23.0844\n 24.8252  29.7588  22.9885  28.0034     28.7318  26.3997  26.019   27.2803\n 23.8504  25.0349  20.1403  22.2076  …  24.6679  21.7806  21.7851  22.3799\n 20.0263  22.023   17.9829  20.6622     21.3617  19.9235  21.8351  21.5375\n 22.3813  25.5924  19.3125  22.7719     24.4725  23.848   23.5627  23.3131\n 25.0231  29.8407  23.5434  25.3326     26.7342  24.6092  25.2959  24.4861\n 22.874   26.0299  20.9581  23.9912     25.9096  23.7     23.8586  24.6916\n 22.7543  26.2432  20.4097  25.0279  …  27.3809  24.721   24.4355  26.0748\n 25.4256  30.0091  22.1195  25.6505     26.6411  24.233   25.3694  25.9158\n 24.6519  27.243   21.2237  24.1523     26.9574  25.4418  23.3858  24.1972\n  ⋮                                  ⋱                             \n 22.2877  27.4741  21.8779  25.6035     26.5338  23.4227  25.7808  24.7436\n 25.7797  28.7837  23.0472  26.992      26.7406  25.9313  27.3336  27.8026\n 23.9616  25.2329  20.9843  23.1032  …  25.5     24.5971  23.6548  26.0905\n 23.9671  28.4682  21.4784  26.8163     26.3495  25.1586  25.6829  25.9901\n 24.8729  29.1563  20.8062  25.7028     28.3692  25.0772  23.9394  28.236\n 24.1732  29.4735  21.1834  25.9207     25.0506  25.0781  24.6209  25.716\n 24.6369  28.7742  22.0081  26.1276     28.1415  27.08    26.6875  26.4206\n 25.5436  26.6702  21.8379  24.6884  …  25.2922  24.5365  24.3202  25.257\n 23.8022  27.3092  21.9259  23.1253     26.5923  23.4442  23.6898  24.4027\n 25.0953  28.9557  23.7193  25.9754     28.8757  25.4649  25.8137  27.0875\n 22.7922  26.9324  20.2291  23.9017     26.5135  22.9873  23.975   24.1223\n 21.9951  26.0003  21.9184  21.6673     23.6457  22.287   22.3392  24.2492\n\n\nThe cost transferring is about 20μs-50μs in each direction, meaning that one needs to be doing operations that cost at least 200μs for GPUs to break even. A good rule of thumb is that GPU computations should take at least a millisecond, or GPU memory should be re-used.\n\n\n5.4.5 Summary of GPUs\n\nGPUs cores are slow\nGPUs are SPMD\nGPUs are generally used for linear algebra\nSuitable for SPMD 1ms computations"
  },
  {
    "objectID": "styles_of_parallelism.html#xeon-phi-accelerators-and-opencl",
    "href": "styles_of_parallelism.html#xeon-phi-accelerators-and-opencl",
    "title": "5  The Different Flavors of Parallelism",
    "section": "5.5 Xeon Phi Accelerators and OpenCL",
    "text": "5.5 Xeon Phi Accelerators and OpenCL\nOther architectures exist to keep in mind. Xeon Phis are a now-defunct accelerator that used X86 (standard processors) as the base, using hundreds of them. For example, the Knights Landing series had 256 core accelerator cards. These were all clocked down, meaning they were still slower than a standard CPU, but there were less restrictions on SPMD (though SPMD-like computations were still preferred in order to heavily make use of SIMD). However, because machine learning essentially only needs linear algebra, and linear algebra is faster when restricting to SPMD-architectures, this failed. These devices can still be found on many high end clusters.\nOne alternative to CUDA is OpenCL which supports alternative architectures such as the Xeon Phi at the same time that it supports GPUs. However, one of the issues with OpenCL is that its BLAS implementation currently does not match the speed of CuBLAS, which makes NVIDIA-specific libraries still the king of machine learning and most scientific computing."
  },
  {
    "objectID": "styles_of_parallelism.html#tpu-computing",
    "href": "styles_of_parallelism.html#tpu-computing",
    "title": "5  The Different Flavors of Parallelism",
    "section": "5.6 TPU Computing",
    "text": "5.6 TPU Computing\nTPUs are tensor processing units, which is Google’s newest accelerator technology. They are essentially just “tensor operation compilers”, which in computer science speak is simply higher dimensional linear algebra. To do this, they internally utilize a BFloat16 type, which is a 16-bit floating point number with the same exponent size as a Float32 with an 8-bit significant. This means that computations are highly prone to catastrophic cancellation. This computational device only works because BFloat16 has primitive operations for FMA which allows 32-bit-like accuracy of multiply-add operations, and thus computations which are only dot products (linear algebra) end up okay. Thus this is simply a GPU-like device which has gone further to completely specialize in linear algebra."
  },
  {
    "objectID": "styles_of_parallelism.html#multiprocessing-distributed-computing",
    "href": "styles_of_parallelism.html#multiprocessing-distributed-computing",
    "title": "5  The Different Flavors of Parallelism",
    "section": "5.7 Multiprocessing (Distributed Computing)",
    "text": "5.7 Multiprocessing (Distributed Computing)\nWhile multithreading computes with multiple threads, multiprocessing computes with multiple independent processes. Note that processes do not share any memory, not heap or data, and thus this mode of computing also allows for distributed computations, which is the case where processes may be on separate computing hardware. However, even if they are on the same hardware, the lack of a shared address space means that multiprocessing has to do message passing, i.e. send data from one process to the other.\n\n5.7.1 Distributed Tasks with Explicit Memory Handling: The Master-Worker Model\nGiven the amount of control over data handling, there are many different models for distributed computing. The simplest, the one that Julia’s Distributed Standard Library defaults to, is the master-worker model. The master-worker model has one process, deemed the master, which controls the worker processes.\nHere we can start by adding some new worker processes:\n\nusing Distributed\naddprocs(4)\n\nThis adds 4 worker processes for the master to control. The simplest computations are those where the master process gives the worker process a job which returns the value afterwards. For example, a pmap operation or @distributed loop gives the worker a function to execute, along with the data, and the worker then computes and returns the result.\nAt a lower level, this is done by Distributed.@spawning jobs, or using a remotecall and fetching the result. ParallelDataTransfer.jl gives an extended set of primitive message passing operations. For example, we can explicitly tell it to compute a function f on the remote process like:\n\n@everywhere f(x) = x.^2 # Define this function on all processes\nt = remotecall(f,2,randn(10))\n\nremotecall is a non-blocking operation that returns a Future. To access the data, one should use the blocking operation fetch to receive the data:\n\nxsq = fetch(t)\n\n\n\n5.7.2 Distributed Tasks with Implicit Memory Handling: Distributed Task-Based Parallelism\nAnother popular programming model for distributed computation is task-based parallelism but where all of the memory handling is implicit. Since, unlike the shared memory parallelism case, data transfers are required for given processes to share heap allocated values, distributed task-based parallelism libraries tend to want a global view of the whole computation in order to build a sophisticated schedule that includes where certain data lives and when transfers will occur. Because of this, distributed task-based parallelism libraries tend to want the entire computational graph of the computation, to be able to restructure the graph as necessary with their own data transfer portions spliced into the compute. Examples of this kind of framework are:\n\nTensorflow\ndask (“distributed tasks”)\nDagger.jl\n\nUsing these kinds of libraries requires building a directed acyclic graph (DAG). For example, the following showcases how to use Dagger.jl to represent a bunch of summations:\n\nusing Dagger\n\nadd1(value) = value + 1\nadd2(value) = value + 2\ncombine(a...) = sum(a)\n\np = delayed(add1)(4)\nq = delayed(add2)(p)\nr = delayed(add1)(3)\ns = delayed(combine)(p, q, r)\n\n@assert collect(s) == 16\n\nOnce the global computation is specified, commands like collect are used to instantiate the graph on given input data, which then run the computation in a (potentially) distributed manner, depending on internal scheduler heuristics.\n\n\n5.7.3 Distributed Array-Based Parallelism: SharedArrays, Elemental, and DArrays\nBecause array operations are a standard way to compute in scientific computing, there are higher level primitives to help with message passing. A SharedArray is an array which acts like a shared memory device. This means that every change to a SharedArray causes message passing to keep them in sync, and thus this should be used with a performance caution. DistributedArrays.jl is a parallel array type which has local blocks and can be used for writing higher level abstractions with explicit message passing. Because it is currently missing high-level parallel linear algebra, currently the recommended tool for distributed linear algebra is Elemental.jl.\n\n\n5.7.4 MapReduce, Hadoop, and Spark: The Map-Reduce Model\nMany data-parallel operations work by mapping a function f onto each piece of data and then reducing it. For example, the sum of squares maps the function x -&gt; x^2 onto each value, and then these values are reduced by performing a summation. MapReduce was a Google framework in the 2000’s built around this as the parallel computing concept, and current data-handling frameworks, like Hadoop and Spark, continue this as the core distributed programming model.\nIn Julia, there exists the mapreduce function for performing serial mapreduce operations. It also work on GPUs. However, it does not auto-distribute. For distributed map-reduce programming, the @distributed for-loop macro can be used. For example, sum of squares of random numbers is:\n\n@distributed (+) for i in 1:1000\n  rand()^2\nend\n\nOne can see that computing summary statistics is easily done in this framework which is why it was majorly adopted among “big data” communities.\n@distributed uses a static scheduler. The dynamic scheduling equivalent is pmap:\n\npmap(i-&gt;rand()^2,1:100)\n\nwhich will dynamically allocate jobs to processes as they declare they have finished jobs. This thus has the same performance difference behavior as Threads.@threads vs Threads.@spawn.\n\n\n5.7.5 MPI: The Distributed SPMD Model\nThe main way to do high-performance multiprocessing is MPI, which is an old distributed computing interface from the C/Fortran days. Julia has access to the MPI programming model through MPI.jl. The programming model for MPI is that every computer is running the same program, and synchronization is performed by blocking communication. For example, let’s look at the following:\n\nusing MPI\nMPI.Init()\n\ncomm = MPI.COMM_WORLD\nrank = MPI.Comm_rank(comm)\nsize = MPI.Comm_size(comm)\n\ndst = mod(rank+1, size)\nsrc = mod(rank-1, size)\n\nN = 4\n\nsend_mesg = Array{Float64}(undef, N)\nrecv_mesg = Array{Float64}(undef, N)\n\nfill!(send_mesg, Float64(rank))\n\nrreq = MPI.Irecv!(recv_mesg, src,  src+32, comm)\n\nprint(\"$rank: Sending   $rank -&gt; $dst = $send_mesg\\n\")\nsreq = MPI.Isend(send_mesg, dst, rank+32, comm)\n\nstats = MPI.Waitall!([rreq, sreq])\n\nprint(\"$rank: Received $src -&gt; $rank = $recv_mesg\\n\")\n\nMPI.Barrier(comm)\n\n#| eval: false\n&gt; mpiexecjl -n 3 julia examples/04-sendrecv.jl\n1: Sending   1 -&gt; 2 = [1.0, 1.0, 1.0, 1.0]\n0: Sending   0 -&gt; 1 = [0.0, 0.0, 0.0, 0.0]\n1: Received 0 -&gt; 1 = [0.0, 0.0, 0.0, 0.0]\n2: Sending   2 -&gt; 0 = [2.0, 2.0, 2.0, 2.0]\n0: Received 2 -&gt; 0 = [2.0, 2.0, 2.0, 2.0]\n2: Received 1 -&gt; 2 = [1.0, 1.0, 1.0, 1.0]\nLet’s investigate this a little bit. Think about having two computers run this line-by-line side by side. They will both locally build arrays, and then call MPI.Irecv!, which is an asynchronous non-blocking call to listen for a message from a given rank (a rank is the ID for a given process). Then they call their sreq = MPI.Isend function, which is an asynchronous non-blocking call to send a message send_mesg to the chosen rank. When the expected message is found, MPI.Irecv! will then run on its green thread and finish, updating the recv_mesg with the information from the message. However, in order to make sure all of the messages are received, we have added in a blocking operation MPI.Waitall!([rreq, sreq]), which will block all further execution on the given rank until both its rreq and sreq tasks are completed. After that is done, each given rank will have its updated data, and the script will continue on all ranks.\nThis model is thus very asynchronous and allows for many different computers to run one highly parallelized program, managing the data transmissions in a sparse way without a single computer in charge of managing the whole computation. However, it can be prone to deadlock, since errors in the program may for example require rank 1 to receive a message from rank 2 before continuing the program, but rank 2 won’t continue to program until it receives a message from rank 1. For this reason, while MPI has been the most successful large-scale distributed computing model and almost all major high-performance computing (HPC) cluster competitions have been won by codes utilizing the MPI model, the MPI model is nowadays considered a last resort due to these safety issues.\n\n\n5.7.6 Summary of Multiprocessing\n\nCost is hardware dependent: only suitable for 1ms or higher depending on the connections through which the messages are being passed and the topology of the network.\nThe Master-worker programming model is Julia’s Distributed model\nThe Map-reduce programming model is a common data-handling model\nArray-based distributed computations are another abstraction, used in all forms of parallelism.\nMPI is a SPMD model of distributed computing, where each process is completely independent and one just controls the memory handling."
  },
  {
    "objectID": "styles_of_parallelism.html#the-bait-and-switch-parallelism-is-about-programming-models",
    "href": "styles_of_parallelism.html#the-bait-and-switch-parallelism-is-about-programming-models",
    "title": "5  The Different Flavors of Parallelism",
    "section": "5.8 The Bait-and-switch: Parallelism is about Programming Models",
    "text": "5.8 The Bait-and-switch: Parallelism is about Programming Models\nWhile this looked like a lecture about parallel programming at the different levels and types of hardware, this wide overview showcases that the real underlying commonality within parallel program is in the parallel programming models, of which there are not too many. There are:\n\nMap-reduce parallelism models. pmap, MapReduce (Hadoop/Spark)\n\nPros: Easy to use\nCons: Requires that your program is specifically only mapping functions f and reducing them. That said, many data science operations like mean, variance, maximum, etc. can be represented as map-reduce calls, which lead to the popularity of these approaches for “big data” operations.\n\nArray-based parallelism models. SIMD (at the compiler level), CuArray, DistributedArray, PyTorch.torch, …\n\nPros: Easy to use, can have very fast library implementations for specific functions\nCons: Less control and restricted to specific functions implemented by the library. Parallelism matches the data structure, so it requires the user to be careful and know the best way to split the data.\n\nLoop-based parallelism models. Threads.@threads, @distributed, OpenMP, MATLAB’s parfor, Chapel’s iterator parallelism\n\nPros: Easy to use, almost no code change can make existing loops parallelized\nCons: Refined operations, like locking and sharing data, can be awkward to write. Less control over fine details like scheduling, meaning less opportunities to optimize.\n\nTask-based parallelism models with implicit distributed data handling. Threads.@spawn, Dagger.jl, TensorFlow, dask\n\nPros: Relatively high level, low risk of errors since parallelism is mostly handled for the user. User simply describes which functions to call in what order.\nCons: When used on distributed systems, implicit data handling is hard, meaning it’s generally not as efficient if you don’t optimize the code yourself or help the optimizer, and these require specific programming constructs for building the computational graph. Note this is only a downside for distributed data parallelism, whereas when applied to shared memory systems these aspects no longer require handling by the task scheduler.\n\nTask-based parallelism models with explicit data handling. Distributed.@spawn\n\nPros: Allows for control over what compute hardware will have specific pieces of data and allows for transferring data manually.\nCons: Requires transferring data manually. All computations are managed by a single process/computer/node and thus it can have some issues scaling to extreme (1000+ node) computing situations.\n\nSPMD kernel parallelism models. CUDA, MPI, KernelAbstractions.jl\n\nPros: Reduces the problem for the user to only specify what happens in small chunks of the problem. Works on accelerator hardware like GPUs, TPUs, and beyond.\nCons: Only works for computations that be represented block-wise, and relies on the compiler to generate good code.\n\n\nIn this sense, the different parallel programming “languages” and features are much more similar than they are all different, falling into similar categories."
  },
  {
    "objectID": "discretizing_odes.html#youtube-video-link-part-1",
    "href": "discretizing_odes.html#youtube-video-link-part-1",
    "title": "6  Ordinary Differential Equations, Applications and Discretizations",
    "section": "6.1 Youtube Video Link Part 1",
    "text": "6.1 Youtube Video Link Part 1"
  },
  {
    "objectID": "discretizing_odes.html#youtube-video-link-part-2",
    "href": "discretizing_odes.html#youtube-video-link-part-2",
    "title": "6  Ordinary Differential Equations, Applications and Discretizations",
    "section": "6.2 Youtube Video Link Part 2",
    "text": "6.2 Youtube Video Link Part 2\nNow that we have a sense of parallelism, let’s return back to our thread on scientific machine learning to start constructing parallel algorithms for integration of scientific models. We previously introduced discrete dynamical systems and their asymptotic behavior. However, many physical systems are not discrete and are in fact continuous. In this discussion we will understand how to numerically compute ordinary differential equations by transforming them into discrete dynamical systems, and use this to come up with simulation techniques for physical systems."
  },
  {
    "objectID": "discretizing_odes.html#what-is-an-ordinary-differential-equation",
    "href": "discretizing_odes.html#what-is-an-ordinary-differential-equation",
    "title": "6  Ordinary Differential Equations, Applications and Discretizations",
    "section": "6.3 What is an Ordinary Differential Equation?",
    "text": "6.3 What is an Ordinary Differential Equation?\nAn ordinary differential equation is an equation defined by a relationship on the derivative. In its general form we have that\n\\[u' = f(u,p,t)\\]\ndescribes the evolution of some variable \\(u(t)\\) which we would like to solve for. In its simplest sense, the solution to the ordinary differential equation is just the integral, since by taking the integral of both sides and applying the Fundamental Theorem of Calculus we have that\n\\[u = \\int_{t_0}^{t_f} f(u,p,t)dt\\]\nThe difficulty of this equation is that the variable \\(u(t)\\) is unknown and dependent on \\(t\\), meaning that the integral cannot readily be solved by simple calculus. In fact, in almost all cases there exists no analytical solution for \\(u\\) which is readily available. However, we can understand the behavior by looking at some simple cases."
  },
  {
    "objectID": "discretizing_odes.html#solving-ordinary-differential-equations-in-julia",
    "href": "discretizing_odes.html#solving-ordinary-differential-equations-in-julia",
    "title": "6  Ordinary Differential Equations, Applications and Discretizations",
    "section": "6.4 Solving Ordinary Differential Equations in Julia",
    "text": "6.4 Solving Ordinary Differential Equations in Julia\nTo solve an ordinary differential equation in Julia, one can use the DifferentialEquations.jl package to define the differential equation you’d like to solve. Let’s say we want to solve the Lorenz equations:\n\\[\n\\begin{align}\n\\frac{dx}{dt} &= σ(y-x) \\\\\n\\frac{dy}{dt} &= x(ρ-z) - y \\\\\n\\frac{dz}{dt} &= xy - βz \\\\\n\\end{align}\n\\]\nwhich was the system used in our investigation of discrete dynamics. The first thing we need to do is give it this differential equation. We can either write it in an in-place form f(du,u,p,t) or an out-of-place form f(u,p,t). Let’s write it in the in-place form:\n\nfunction lorenz(du,u,p,t)\n du[1] = p[1]*(u[2]-u[1])\n du[2] = u[1]*(p[2]-u[3]) - u[2]\n du[3] = u[1]*u[2] - p[3]*u[3]\nend\n\nlorenz (generic function with 1 method)\n\n\nQuestion: How could I maybe speed this up a little?\nNext we give an initial condition. Here, this is a vector of equations, so our initial condition has to be a vector. Let’s choose the following initial condition:\n\nu0 = [1.0,0.0,0.0]\n\n3-element Vector{Float64}:\n 1.0\n 0.0\n 0.0\n\n\nNotice that I made sure to use Float64 values in the initial condition. The Julia library’s functions are generic and internally use the corresponding types that you give it. Integer types do not bode well for continuous problems.\nNext, we have to tell it the timespan to solve on. Here, let’s some from time 0 to 100. This means that we would use:\n\ntspan = (0.0,100.0)\n\n(0.0, 100.0)\n\n\nNow we need to define our parameters. We will use the same ones as from our discrete dynamical system investigation.\n\np = (10.0,28.0,8/3)\n\n(10.0, 28.0, 2.6666666666666665)\n\n\nThese describe an ODEProblem. Let’s bring in DifferentialEquations.jl and define the ODE:\n\nusing DifferentialEquations\nprob = ODEProblem(lorenz,u0,tspan,p)\n\n\nODEProblem with uType Vector{Float64} and tType Float64. In-place: true\ntimespan: (0.0, 100.0)\nu0: 3-element Vector{Float64}:\n 1.0\n 0.0\n 0.0\n\n\n\nNow we can solve it by calling solve:\n\nsol = solve(prob)\n\nretcode: Success\nInterpolation: specialized 4th order \"free\" interpolation, specialized 2nd order \"free\" stiffness-aware interpolation\nt: 1263-element Vector{Float64}:\n   0.0\n   3.5678604836301404e-5\n   0.0003924646531993154\n   0.0032624077544510573\n   0.009058075635317072\n   0.01695646895607931\n   0.02768995855685593\n   0.04185635042021763\n   0.06024041165841079\n   0.08368541255159562\n   0.11336499649094857\n   0.1486218182609657\n   0.18703978481550704\n   ⋮\n  99.05535949898116\n  99.14118781914485\n  99.22588252940076\n  99.30760258626904\n  99.39665422328268\n  99.49536147459878\n  99.58822928767293\n  99.68983993598462\n  99.77864535713971\n  99.85744078539504\n  99.93773320913628\n 100.0\nu: 1263-element Vector{Vector{Float64}}:\n [1.0, 0.0, 0.0]\n [0.9996434557625105, 0.0009988049817849058, 1.781434788799208e-8]\n [0.9961045497425811, 0.010965399721242457, 2.146955365838907e-6]\n [0.9693591634199452, 0.08977060667778931, 0.0001438018342266937]\n [0.9242043615038835, 0.24228912482984957, 0.0010461623302512404]\n [0.8800455868998046, 0.43873645009348244, 0.0034242593451028745]\n [0.8483309847495312, 0.6915629321083602, 0.008487624590227805]\n [0.8495036669651213, 1.0145426355349096, 0.01821208962127994]\n [0.9139069574560097, 1.4425599806525806, 0.03669382197085303]\n [1.088863826836895, 2.052326595543049, 0.0740257368585531]\n [1.4608627354936607, 3.0206721193016133, 0.16003937020467585]\n [2.162723488309695, 4.633363843843712, 0.37711740539408584]\n [3.3684644104189387, 7.26769410983553, 0.936355641713984]\n ⋮\n [12.265454131109882, 12.598146409807255, 31.546057337607913]\n [10.48677626670755, 6.494631680470132, 33.669742813875764]\n [6.893277189568002, 3.1027383340030155, 29.77818388970318]\n [4.669609096878053, 3.061564434452441, 25.1424735017959]\n [4.188801916573263, 4.617474401440693, 21.09864175382292]\n [5.559603854699961, 7.905631612648314, 18.79323210016923]\n [8.556629716266505, 12.533041060088328, 20.6623639692711]\n [12.280585075547771, 14.505154761545633, 29.332088452699942]\n [11.736883151600804, 8.279294641640229, 34.68007510231878]\n [8.10973327066804, 3.2495066495235854, 31.97052076740117]\n [4.958629886040755, 2.194919965065022, 26.948439650907677]\n [3.8020065515435855, 2.787021797920187, 23.420567509786622]\n\n\nTo see what the solution looks like, we can call plot:\n\nusing Plots\nplot(sol)\n\n\n\n\nWe can also plot phase space diagrams by telling it which vars to compare on which axis. Let’s plot this in the (x,y,z) plane:\n\nplot(sol,vars=(1,2,3))\n\n\n\n\nNote that the sentinal to time is 0, so we can also do (t,y,z) with:\n\nplot(sol,vars=(0,2,3))\n\n\n\n\nThe equation is continuous and therefore the solution is continuous. We can see this by checking how it is at any random time value:\n\nsol(0.5)\n\n3-element Vector{Float64}:\n  6.503654868503323\n -8.508354689912013\n 38.09199724760152\n\n\nwhich gives the current evolution at that time point."
  },
  {
    "objectID": "discretizing_odes.html#differential-equations-from-scientific-contexts",
    "href": "discretizing_odes.html#differential-equations-from-scientific-contexts",
    "title": "6  Ordinary Differential Equations, Applications and Discretizations",
    "section": "6.5 Differential Equations from Scientific Contexts",
    "text": "6.5 Differential Equations from Scientific Contexts\n\n6.5.1 N-Body Problems and Astronomy\nThere are many different contexts in which differential equations show up. In fact, it’s not a stretch to say that the laws in all fields of science are encoded in differential equations. The starting point for physics is Newton’s laws of gravity, which define an N-body ordinary differential equation system by describing the force between two particles as:\n\\[F = G \\frac{m_1m_2}{r^2}\\]\nwhere \\(r^2\\) is the Euclidian distance between the two particles. From here, we use the fact that\n\\[F = ma\\]\nto receive differential equations in terms of the accelerations of each particle. The differential equation is a system, where we know the change in position is due to the current velocity:\n\\[x' = v\\]\nand the change in velocity is the acceleration:\n\\[v' = F/m = G \\frac{m_i}{r_i^2}\\]\nwhere \\(i\\) runs over the other particles. Thus we have a vector of position derivatives and a vector of velocity derivatives that evolve over time to give the evolving positions and velocity.\nAn example of this is the Pleiades problem, which is an approximation to a 7-star chaotic system. It can be written as:\n\nusing OrdinaryDiffEq\n\nfunction pleiades(du,u,p,t)\n  @inbounds begin\n  x = view(u,1:7)   # x\n  y = view(u,8:14)  # y\n  v = view(u,15:21) # x′\n  w = view(u,22:28) # y′\n  du[1:7] .= v\n  du[8:14].= w\n  for i in 15:28\n    du[i] = zero(u[1])\n  end\n  for i=1:7,j=1:7\n    if i != j\n      r = ((x[i]-x[j])^2 + (y[i] - y[j])^2)^(3/2)\n      du[14+i] += j*(x[j] - x[i])/r\n      du[21+i] += j*(y[j] - y[i])/r\n    end\n  end\n  end\nend\ntspan = (0.0,3.0)\nprob = ODEProblem(pleiades,[3.0,3.0,-1.0,-3.0,2.0,-2.0,2.0,3.0,-3.0,2.0,0,0,-4.0,4.0,0,0,0,0,0,1.75,-1.5,0,0,0,-1.25,1,0,0],tspan)\n\n\nODEProblem with uType Vector{Float64} and tType Float64. In-place: true\ntimespan: (0.0, 3.0)\nu0: 28-element Vector{Float64}:\n  3.0\n  3.0\n -1.0\n -3.0\n  2.0\n -2.0\n  2.0\n  3.0\n -3.0\n  2.0\n  0.0\n  0.0\n -4.0\n  ⋮\n  0.0\n  0.0\n  0.0\n  1.75\n -1.5\n  0.0\n  0.0\n  0.0\n -1.25\n  1.0\n  0.0\n  0.0\n\n\n\nwhere we assume \\(m_i = i\\). When we solve this equation we receive the following:\n\nsol = solve(prob,Vern8(),abstol=1e-10,reltol=1e-10)\nplot(sol)\n\n\n\n\n\ntspan = (0.0,200.0)\nprob = ODEProblem(pleiades,[3.0,3.0,-1.0,-3.0,2.0,-2.0,2.0,3.0,-3.0,2.0,0,0,-4.0,4.0,0,0,0,0,0,1.75,-1.5,0,0,0,-1.25,1,0,0],tspan)\nsol = solve(prob,Vern8(),abstol=1e-10,reltol=1e-10)\nplot(sol,vars=((1:7),(8:14)))\n\n\n\n\n\n\n6.5.2 Population Ecology: Lotka-Volterra\nPopulation ecology’s starting point is the Lotka-Volterra equations which describes the interactions between a predator and a prey. In this case, the prey grows at an exponential rate but has a term that reduces its population by being eaten by the predator. The predator’s growth is dependent on the available food (the amount of prey) and has a decay rate due to old age. This model is then written as follows:\n\nfunction lotka(du,u,p,t)\n  du[1] = p[1]*u[1] - p[2]*u[1]*u[2]\n  du[2] = -p[3]*u[2] + p[4]*u[1]*u[2]\nend\n\np = [1.5,1.0,3.0,1.0]\nprob = ODEProblem(lotka,[1.0,1.0],(0.0,10.0),p)\nsol = solve(prob)\nplot(sol)\n\n\n\n\n\n\n6.5.3 Biochemistry: Robertson Equations\nBiochemical equations commonly display large separation of timescales which lead to a stiffness phenomena that will be investigated later. The classic “hard” equations for ODE integration thus tend to come from biology (not physics!) due to this property. One of the standard models is the Robertson model, which can be described as:\n\nusing Sundials, ParameterizedFunctions\nfunction rober(du,u,p,t)\n  y₁,y₂,y₃ = u\n  k₁,k₂,k₃ = p\n  du[1] = -k₁*y₁+k₃*y₂*y₃\n  du[2] =  k₁*y₁-k₂*y₂^2-k₃*y₂*y₃\n  du[3] =  k₂*y₂^2\nend\nprob = ODEProblem(rober,[1.0,0.0,0.0],(0.0,1e5),(0.04,3e7,1e4))\nsol = solve(prob,Rosenbrock23())\nplot(sol)\n\n\n\n\n\nplot(sol, xscale=:log10, tspan=(1e-6, 1e5), layout=(3,1))\n\n\n\n\n\n\n6.5.4 Chemical Physics: Pollution Models\nChemical reactions in physical models are also described as differential equation systems. The following is a classic model of dynamics between different species of pollutants:\n\nk1=.35e0\nk2=.266e2\nk3=.123e5\nk4=.86e-3\nk5=.82e-3\nk6=.15e5\nk7=.13e-3\nk8=.24e5\nk9=.165e5\nk10=.9e4\nk11=.22e-1\nk12=.12e5\nk13=.188e1\nk14=.163e5\nk15=.48e7\nk16=.35e-3\nk17=.175e-1\nk18=.1e9\nk19=.444e12\nk20=.124e4\nk21=.21e1\nk22=.578e1\nk23=.474e-1\nk24=.178e4\nk25=.312e1\np = (k1,k2,k3,k4,k5,k6,k7,k8,k9,k10,k11,k12,k13,k14,k15,k16,k17,k18,k19,k20,k21,k22,k23,k24,k25)\nfunction f(dy,y,p,t)\n k1,k2,k3,k4,k5,k6,k7,k8,k9,k10,k11,k12,k13,k14,k15,k16,k17,k18,k19,k20,k21,k22,k23,k24,k25 = p\n r1  = k1 *y[1]\n r2  = k2 *y[2]*y[4]\n r3  = k3 *y[5]*y[2]\n r4  = k4 *y[7]\n r5  = k5 *y[7]\n r6  = k6 *y[7]*y[6]\n r7  = k7 *y[9]\n r8  = k8 *y[9]*y[6]\n r9  = k9 *y[11]*y[2]\n r10 = k10*y[11]*y[1]\n r11 = k11*y[13]\n r12 = k12*y[10]*y[2]\n r13 = k13*y[14]\n r14 = k14*y[1]*y[6]\n r15 = k15*y[3]\n r16 = k16*y[4]\n r17 = k17*y[4]\n r18 = k18*y[16]\n r19 = k19*y[16]\n r20 = k20*y[17]*y[6]\n r21 = k21*y[19]\n r22 = k22*y[19]\n r23 = k23*y[1]*y[4]\n r24 = k24*y[19]*y[1]\n r25 = k25*y[20]\n\n dy[1]  = -r1-r10-r14-r23-r24+\n          r2+r3+r9+r11+r12+r22+r25\n dy[2]  = -r2-r3-r9-r12+r1+r21\n dy[3]  = -r15+r1+r17+r19+r22\n dy[4]  = -r2-r16-r17-r23+r15\n dy[5]  = -r3+r4+r4+r6+r7+r13+r20\n dy[6]  = -r6-r8-r14-r20+r3+r18+r18\n dy[7]  = -r4-r5-r6+r13\n dy[8]  = r4+r5+r6+r7\n dy[9]  = -r7-r8\n dy[10] = -r12+r7+r9\n dy[11] = -r9-r10+r8+r11\n dy[12] = r9\n dy[13] = -r11+r10\n dy[14] = -r13+r12\n dy[15] = r14\n dy[16] = -r18-r19+r16\n dy[17] = -r20\n dy[18] = r20\n dy[19] = -r21-r22-r24+r23+r25\n dy[20] = -r25+r24\nend\n\nf (generic function with 1 method)\n\n\n\nu0 = zeros(20)\nu0[2]  = 0.2\nu0[4]  = 0.04\nu0[7]  = 0.1\nu0[8]  = 0.3\nu0[9]  = 0.01\nu0[17] = 0.007\nprob = ODEProblem(f,u0,(0.0,60.0),p)\nsol = solve(prob,Rodas5())\n\nretcode: Success\nInterpolation: specialized 4rd order \"free\" stiffness-aware interpolation\nt: 29-element Vector{Float64}:\n  0.0\n  0.0013845590497824308\n  0.003242540880561935\n  0.007901605227525086\n  0.016011572765091416\n  0.02740615678429701\n  0.044612092501151696\n  0.07720629370280543\n  0.11607398786651008\n  0.1763063703057767\n  0.2579025569276484\n  0.3684040016675436\n  0.4976731407000709\n  ⋮\n  1.6692678869290878\n  2.1120042389942575\n  2.7287854185402995\n  3.676321507769747\n  5.344945477147836\n  7.985769209063678\n 11.981251310096694\n 17.452504634746386\n 24.882247321193052\n 34.66462280306778\n 47.27763232418448\n 60.0\nu: 29-element Vector{Vector{Float64}}:\n [0.0, 0.2, 0.0, 0.04, 0.0, 0.0, 0.1, 0.3, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007, 0.0, 0.0, 0.0]\n [0.0002935083676916062, 0.19970649101355778, 1.693835912963263e-10, 0.03970673366392418, 1.1424841091201038e-7, 1.0937647553427759e-7, 0.09999966676440009, 0.3000003350396963, 0.00999998209858389, 5.6037724774041996e-9, 6.047938337401518e-9, 1.004757080331196e-8, 5.9812281205517996e-12, 6.239553394674922e-9, 2.3022538431539757e-10, 3.129330582279999e-17, 0.006999999417662247, 5.823377531818463e-10, 3.8240538930006997e-10, 6.930819262664895e-14]\n [0.0006836584262738185, 0.19931633629695783, 1.9606016564783855e-10, 0.03931745222652303, 2.0821314359971526e-7, 2.5242637698192594e-7, 0.09999884087661927, 0.30000116344748634, 0.009999897466494299, 2.060514512836631e-8, 1.6844788957556812e-8, 8.136618249763386e-8, 1.0724538885842355e-10, 6.486750944005252e-8, 3.1010416395449586e-9, 3.098650817258985e-17, 0.006999996444140231, 3.555859768109285e-9, 2.064386087388623e-9, 2.0476275988812504e-12]\n [0.0016428179821615391, 0.19835713123489693, 2.6245924414625437e-10, 0.0383623109751218, 3.515273109946351e-7, 4.664572523382881e-7, 0.09999544921365162, 0.3000045632051474, 0.00999947365101553, 4.4964384052575006e-8, 3.335028459989388e-8, 4.812858451973164e-7, 1.440960635479933e-9, 4.444464501496583e-7, 3.745751745418737e-8, 3.0233751050330334e-17, 0.006999981334742691, 1.8665257309560864e-8, 1.174674271331608e-8, 6.886039278040623e-11]\n [0.0032462849525836317, 0.19675344580878912, 3.7349154721240826e-10, 0.03676859821471883, 4.320335375819997e-7, 5.784969040588635e-7, 0.09998753646949957, 0.30001250073250896, 0.00999841279050207, 5.847792696989046e-8, 4.2283730165807424e-8, 1.5155870313913987e-6, 8.52498025471751e-9, 1.461534608031614e-6, 2.142214641931594e-7, 2.897772883427687e-17, 0.006999943344407836, 5.6655592163595973e-8, 4.4368496659438254e-8, 1.0618431054301687e-9]\n [0.005361393004057221, 0.19463777403944538, 5.199898853042826e-10, 0.03466792308144617, 4.301350110500244e-7, 5.629734298792613e-7, 0.09997580962893295, 0.30002429018799176, 0.00999682035482297, 5.777644541817105e-8, 4.151145797901329e-8, 3.075243316490961e-6, 2.7267223985258345e-8, 2.9888963034290736e-6, 6.762049531575719e-7, 2.732216410413453e-17, 0.006999886274019657, 1.1372598034289956e-7, 1.1359725262068193e-7, 7.94353386313709e-9]\n [0.008274964935351176, 0.19172294848166443, 7.218269559835735e-10, 0.0317741799287896, 3.9826084453929684e-7, 5.005231467491415e-7, 0.09995935394408918, 0.30004089901876024, 0.00999460673549988, 5.178145386630504e-8, 3.714992485079132e-8, 5.229419020007247e-6, 6.871429675796394e-8, 5.040637233611971e-6, 1.6909614724980253e-6, 2.5041573913987372e-17, 0.006999806991160331, 1.9300883966852184e-7, 2.3808903519156556e-7, 4.4409090022701065e-8]\n [0.013002670675854911, 0.18699194546955783, 1.049323517342023e-9, 0.02707833467623004, 3.596573982134101e-7, 4.230620972419726e-7, 0.09993191719332849, 0.3000687867927592, 0.009990985412449237, 4.426632357885486e-8, 3.1717470184457415e-8, 8.707148275200555e-6, 1.753988228736806e-7, 8.159541828820095e-6, 4.28235499736743e-6, 2.134072762166874e-17, 0.006999677462783783, 3.225372162160811e-7, 4.29253130312003e-7, 2.484238183958042e-7]\n [0.01754950653069442, 0.18244010960656096, 1.3641806124928415e-9, 0.022563570556883728, 3.361162798275428e-7, 3.7150093428726335e-7, 0.09990309462743716, 0.30009836544883595, 0.009987255299681627, 3.931687421453365e-8, 2.8130812802630142e-8, 1.2231150049080595e-5, 3.3462364587004975e-7, 1.103334852290569e-5, 8.113769678679281e-6, 1.7782593323554993e-17, 0.0069995442449338795, 4.55755066120686e-7, 5.171121404557453e-7, 7.09178639855165e-7]\n [0.022860660993157974, 0.17711993361265657, 1.7318456679719292e-9, 0.017295319170346254, 3.1879075517672294e-7, 3.280175981175663e-7, 0.0998630917757628, 0.30013989594565554, 0.009982163705371506, 3.519652711590188e-8, 2.5136768154051193e-8, 1.6956872533522293e-5, 6.253075543101413e-7, 1.439191013308264e-5, 1.50368204130258e-5, 1.3630627583154951e-17, 0.006999362663023951, 6.373369760492056e-7, 5.039634032907156e-7, 1.6196514074344916e-6]\n [0.027755823531265923, 0.1722114557342829, 2.070707321657687e-9, 0.012451819509586418, 3.09577570200403e-7, 2.984875701896153e-7, 0.0998138159158085, 0.3001917511617734, 0.00997597062665378, 3.244422538320162e-8, 2.313126337034893e-8, 2.2598805585100702e-5, 1.0726062363264699e-6, 1.7668944300683476e-5, 2.5579706458181664e-5, 9.813413259159095e-18, 0.006999142070090901, 8.579299090988792e-7, 4.4917481055362547e-7, 2.8096234730725815e-6]\n [0.03180047574896728, 0.16814808737553658, 2.3507725368492532e-9, 0.008472078039750567, 3.0608021229872864e-7, 2.798406006589666e-7, 0.09975168783182878, 0.300258033967349, 0.009968213586476125, 3.07440294917979e-8, 2.1888709888538225e-8, 2.9549391209733653e-5, 1.737052957445355e-6, 2.0753009296102627e-5, 4.1107860274511314e-5, 6.676936081814453e-18, 0.006998865973340087, 1.1340266599123846e-6, 4.0097466626237656e-7, 4.09549379898284e-6]\n [0.03442055718853232, 0.16550584277895308, 2.5323171579936213e-9, 0.005925149987065278, 3.0614394582593074e-7, 2.7051628714958993e-7, 0.09968231993256006, 0.30033293123066496, 0.009959560939629304, 2.9921030796856565e-8, 2.128523844055769e-8, 3.7208263766210764e-5, 2.5639881218245094e-6, 2.3218225998936952e-5, 6.0332749267068424e-5, 4.669674612663626e-18, 0.0069985580361970326, 1.4419638029664935e-6, 3.7454926288577253e-7, 5.1643729314177814e-6]\n ⋮\n [0.03791793802829607, 0.16180449828280613, 2.7768370495177913e-9, 0.0030823660560272107, 3.14521832227949e-7, 2.663442341942579e-7, 0.099083015665936, 0.3009948161266857, 0.00988381094641692, 2.961991770410293e-8, 2.1060454565632512e-8, 0.00010338319436372564, 1.0628140212378494e-5, 2.9835098929070707e-5, 0.00024893814146343687, 2.4292459347208345e-18, 0.006995852938418454, 4.1470615815449725e-6, 4.3591304814339387e-7, 8.780747086917043e-6]\n [0.03815122407263181, 0.16149335659572692, 2.7939409720765736e-9, 0.003096645006682394, 3.1441730293333357e-7, 2.6577110284299975e-7, 0.09885950529038474, 0.3012439180124001, 0.009855333713220338, 2.9534366335576414e-8, 2.0998617396183096e-8, 0.00012822282433546487, 1.3697755561961454e-5, 3.0219403713939676e-5, 0.00032198854211351434, 2.440499329742852e-18, 0.006994830897031649, 5.169102968349309e-6, 4.6944787133821387e-7, 9.631793047227356e-6]\n [0.03846176807523067, 0.16107438880150843, 2.8167539200764392e-9, 0.0031287167377469284, 3.130048446599601e-7, 2.6394989793090676e-7, 0.09855039498642384, 0.30158891371945085, 0.009815972945207875, 2.930446690402043e-8, 2.083203837538137e-8, 0.00016256117508267194, 1.7931704280207427e-5, 3.0249851522808104e-5, 0.0004240175943222341, 2.465775406915971e-18, 0.006993413750805722, 6.586249194276202e-6, 5.108442061897667e-7, 1.0691490226114317e-5]\n [0.038929997230639016, 0.1604402370546664, 2.851149288198812e-9, 0.0031795600755990406, 3.0972897906069453e-7, 2.602522829601712e-7, 0.09808114604225779, 0.3021129375348955, 0.009756427876936317, 2.8857054777614536e-8, 2.0507650745238607e-8, 0.00021451971258084569, 2.4313110493778785e-5, 2.984486304939207e-5, 0.0005806906609807024, 2.505845589866296e-18, 0.0069912600590668235, 8.73994093317433e-6, 5.644795502477369e-7, 1.20987318349133e-5]\n [0.039731796287462866, 0.15934875829742107, 2.910031107146615e-9, 0.0032678397244506998, 3.0340454593255876e-7, 2.5336827192290844e-7, 0.09727159520683847, 0.30301666821862416, 0.009654512408596433, 2.8039000372424075e-8, 1.991446359639732e-8, 0.0003034866969101804, 3.515693408225824e-5, 2.884332434222363e-5, 0.0008554037161571963, 2.5754197332981396e-18, 0.006987546308720154, 1.2453691279844245e-5, 6.384025603676968e-7, 1.4123181158100555e-5]\n [0.04094318678623546, 0.15768509187834287, 2.9989714875514865e-9, 0.0034038157823948153, 2.939882128755269e-7, 2.43206001729755e-7, 0.09603188713491921, 0.304398991005756, 0.009500566541513962, 2.6846369481449033e-8, 1.904976991469038e-8, 0.0004379930888116984, 5.130948406305055e-5, 2.731177344979706e-5, 0.0012863759435122486, 2.6825839311824933e-18, 0.006981869477596392, 1.8130522403605936e-5, 7.24905597139957e-7, 1.6655501124600897e-5]\n [0.04265456099862558, 0.15530170721543893, 3.1246155060271677e-9, 0.0036015752224973375, 2.814477983198598e-7, 2.2972054943079987e-7, 0.09424347009700185, 0.30638970302322815, 0.00928289784467482, 2.528730342602189e-8, 1.7919684960259242e-8, 0.0006285309299580898, 7.356402881817139e-5, 2.5311076152674727e-5, 0.001929659048706679, 2.8384402789327404e-18, 0.0069737007495245485, 2.6299250475450097e-5, 8.253072229358654e-7, 1.9841700593870324e-5]\n [0.044793931660118226, 0.15226289579317065, 3.281711134773703e-9, 0.0038589221832575496, 2.668944396446824e-7, 2.1414675862675267e-7, 0.09194251736341381, 0.3089455007693317, 0.009010154414799033, 2.352093124222566e-8, 1.663978869859591e-8, 0.00086811208533013, 0.00010022358782918614, 2.305697615943232e-5, 0.0027942303053253683, 3.0412581944159112e-18, 0.006963219964939301, 3.678003506069795e-5, 9.439336216189184e-7, 2.3887359967470606e-5]\n [0.04738795712746251, 0.14848196819379955, 3.472264655035832e-9, 0.004187711120538974, 2.5064148591231464e-7, 1.968814170125402e-7, 0.08904810542258144, 0.3121528528950734, 0.008678047248147587, 2.160558060909926e-8, 1.5252579424714244e-8, 0.0011615458041670677, 0.00013035939391932964, 2.063048330498102e-5, 0.003939836311621362, 3.3003803021585388e-18, 0.006950068615422142, 4.99313845778581e-5, 1.0960311780641945e-6, 2.93914710095533e-5]\n [0.05036882066239134, 0.14399340437115474, 3.6913545076957686e-9, 0.004591322479293651, 2.3347510067678841e-7, 1.7884243621649037e-7, 0.08556749410075942, 0.3160000520160919, 0.008293774589266539, 1.9649652481043972e-8, 1.383677779299112e-8, 0.0015041058995731492, 0.00016128741143367365, 1.8176658967239333e-5, 0.0054015182090251, 3.618470767288328e-18, 0.006934279699570879, 6.572030042912128e-5, 1.2906023386937797e-6, 3.683937182822645e-5]\n [0.05364912800384795, 0.1388525663433388, 3.932627740345212e-9, 0.005072852392385736, 2.160105896887423e-7, 1.6077416721927868e-7, 0.08152165691845746, 0.3204605702073428, 0.007866251982276732, 1.772858450647807e-8, 1.2447110292692606e-8, 0.0018899286669815385, 0.00018974903053163954, 1.5799558796591456e-5, 0.007213605828677907, 3.997969685509972e-18, 0.006915930516551771, 8.406948344822905e-5, 1.5343173858417772e-6, 4.67082381089522e-5]\n [0.05646254720110724, 0.13424842010171048, 4.139733781936821e-9, 0.00552313972545095, 2.0189808613219926e-7, 1.464545080585167e-7, 0.0778424951212836, 0.324507530466733, 0.00749401417188585, 1.622296660466674e-8, 1.13586637905381e-8, 0.0022305051646553066, 0.0002087163072622969, 1.396934873781403e-5, 0.008964884997558698, 4.352845989434373e-18, 0.006899219722669947, 0.00010078027733005331, 1.7721521050343073e-6, 5.682962012818098e-5]\n\n\n\nplot(sol)\n\n\n\n\n\nplot(sol, xscale=:log10, tspan=(1e-6, 60), layout=(3,1))"
  },
  {
    "objectID": "discretizing_odes.html#geometric-properties",
    "href": "discretizing_odes.html#geometric-properties",
    "title": "6  Ordinary Differential Equations, Applications and Discretizations",
    "section": "6.6 Geometric Properties",
    "text": "6.6 Geometric Properties\n\n6.6.1 Linear Ordinary Differential Equations\nThe simplest ordinary differential equation is the scalar linear ODE, which is given in the form\n\\[u' = \\alpha u\\]\nWe can solve this by noticing that \\((e^{\\alpha t})^\\prime = \\alpha e^{\\alpha t}\\) satisfies the differential equation and thus the general solution is:\n\\[u(t) = u(0)e^{\\alpha t}\\]\nFrom the analytical solution we have that:\n\nIf \\(Re(\\alpha) &gt; 0\\) then \\(u(t) \\rightarrow \\infty\\) as \\(t \\rightarrow \\infty\\)\nIf \\(Re(\\alpha) &lt; 0\\) then \\(u(t) \\rightarrow 0\\) as \\(t \\rightarrow \\infty\\)\nIf \\(Re(\\alpha) = 0\\) then \\(u(t)\\) has a constant or periodic solution.\n\nThis theory can then be extended to multivariable systems in the same way as the discrete dynamics case. Let \\(u\\) be a vector and have\n\\[u' = Au\\]\nbe a linear ordinary differential equation. Assuming \\(A\\) is diagonalizable, we diagonalize \\(A = P^{-1}DP\\) to get\n\\[Pu' = DPu\\]\nand change coordinates \\(z = Pu\\) so that we have\n\\[z' = Dz\\]\nwhich decouples the equation into a system of linear ordinary differential equations which we solve individually. Thus we see that, similarly to the discrete dynamical system, we have that:\n\nIf all of the eigenvalues negative, then \\(u(t) \\rightarrow 0\\) as \\(t \\rightarrow \\infty\\)\nIf any eigenvalue is positive, then \\(u(t) \\rightarrow \\infty\\) as \\(t \\rightarrow \\infty\\)\n\n\n\n6.6.2 Nonlinear Ordinary Differential Equations\nAs with discrete dynamical systems, the geometric properties extend locally to the linearization of the continuous dynamical system as defined by:\n\\[u' = \\frac{df}{du} u\\]\nwhere \\(\\frac{df}{du}\\) is the Jacobian of the system. This is a consequence of the Hartman-Grubman Theorem."
  },
  {
    "objectID": "discretizing_odes.html#numerically-solving-ordinary-differential-equations",
    "href": "discretizing_odes.html#numerically-solving-ordinary-differential-equations",
    "title": "6  Ordinary Differential Equations, Applications and Discretizations",
    "section": "6.7 Numerically Solving Ordinary Differential Equations",
    "text": "6.7 Numerically Solving Ordinary Differential Equations\n\n6.7.1 Euler’s Method\nTo numerically solve an ordinary differential equation, one turns the continuous equation into a discrete equation by discretizing it. The simplest discretization is the Euler method. The Euler method can be thought of as a simple approximation replacing \\(dt\\) with a small non-infinitesimal \\(\\Delta t\\). Thus we can approximate\n\\[f(u,p,t) = u' = \\frac{du}{dt} \\approx \\frac{\\Delta u}{\\Delta t}\\]\nand now since \\(\\Delta u = u_{n+1} - u_n\\) we have that\n\\[\\Delta t f(u,p,t) = u_{n+1} - u_n\\]\nWe need to make a choice as to where we evaluate \\(f\\) at. The simplest approximation is to evaluate it at \\(t_n\\) with \\(u_n\\) where we already have the data, and thus we re-arrange to get\n\\[u_{n+1} = u_n + \\Delta t f(u,p,t)\\]\nThis is the Euler method.\nWe can interpret it more rigorously by looking at the Taylor series expansion. First write out the Taylor series for the ODE’s solution in the near future:\n\\[u(t+\\Delta t) = u(t) + \\Delta t u'(t) + \\frac{\\Delta t^2}{2} u''(t) + \\ldots\\]\nRecall that \\(u' = f(u,p,t)\\) by the definition of the ODE system, and thus we have that\n\\[u(t+\\Delta t) = u(t) + \\Delta t f(u,p,t) + \\mathcal{O}(\\Delta t^2)\\]\nThis is a first order approximation because the error in our step can be expresed as an error in the derivative, i.e.\n\\[\\frac{u(t + \\Delta t) - u(t)}{\\Delta t} = f(u,p,t) + \\mathcal{O}(\\Delta t)\\]\n\n\n6.7.2 Higher Order Methods\nWe can use this analysis to extend our methods to higher order approximation by simply matching the Taylor series to a higher order. Intuitively, when we developed the Euler method we had to make a choice:\n\\[u_{n+1} = u_n + \\Delta t f(u,p,t)\\]\nwhere do we evaluate \\(f\\)? One may think that the best derivative approximation my come from the middle of the interval, in which case we might want to evaluate it at \\(t + \\frac{\\Delta t}{2}\\). To do so, we can use the Euler method to approximate the value at \\(t + \\frac{\\Delta t}{2}\\) and then use that value to approximate the derivative at \\(t + \\frac{\\Delta t}{2}\\). This looks like:\n\\[\n\\begin{align}\nk_1 &= f(u_n,p,t)\\\\\nk_2 &= f(u_n + \\frac{\\Delta t}{2} k_1,p,t + \\frac{\\Delta t}{2})\\\\\nu_{n+1} &= u_n + \\Delta t k_2\n\\end{align}\n\\]\nwhich we can also write as:\n\\[\nu_{n+1} = u_n + \\Delta t f(u_n + \\frac{\\Delta t}{2} f_n,p,t + \\frac{\\Delta t}{2})\n\\]\nwhere \\(f_n = f(u_n,p,t)\\). If we do the two-dimensional Taylor expansion we get:\n\\[\n\\begin{align}\nu_{n+1} &= u_n\\\\\n&+ \\Delta t f_n\\\\\n&+ \\frac{\\Delta t^2}{2}(f_t + f_u f)(u_n,p,t)\\\\\n&+ \\frac{\\Delta t^3}{6} (f_{tt} + 2f_{tu}f + f_{uu}f^2)(u_n,p,t)\n\\end{align}\n\\]\nwhich when we compare against the true Taylor series:\n\\[\n\\begin{align}\nu(t+\\Delta t) &= u_n\\\\\n&+ \\Delta t f(u_n,p,t)\\\\\n&+ \\frac{\\Delta t^2}{2}(f_t + f_u f)(u_n,p,t)\\\\\n&+ \\frac{\\Delta t^3}{6}(f_{tt} + 2f_{tu} + f_{uu}f^2 + f_t f_u + f_u^2 f)(u_n,p,t)\n\\end{align}\n\\]\nand thus we see that\n\\[\nu(t + \\Delta t) - u_n = \\mathcal{O}(\\Delta t^3)\n\\]\n\n\n6.7.3 Runge-Kutta Methods\nMore generally, Runge-Kutta methods are of the form:\n\\[\n\\begin{align}\nk_1 &= f(u_n,p,t)\\\\\nk_2 &= f(u_n + \\Delta t (a_{21} k_1),p,t + \\Delta t c_2)\\\\\nk_3 &= f(u_n + \\Delta t (a_{31} k_1 + a_{32} k_2),p,t + \\Delta t c_3)\\\\\n\\vdots\\\\\nu_{n+1} &= u_n + \\Delta t (b_1 k_1 + \\ldots + b_s k_s)\n\\end{align}\n\\]\nwhere \\(s\\) is the number of stages. These can be expressed as a tableau:\n\nThe order of the Runge-Kutta method is simply the number of terms in the Taylor series that ends up being matched by the resulting expansion. For example, for the 4th order you can expand out and see that the following equations need to be satisfied:\n\nThe classic Runge-Kutta method is also known as RK4 and is the following 4th order method:\n\\[\n\\begin{align}\nk_1 &= f(u_n, p, t)\\\\\nk_2 &= f(u_n + \\frac{\\Delta t}{2} k_1, p, t + \\frac{\\Delta t}{2})\\\\\nk_3 &= f(u_n + \\frac{\\Delta t}{2} k_2, p, t + \\frac{\\Delta t}{2})\\\\\nk_4 &= f(u_n + \\Delta t k_3, p, t + \\Delta t)\\\\\n\\vdots\\\\\nu_{n+1} &= u_n + \\frac{\\Delta t}{6}(k_1 + 2 k_2 + 2 k_3 + k_4)\\\\\n\\end{align}\n\\]\nWhile it’s widely known and simple to remember, it’s not necessarily good. The way to judge a Runge-Kutta method is by looking at the size of the coefficient of the next term in the Taylor series: if it’s large then the true error can be larger, even if it matches another one asymptotically."
  },
  {
    "objectID": "discretizing_odes.html#what-makes-a-good-method",
    "href": "discretizing_odes.html#what-makes-a-good-method",
    "title": "6  Ordinary Differential Equations, Applications and Discretizations",
    "section": "6.8 What Makes a Good Method?",
    "text": "6.8 What Makes a Good Method?\n\n6.8.1 Leading Truncation Coefficients\nFor given orders of explicit Runge-Kutta methods, lower bounds for the number of f evaluations (stages) required to receive a given order are known:\n\nWhile unintuitive, using the method is not necessarily the one that reduces the coefficient the most. The reason is because what is attempted in ODE solving is precisely the opposite of the analysis. In the ODE analysis, we’re looking at behavior as \\(\\Delta t \\rightarrow 0\\). However, when efficiently solving ODEs, we want to use the largest \\(\\Delta t\\) which satisfies error tolerances.\nThe most widely used method is the Dormand-Prince 5th order Runge-Kutta method, whose tableau is represented as:\n\nNotice that this method takes 7 calls to f for 5th order. The key to this method is that it has optimized leading truncation error coefficients, under some extra assumptions which allow for the analysis to be simplified.\n\n\n6.8.2 Looking at the Effects of RK Method Choices and Code Optimizations\nPulling from the SciML Benchmarks, we can see the general effect of these different properties on a given set of Runge-Kutta methods:\n\nHere, the order of the method is given in the name. We can see one immediate factor is that, as the requested error in the calculation decreases, the higher order methods become more efficient. This is because to decrease error, you decrease \\(\\Delta t\\), and thus the exponent difference with respect to \\(\\Delta t\\) has more of a chance to pay off for the extra calls to f. Additionally, we can see that order is not the only determining factor for efficiency: the Vern8 method seems to have a clear approximate 2.5x performance advantage over the whole span of the benchmark compared to the DP8 method, even though both are 8th order methods. This is because of the leading truncation terms: with a small enough \\(\\Delta t\\), the more optimized method (Vern8) will generally have low error in a step for the same \\(\\Delta t\\) because the coefficients in the expansion are generally smaller.\nThis is a factor which is generally ignored in high level discussions of numerical differential equations, but can lead to orders of magnitude differences! This is highlighted in the following plot:\n\nHere we see ODEInterface.jl’s ODEInterfaceDiffEq.jl wrapper into the SciML common interface for the standard dopri method from Fortran, and ODE.jl, the original ODE solvers in Julia, have a performance disadvantage compared to the DifferentialEquations.jl methods due in part to some of the coding performance pieces that we discussed in the first few lectures.\nSpecifically, a large part of this can be attributed to inlining of the higher order functions, i.e. ODEs are defined by a user function and then have to be called from the solver. If the solver code is compiled as a shared library ahead of time, like is commonly done in C++ or Fortran, then there can be a function call overhead that is eliminated by JIT compilation optimizing across the function call barriers (known as interprocedural optimization). This is one way which a JIT system can outperform an AOT (ahead of time) compiled system in real-world code (for completeness, two other ways are by doing full function specialization, which is something that is not generally possible in AOT languages given that you cannot know all types ahead of time for a fully generic function, and calling C itself, i.e. c-ffi (foreign function interface), can be optimized using the runtime information of the JIT compiler to outperform C!).\nThe other performance difference being shown here is due to optimization of the method. While a slightly different order, we can see a clear difference in the performance of RK4 vs the coefficient optimized methods. It’s about the same order of magnitude as “highly optimized code differences”, showing that both the Runge-Kutta coefficients and the code implementation can have a significant impact on performance.\nTaking a look at what happens when interpreted languages get involved highlights some of the code challenges in this domain. Let’s take a look at for example the results when simulating 3 ODE systems with the various RK methods:\n\nWe see that using interpreted languages introduces around a 50x-100x performance penalty. If you recall in your previous lecture, the discrete dynamical system that was being simulated was the 3-dimensional Lorenz equation discretized by Euler’s method, meaning that the performance of that implementation is a good proxy for understanding the performance differences in this graph. Recall that in previous lectures we saw an approximately 5x performance advantage when specializing on the system function and size and around 10x by reducing allocations: these features account for the performance differences noticed between library implementations, which are then compounded by the use of different RK methods (note that R uses “call by copy” which even further increases the memory usages and makes standard usage of the language incompatible with mutating function calls!).\n\n\n6.8.3 Stability of a Method\nSimply having an order on the truncation error does not imply convergence of the method. The disconnect is that the errors at a given time point may not dissipate. What also needs to be checked is the asymptotic behavior of a disturbance. To see this, one can utilize the linear test problem:\n\\[u' = \\alpha u\\]\nand ask the question, does the discrete dynamical system defined by the discretized ODE end up going to zero? You would hope that the discretized dynamical system and the continuous dynamical system have the same properties in this simple case, and this is known as linear stability analysis of the method.\nAs an example, take a look at the Euler method. Recall that the Euler method was given by:\n\\[u_{n+1} = u_n + \\Delta t f(u_n,p,t)\\]\nWhen we plug in the linear test equation, we get that\n\\[u_{n+1} = u_n + \\Delta t \\alpha u_n\\]\nIf we let \\(z = \\Delta t \\alpha\\), then we get the following:\n\\[u_{n+1} = u_n + z u_n = (1+z)u_n\\]\nwhich is stable when \\(z\\) is in the shifted unit circle. This means that, as a necessary condition, the step size \\(\\Delta t\\) needs to be small enough that \\(z\\) satisfies this condition, placing a stepsize limit on the method.\n\nIf \\(\\Delta t\\) is ever too large, it will cause the equation to overshoot zero, which then causes oscillations that spiral out to infinity.\n\n\nThus the stability condition places a hard constraint on the allowed \\(\\Delta t\\) which will result in a realistic simulation.\nFor reference, the stability regions of the 2nd and 4th order Runge-Kutta methods that we discussed are as follows:\n\n\n\n6.8.4 Interpretation of the Linear Stability Condition\nTo interpret the linear stability condition, recall that the linearization of a system interprets the dynamics as locally being due to the Jacobian of the system. Thus\n\\[u' = f(u,p,t)\\]\nis locally equivalent to\n\\[u' = \\frac{df}{du}u\\]\nYou can understand the local behavior through diagonalizing this matrix. Therefore, the scalar for the linear stability analysis is performing an analysis on the eigenvalues of the Jacobian. The method will be stable if the largest eigenvalues of df/du are all within the stability limit. This means that stability effects are different throughout the solution of a nonlinear equation and are generally understood locally (though different more comprehensive stability conditions exist!).\n\n\n6.8.5 Implicit Methods\nIf instead of the Euler method we defined \\(f\\) to be evaluated at the future point, we would receive a method like:\n\\[u_{n+1} = u_n + \\Delta t f(u_{n+1},p,t+\\Delta t)\\]\nin which case, for the stability calculation we would have that\n\\[u_{n+1} = u_n + \\Delta t \\alpha u_n\\]\nor\n\\[(1-z) u_{n+1} = u_n\\]\nwhich means that\n\\[u_{n+1} = \\frac{1}{1-z} u_n\\]\nwhich is stable for all \\(Re(z) &lt; 0\\) a property which is known as A-stability. It is also stable as \\(z \\rightarrow \\infty\\), a property known as L-stability. This means that for equations with very ill-conditioned Jacobians, this method is still able to be use reasonably large stepsizes and can thus be efficient.\n\n\n\n6.8.6 Stiffness and Timescale Separation\nFrom this we see that there is a maximal stepsize whenever the eigenvalues of the Jacobian are sufficiently large. It turns out that’s not an issue if the phenomena we see are fast, since then the total integration time tends to be small. However, if we have some equations with both fast modes and slow modes, like the Robertson equation, then it is very difficult because in order to resolve the slow dynamics over a long timespan, one needs to ensure that the fast dynamics do not diverge. This is a property known as stiffness. Stiffness can thus be approximated in some sense by the condition number of the Jacobian. The condition number of a matrix is its maximal eigenvalue divided by its minimal eigenvalue and gives a rough measure of the local timescale separations. If this value is large and one wants to resolve the slow dynamics, then explict integrators, like the explicit Runge-Kutta methods described before, have issues with stability. In this case implicit integrators (or other forms of stabilized stepping) are required in order to efficiently reach the end time step."
  },
  {
    "objectID": "discretizing_odes.html#exploiting-continuity",
    "href": "discretizing_odes.html#exploiting-continuity",
    "title": "6  Ordinary Differential Equations, Applications and Discretizations",
    "section": "6.9 Exploiting Continuity",
    "text": "6.9 Exploiting Continuity\nSo far, we have looked at ordinary differential equations as a \\(\\Delta t \\rightarrow 0\\) formulation of a discrete dynamical system. However, continuous dynamics and discrete dynamics have very different characteristics which can be utilized in order to arrive at simpler models and faster computations.\n\n6.9.1 Geometric Properties: No Jumping and the Poincaré–Bendixson theorem\nIn terms of geometric properties, continuity places a large constraint on the possible dynamics. This is because of the physical constraint on “jumping”, i.e. flows of differential equations cannot jump over each other. If you are ever at some point in phase space and \\(f\\) is not explicitly time-dependent, then the direction of \\(u'\\) is uniquely determined (given reasonable assumptions on \\(f\\)), meaning that flow lines (solutions to the differential equation) can never cross.\nA result from this is the Poincaré–Bendixson theorem, which states that, with any arbitrary (but nice) two dimensional continuous system, you can only have 3 behaviors:\n\nSteady state behavior\nDivergence\nPeriodic orbits\n\nA simple proof by picture shows this."
  },
  {
    "objectID": "automatic_differentiation.html#youtube-video-link",
    "href": "automatic_differentiation.html#youtube-video-link",
    "title": "7  Forward-Mode Automatic Differentiation (AD) via High Dimensional Algebras",
    "section": "7.1 Youtube Video Link",
    "text": "7.1 Youtube Video Link"
  },
  {
    "objectID": "automatic_differentiation.html#machine-epsilon-and-roundoff-error",
    "href": "automatic_differentiation.html#machine-epsilon-and-roundoff-error",
    "title": "7  Forward-Mode Automatic Differentiation (AD) via High Dimensional Algebras",
    "section": "7.2 Machine Epsilon and Roundoff Error",
    "text": "7.2 Machine Epsilon and Roundoff Error\nFloating point arithmetic is relatively scaled, which means that the precision that you get from calculations is relative to the size of the floating point numbers. Generally, you have 16 digits of accuracy in (64-bit) floating point operations. To measure this, we define machine epsilon as the value by which 1 + E = 1. For floating point numbers, this is:\n\neps(Float64)\n\n2.220446049250313e-16\n\n\nHowever, since it’s relative, this value changes as we change our reference value:\n\n@show eps(1.0)\n@show eps(0.1)\n@show eps(0.01)\n\neps(1.0) = 2.220446049250313e-16\neps(0.1) = 1.3877787807814457e-17\neps(0.01) = 1.734723475976807e-18\n\n\n1.734723475976807e-18\n\n\nThus issues with roundoff error come when one subtracts out the higher digits. For example, \\((x + \\epsilon) - x\\) should just be \\(\\epsilon\\) if there was no roundoff error, but if \\(\\epsilon\\) is small then this kicks in. If \\(x = 1\\) and \\(\\epsilon\\) is of size around \\(10^{-10}\\), then \\(x+ \\epsilon\\) is correct for 10 digits, dropping off the smallest 6 due to error in the addition to \\(1\\). But when you subtract off \\(x\\), you don’t get those digits back, and thus you only have 6 digits of \\(\\epsilon\\) correct.\nLet’s see this in action:\n\nϵ = 1e-10rand()\n@show ϵ\n@show (1+ϵ)\nϵ2 = (1+ϵ) - 1\n(ϵ - ϵ2)\n\nϵ = 8.077473707516503e-12\n1 + ϵ = 1.0000000000080775\n\n\n-6.493044628591267e-17\n\n\nSee how \\(\\epsilon\\) is only rebuilt at accuracy around \\(10^{-16}\\) and thus we only keep around 6 digits of accuracy when it’s generated at the size of around \\(10^{-10}\\)!\n\n7.2.1 Finite Differencing and Numerical Stability\nTo start understanding how to compute derivatives on a computer, we start with finite differencing. For finite differencing, recall that the definition of the derivative is:\n\\[f'(x) = \\lim_{\\epsilon \\rightarrow 0} \\frac{f(x+\\epsilon)-f(x)}{\\epsilon}\\]\nFinite differencing directly follows from this definition by choosing a small \\(\\epsilon\\). However, choosing a good \\(\\epsilon\\) is very difficult. If \\(\\epsilon\\) is too large than there is error since this definition is asymptotic. However, if \\(\\epsilon\\) is too small, you receive roundoff error. To understand why you would get roundoff error, recall that floating point error is relative, and can essentially store 16 digits of accuracy. So let’s say we choose \\(\\epsilon = 10^{-6}\\). Then \\(f(x+\\epsilon) - f(x)\\) is roughly the same in the first 6 digits, meaning that after the subtraction there is only 10 digits of accuracy, and then dividing by \\(10^{-6}\\) simply brings those 10 digits back up to the correct relative size.\n\nThis means that we want to choose \\(\\epsilon\\) small enough that the \\(\\mathcal{O}(\\epsilon^2)\\) error of the truncation is balanced by the \\(O(1/\\epsilon)\\) roundoff error. Under some minor assumptions, one can argue that the average best point is \\(\\sqrt(E)\\), where E is machine epsilon\n\n@show eps(Float64)\n@show sqrt(eps(Float64))\n\neps(Float64) = 2.220446049250313e-16\nsqrt(eps(Float64)) = 1.4901161193847656e-8\n\n\n1.4901161193847656e-8\n\n\nThis means we should not expect better than 8 digits of accuracy, even when things are good with finite differencing.\n\nThe centered difference formula is a little bit better, but this picture suggests something much better…\n\n\n7.2.2 Differencing in a Different Dimension: Complex Step Differentiation\nThe problem with finite differencing is that we are mixing our really small number with the really large number, and so when we do the subtract we lose accuracy. Instead, we want to keep the small perturbation completely separate.\nTo see how to do this, assume that \\(x \\in \\mathbb{R}\\) and assume that \\(f\\) is complex analytic. You want to calculate a real derivative, but your function just happens to also be complex analytic when extended to the complex plane. Thus it has a Taylor series, and let’s see what happens when we expand out this Taylor series purely in the complex direction:\n\\[f(x+ih) = f(x) + f'(x)ih - \\frac{1}{2}f''(x)h^2 + \\mathcal{O}(h^3)\\]\nwhich we can re-arrange as:\n\\[if'(x) = \\frac{f(x+ih) - f(x)}{h} + \\frac{1}{2}f''(x)h + \\mathcal{O}(h^2)\\]\nSince \\(x\\) is real and \\(f\\) is real-valued on the reals, \\(if'\\) is purely imaginary. So let’s take the imaginary parts of both sides:\n\\[f'(x) = \\frac{Im(f(x+ih))}{h} + \\mathcal{O}(h^2)\\]\nsince \\(Im(f(x)) = 0\\) (since it’s real valued, the next order term cancels for the same reason). Thus with a sufficiently small choice of \\(h\\), this is the complex step differentiation formula for calculating the derivative.\nBut to understand the computational advantage, recall that \\(x\\) is pure real, and thus \\(x+ih\\) is a complex number where the \\(h\\) never directly interacts with \\(x\\) since a complex number is a two dimensional number where you keep the two pieces separate. Thus there is no numerical cancellation by using a small value of \\(h\\), and thus, due to the relative precision of floating point numbers, both the real and imaginary parts will be computed to (approximately) 16 digits of accuracy for any choice of \\(h\\).\n\n\n7.2.3 Derivatives as nilpotent sensitivities\nThe derivative measures the sensitivity of a function, i.e. how much the function output changes when the input changes by a small amount \\(\\epsilon\\):\n\\[f(a + \\epsilon) = f(a) + f'(a) \\epsilon + o(\\epsilon).\\]\nIn the following we will ignore higher-order terms; formally we set \\(\\epsilon^2 = 0\\). This form of analysis can be made rigorous through a form of non-standard analysis called Smooth Infinitesimal Analysis [1], though note that nilpotent infinitesimal requires constructive logic, and thus proof by contradiction is not allowed in this logic due to a lack of the law of the excluded middle.\nA function \\(f\\) will be represented by its value \\(f(a)\\) and derivative \\(f'(a)\\), encoded as the coefficients of a degree-1 (Taylor) polynomial in \\(\\epsilon\\):\n\\[f \\rightsquigarrow f(a) + \\epsilon f'(a)\\]\nConversely, if we have such an expansion in \\(\\epsilon\\) for a given function \\(f\\), then we can identify the coefficient of \\(\\epsilon\\) as the derivative of \\(f\\).\n\n\n7.2.4 Dual numbers\nThus, to extend the idea of complex step differentiation beyond complex analytic functions, we define a new number type, the dual number. A dual number is a multidimensional number where the sensitivity of the function is propagated along the dual portion.\nHere we will now start to use \\(\\epsilon\\) as a dimensional signifier, like \\(i\\), \\(j\\), or \\(k\\) for quaternion numbers. In order for this to work out, we need to derive an appropriate algebra for our numbers. To do this, we will look at Taylor series to make our algebra reconstruct differentiation.\nNote that the chain rule has been explicitly encoded in the derivative part.\n\\[f(a + \\epsilon) = f(a) + \\epsilon f'(a)\\]\nto first order. If we have two functions\n\\[f \\rightsquigarrow f(a) + \\epsilon f'(a)\\] \\[g \\rightsquigarrow g(a) + \\epsilon g'(a)\\]\nthen we can manipulate these Taylor expansions to calculate combinations of these functions as follows. Using the nilpotent algebra, we have that:\n\\[(f + g) = [f(a) + g(a)] + \\epsilon[f'(a) + g'(a)]\\]\n\\[(f \\cdot g) = [f(a) \\cdot g(a)] + \\epsilon[f(a) \\cdot g'(a) + g(a) \\cdot f'(a) ]\\]\nFrom these we can infer the derivatives by taking the component of \\(\\epsilon\\). These also tell us the way to implement these in the computer.\n\n\n7.2.5 Computer representation\nSetup (not necessary from the REPL):\n\nusing InteractiveUtils  # only needed when using Weave\n\nEach function requires two pieces of information and some particular “behavior”, so we store these in a struct. It’s common to call this a “dual number”:\n\nstruct Dual{T}\n    val::T   # value\n    der::T  # derivative\nend\n\nEach Dual object represents a function. We define arithmetic operations to mirror performing those operations on the corresponding functions.\nWe must first import the operations from Base:\n\nBase.:+(f::Dual, g::Dual) = Dual(f.val + g.val, f.der + g.der)\nBase.:+(f::Dual, α::Number) = Dual(f.val + α, f.der)\nBase.:+(α::Number, f::Dual) = f + α\n\n#=\nYou can also write:\nimport Base: +\nf::Dual + g::Dual = Dual(f.val + g.val, f.der + g.der)\n=#\n\nBase.:-(f::Dual, g::Dual) = Dual(f.val - g.val, f.der - g.der)\n\n# Product Rule\nBase.:*(f::Dual, g::Dual) = Dual(f.val*g.val, f.der*g.val + f.val*g.der)\nBase.:*(α::Number, f::Dual) = Dual(f.val * α, f.der * α)\nBase.:*(f::Dual, α::Number) = α * f\n\n# Quotient Rule\nBase.:/(f::Dual, g::Dual) = Dual(f.val/g.val, (f.der*g.val - f.val*g.der)/(g.val^2))\nBase.:/(α::Number, f::Dual) = Dual(α/f.val, -α*f.der/f.val^2)\nBase.:/(f::Dual, α::Number) = f * inv(α) # Dual(f.val/α, f.der * (1/α))\n\nBase.:^(f::Dual, n::Integer) = Base.power_by_squaring(f, n)  # use repeated squaring for integer powers\n\nWe can now define Duals and manipulate them:\n\nfd = Dual(3, 4)\ngd = Dual(5, 6)\n\nfd + gd\n\nDual{Int64}(8, 10)\n\n\n\nfd * gd\n\nDual{Int64}(15, 38)\n\n\n\nfd * (gd + gd)\n\nDual{Int64}(30, 76)\n\n\n\n\n7.2.6 Performance\nIt seems like we may have introduced significant computational overhead by creating a new data structure, and associated methods. Let’s see how the performance is:\n\nadd(a1, a2, b1, b2) = (a1+b1, a2+b2)\n\nadd (generic function with 1 method)\n\n\n\nadd(1, 2, 3, 4)\n\nusing BenchmarkTools\na, b, c, d = 1, 2, 3, 4\n@btime add($(Ref(a))[], $(Ref(b))[], $(Ref(c))[], $(Ref(d))[])\n\n  2.730 ns (0 allocations: 0 bytes)\n\n\n(4, 6)\n\n\n\na = Dual(1, 2)\nb = Dual(3, 4)\n\nadd(j1, j2) = j1 + j2\nadd(a, b)\n@btime add($(Ref(a))[], $(Ref(b))[])\n\n  2.924 ns (0 allocations: 0 bytes)\n\n\nDual{Int64}(4, 6)\n\n\nIt seems like we have lost no performance.\n\n@code_native add(1, 2, 3, 4)\n\n    \n\n\n.text\n    .file   \"add\"\n    .globl  julia_add_1491                  # -- Begin function julia_add_1491\n    .p2align    4, 0x90\n    .type   julia_add_1491,@function\njulia_add_1491:                         # @julia_add_1491\n; ┌ @ In[13]:1 within `add`\n    .cfi_startproc\n# %bb.0:                                # %top\n    pushq   %rbp\n    .cfi_def_cfa_offset 16\n    .cfi_offset %rbp, -16\n    movq    %rsp, %rbp\n    .cfi_def_cfa_register %rbp\n    movq    %rdi, %rax\n; │┌ @ int.jl:87 within `+`\n    addq    %rcx, %rsi\n    addq    %r8, %rdx\n; │└\n    movq    %rsi, (%rdi)\n    movq    %rdx, 8(%rdi)\n    popq    %rbp\n    .cfi_def_cfa %rsp, 8\n    retq\n.Lfunc_end0:\n    .size   julia_add_1491, .Lfunc_end0-julia_add_1491\n    .cfi_endproc\n; └\n                                        # -- End function\n    .section    \".note.GNU-stack\",\"\",@progbits\n\n\n\n@code_native add(a, b)\n\n    .text\n    .file   \"add\"\n    .globl  julia_add_1514                  # -- Begin function julia_add_1514\n    .p2align    4, 0x90\n    .type   julia_add_1514,@function\njulia_add_1514:                         # @julia_add_1514\n; ┌ @ In[15]:4 within `add`\n    .cfi_startproc\n# %bb.0:                                # %top\n    pushq   %rbp\n    .cfi_def_cfa_offset 16\n    .cfi_offset %rbp, -16\n    movq    %rsp, %rbp\n    .cfi_def_cfa_register %rbp\n    movq    %rdi, %rax\n; │┌ @ In[9]:1 within `+` @ int.jl:87\n    vmovdqu (%rdx), %xmm0\n    vpaddq  (%rsi), %xmm0, %xmm0\n; │└\n    vmovdqu %xmm0, (%rdi)\n    popq    %rbp\n    .cfi_def_cfa %rsp, 8\n    retq\n.Lfunc_end0:\n    .size   julia_add_1514, .Lfunc_end0-julia_add_1514\n    .cfi_endproc\n; └\n                                        # -- End function\n    .section    \".note.GNU-stack\",\"\",@progbits\n\n\nWe see that the data structure itself has disappeared, and we basically have a standard Julia tuple.\n\n\n7.2.7 Defining Higher Order Primitives\nWe can also define functions of Dual objects, using the chain rule. To speed up our derivative function, we can directly hardcode the derivative of known functions which we call primitives. If f is a Dual representing the function \\(f\\), then exp(f) should be a Dual representing the function \\(\\exp \\circ f\\), i.e. with value \\(\\exp(f(a))\\) and derivative \\((\\exp \\circ f)'(a) = \\exp(f(a)) \\, f'(a)\\):\n\nimport Base: exp\n\n\nexp(f::Dual) = Dual(exp(f.val), exp(f.val) * f.der)\n\nexp (generic function with 14 methods)\n\n\n\nfd\n\nDual{Int64}(3, 4)\n\n\n\nexp(fd)\n\nDual{Float64}(20.085536923187668, 80.34214769275067)"
  },
  {
    "objectID": "automatic_differentiation.html#differentiating-arbitrary-functions",
    "href": "automatic_differentiation.html#differentiating-arbitrary-functions",
    "title": "7  Forward-Mode Automatic Differentiation (AD) via High Dimensional Algebras",
    "section": "7.3 Differentiating arbitrary functions",
    "text": "7.3 Differentiating arbitrary functions\nFor functions where we don’t have a rule, we can recursively do dual number arithmetic within the function until we hit primitives where we know the derivative, and then use the chain rule to propagate the information back up. Under this algebra, we can represent \\(a + \\epsilon\\) as Dual(a, 1). Thus, applying f to Dual(a, 1) should give Dual(f(a), f'(a)). This is thus a 2-dimensional number for calculating the derivative without floating point error, using the compiler to transform our equations into dual number arithmetic. To differentiate an arbitrary function, we define a generic function and then change the algebra.\n\nhf(x) = x^2 + 2\na = 3\nxx = Dual(a, 1)\n\nDual{Int64}(3, 1)\n\n\nNow we simply evaluate the function h at the Dual number xx:\n\nhf(xx)\n\nDual{Int64}(11, 6)\n\n\nThe first component of the resulting Dual is the value \\(h(a)\\), and the second component is the derivative, \\(h'(a)\\)!\nWe can codify this into a function as follows:\n\nderivative(f, x) = f(Dual(x, one(x))).der\n\nderivative (generic function with 1 method)\n\n\nHere, one is the function that gives the value \\(1\\) with the same type as that of x.\nFinally we can now calculate derivatives such as\n\nderivative(x -&gt; 3x^5 + 2, 2)\n\n240\n\n\nAs a bigger example, we can take a pure Julia sqrt function and differentiate it by changing the internal algebra:\n\nfunction newtons(x)\n   a = x\n   for i in 1:300\n       a = 0.5 * (a + x/a)\n   end\n   a\nend\n@show newtons(2.0)\n@show (newtons(2.0+sqrt(eps())) - newtons(2.0))/ sqrt(eps())\nnewtons(Dual(2.0,1.0))\n\nnewtons(2.0) = 1.414213562373095\n(newtons(2.0 + sqrt(eps())) - newtons(2.0)) / sqrt(eps()) = 0.3535533994436264\n\n\nDual{Float64}(1.414213562373095, 0.35355339059327373)\n\n\n\n7.3.1 Higher dimensions\nHow can we extend this to higher dimensional functions? For example, we wish to differentiate the following function \\(f: \\mathbb{R}^2 \\to \\mathbb{R}\\):\n\nfquad(x, y) = x^2 + x*y\n\nfquad (generic function with 1 method)\n\n\nRecall that the partial derivative \\(\\partial f/\\partial x\\) is defined by fixing \\(y\\) and differentiating the resulting function of \\(x\\):\n\na, b = 3.0, 4.0\n\nfquad_1(x) = fquad(x, b)  # single-variable function\n\nfquad_1 (generic function with 1 method)\n\n\nSince we now have a single-variable function, we can differentiate it:\n\nderivative(fquad_1, a)\n\n10.0\n\n\nUnder the hood this is doing\n\nfquad(Dual(a, one(a)), b)\n\nDual{Float64}(21.0, 10.0)\n\n\nSimilarly, we can differentiate with respect to \\(y\\) by doing\n\nfquad_2(y) = fquad(a, y)  # single-variable function\n\nderivative(fquad_2, b)\n\n3.0\n\n\nNote that we must do two separate calculations to get the two partial derivatives; in general, calculating the gradient \\(\\nabla\\) of a function \\(f:\\mathbb{R}^n \\to \\mathbb{R}\\) requires \\(n\\) separate calculations.\n\n\n7.3.2 Implementation of higher-dimensional forward-mode AD\nWe can implement derivatives of functions \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) by adding several independent partial derivative components to our dual numbers.\nWe can think of these as \\(\\epsilon\\) perturbations in different directions, which satisfy \\(\\epsilon_i^2 = \\epsilon_i \\epsilon_j = 0\\), and we will call \\(\\epsilon\\) the vector of all perturbations. Then we have\n\\[f(a + \\epsilon) = f(a) + \\nabla f(a) \\cdot \\epsilon + \\mathcal{O}(\\epsilon^2),\\]\nwhere \\(a \\in \\mathbb{R}^n\\) and \\(\\nabla f(a)\\) is the gradient of \\(f\\) at \\(a\\), i.e. the vector of partial derivatives in each direction. \\(\\nabla f(a) \\cdot \\epsilon\\) is the directional derivative of \\(f\\) in the direction \\(\\epsilon\\).\nWe now proceed similarly to the univariate case:\n\\[(f + g)(a + \\epsilon) = [f(a) + g(a)] + [\\nabla f(a) + \\nabla g(a)] \\cdot \\epsilon\\]\n\\[\\begin{align}\n(f \\cdot g)(a + \\epsilon) &= [f(a) + \\nabla f(a) \\cdot \\epsilon ] \\, [g(a) + \\nabla g(a) \\cdot \\epsilon ] \\\\\n&= f(a) g(a) + [f(a) \\nabla g(a) + g(a) \\nabla f(a)] \\cdot \\epsilon.\n\\end{align}\\]\nWe will use the StaticArrays.jl package for efficient small vectors:\n\nusing StaticArrays\n\nstruct MultiDual{N,T}\n    val::T\n    derivs::SVector{N,T}\nend\n\nimport Base: +, *\n\nfunction +(f::MultiDual{N,T}, g::MultiDual{N,T}) where {N,T}\n    return MultiDual{N,T}(f.val + g.val, f.derivs + g.derivs)\nend\n\nfunction *(f::MultiDual{N,T}, g::MultiDual{N,T}) where {N,T}\n    return MultiDual{N,T}(f.val * g.val, f.val .* g.derivs + g.val .* f.derivs)\nend\n\n* (generic function with 335 methods)\n\n\n\ngcubic(x, y) = x*x*y + x + y\n\n(a, b) = (1.0, 2.0)\n\nxx = MultiDual(a, SVector(1.0, 0.0))\nyy = MultiDual(b, SVector(0.0, 1.0))\n\ngcubic(xx, yy)\n\nMultiDual{2, Float64}(5.0, [5.0, 2.0])\n\n\nWe can calculate the Jacobian of a function \\(\\mathbb{R}^n \\to \\mathbb{R}^m\\) by applying this to each component function:\n\nfsvec(x, y) = SVector(x*x + y*y , x + y)\n\nfsvec(xx, yy)\n\n2-element SVector{2, MultiDual{2, Float64}} with indices SOneTo(2):\n MultiDual{2, Float64}(5.0, [2.0, 4.0])\n MultiDual{2, Float64}(3.0, [1.0, 1.0])\n\n\nIt would be possible (and better for performance in many cases) to store all of the partials in a matrix instead.\nForward-mode AD is implemented in a clean and efficient way in the ForwardDiff.jl package:\n\nusing ForwardDiff, StaticArrays\n\nForwardDiff.gradient( xx -&gt; ( (x, y) = xx; x^2 * y + x*y ), [1, 2])\n\n2-element Vector{Int64}:\n 6\n 2\n\n\n\n\n7.3.3 Directional derivative and gradient of functions \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\)\nFor a function \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) the basic operation is the directional derivative:\n\\[\\lim_{\\epsilon \\to 0} \\frac{f(\\mathbf{x} + \\epsilon \\mathbf{v}) - f(\\mathbf{x})}{\\epsilon} =\n[\\nabla f(\\mathbf{x})] \\cdot \\mathbf{v},\\]\nwhere \\(\\epsilon\\) is still a single dimension and \\(\\nabla f(\\mathbf{x})\\) is the direction in which we calculate.\nWe can directly do this using the same simple Dual numbers as above, using the same \\(\\epsilon\\), e.g.\n\\[f(x, y) = x^2  \\sin(y)\\]\n\\[\\begin{align}\nf(x_0 + a\\epsilon, y_0 + b\\epsilon) &= (x_0 + a\\epsilon)^2  \\sin(y_0 + b\\epsilon) \\\\\n&= x_0^2  \\sin(y_0) + \\epsilon[2ax_0  \\sin(y_0) + x_0^2 b \\cos(y_0)] + o(\\epsilon)\n\\end{align}\\]\nso we have indeed calculated \\(\\nabla f(x_0, y_0) \\cdot \\mathbf{v},\\) where \\(\\mathbf{v} = (a, b)\\) are the components that we put into the derivative component of the Dual numbers.\nIf we wish to calculate the directional derivative in another direction, we could repeat the calculation with a different \\(\\mathbf{v}\\). A better solution is to use another independent epsilon \\(\\epsilon\\), expanding \\[x = x_0 + a_1 \\epsilon_1 + a_2 \\epsilon_2\\] and putting \\(\\epsilon_1 \\epsilon_2 = 0\\).\nIn particular, if we wish to calculate the gradient itself, \\(\\nabla f(x_0, y_0)\\), we need to calculate both partial derivatives, which corresponds to two directional derivatives, in the directions \\((1, 0)\\) and \\((0, 1)\\), respectively.\n\n\n7.3.4 Forward-Mode AD as jvp\nNote that another representation of the directional derivative is \\(f'(x)v\\), where \\(f'(x)\\) is the Jacobian or total derivative of \\(f\\) at \\(x\\). To see the equivalence of this to a directional derivative, write it out in the standard basis:\n\\[w_i = \\sum_{j}^{m} J_{ij} v_{j}\\]\nNow write out what \\(J\\) means and we see that:\n\\[w_i = \\sum_j^{m} \\frac{df_i}{dx_j} v_j = \\nabla f_i(x) \\cdot v\\]\nThe primitive action of forward-mode AD is \\(f'(x)v\\)!\nThis is also known as a Jacobian-vector product, or jvp for short.\nWe can thus represent vector calculus with multidimensional dual numbers as follows. Let \\(d =[x,y]\\), the vector of dual numbers. We can instead represent this as:\n\\[d = d_0 + v_1 \\epsilon_1 + v_2 \\epsilon_2\\]\nwhere \\(d_0\\) is the primal vector \\([x_0,y_0]\\) and the \\(v_i\\) are the vectors for the dual directions. If you work out this algebra, then note that a single application of \\(f\\) to a multidimensional dual number calculates:\n\\[f(d) = f(d_0) + f'(d_0)v_1 \\epsilon_1 + f'(d_0)v_2 \\epsilon_2\\]\ni.e. it calculates the result of \\(f(x,y)\\) and two separate directional derivatives. Note that because the information about \\(f(d_0)\\) is shared between the calculations, this is more efficient than doing multiple applications of \\(f\\). And of course, this is then generalized to \\(m\\) many directional derivatives at once by:\n\\[d = d_0 + v_1 \\epsilon_1 + v_2 \\epsilon_2 + \\ldots + v_m \\epsilon_m\\]\n\n\n7.3.5 Jacobian\nFor a function \\(f: \\mathbb{R}^n \\to \\mathbb{R}^m\\), we reduce (conceptually, although not necessarily in code) to its component functions \\(f_i: \\mathbb{R}^n \\to \\mathbb{R}\\), where \\(f(x) = (f_1(x), f_2(x), \\ldots, f_m(x))\\).\nThen\n\\[\\begin{align}\nf(x + \\epsilon v) &= (f_1(x + \\epsilon v), \\ldots, f_m(x + \\epsilon v)) \\\\\n&= (f_1(x) + \\epsilon[\\nabla f_1(x) \\cdot v], \\dots, f_m(x) + \\epsilon[\\nabla f_m(x) \\cdot v] \\\\\n&= f(x) + [f'(x) \\cdot v] \\epsilon,\n\\end{align}\\]\nTo calculate the complete Jacobian, we calculate these directional derivatives in the \\(n\\) different directions of the basis vectors, i.e. if\n\\(d = d_0 + e_1 \\epsilon_1 + \\ldots + e_n \\epsilon_n\\)\nfor \\(e_i\\) the \\(i\\)th basis vector, then\n\\(f(d) = f(d_0) + Je_1 \\epsilon_1 + \\ldots + Je_n \\epsilon_n\\)\ncomputes all columns of the Jacobian simultaneously.\n\n\n7.3.6 Array of Structs Representation\nInstead of thinking about a vector of dual numbers, thus we can instead think of dual numbers with vectors for the components. But if there are vectors for the components, then we can think of the grouping of dual components as a matrix. Thus define our multidimensional multi-partial dual number as:\n\\[D_0 = [d_1,d_2,d_3,\\ldots,d_n]\\]\n\\[\\Sigma = \\begin{bmatrix}\n        d_{11} & d_{12} & \\cdots & d_{1n} \\\\\n        d_{21} & d_{22} &  & \\vdots \\\\\n        \\vdots & & \\ddots & \\vdots \\\\\n        d_{m1} & \\hdots & \\hdots & d_{mn}\n    \\end{bmatrix}\\]\n\\[\\epsilon=[\\epsilon_1,\\epsilon_2,\\ldots,\\epsilon_m]\\]\n\\[D = D_0 + \\Sigma \\epsilon\\]\nwhere \\(D_0\\) is a vector in \\(\\mathbb{R}^n\\), \\(\\epsilon\\) is a vector of dimensional signifiers and \\(\\Sigma\\) is a matrix in \\(\\mathbb{R}^{n \\times m}\\) where \\(m\\) is the number of concurrent differentiation dimensions. Each row of this is a dual number, but now we can use this to easily define higher dimensional primitives.\nFor example, let \\(f(x) = Ax\\), matrix multiplication. Then, we can show with our dual number arithmetic that:\n\\[f(D) = A*D_0 + A*\\Sigma*\\epsilon\\]\nis how one would compute the value of \\(f(D_0)\\) and the derivative \\(f'(D_0)\\) in all directions signified by the columns of \\(\\Sigma\\) simultaneously. Using multidimensional Taylor series expansions and doing the manipulations like before indeed implies that the arithmetic on this object should follow:\n\\[f(D) = f(D_0) + f'(D_0)\\Sigma \\epsilon\\]\nwhere \\(f'\\) is the total derivative or the Jacobian of \\(f\\). This then allows our system to be highly efficient by allowing the definition of multidimensional functions, like linear algebra, to be primitives of multi-directional derivatives.\n\n\n7.3.7 Higher derivatives\nThe above techniques can be extended to higher derivatives by adding more terms to the Taylor polynomial, e.g.\n\\[f(a + \\epsilon) = f(a) + \\epsilon f'(a) + \\frac{1}{2} \\epsilon^2 f''(a) + o(\\epsilon^2).\\]\nWe treat this as a degree-2 (or degree-\\(n\\), in general) polynomial and do polynomial arithmetic to calculate the new polynomials. The coefficients of powers of \\(\\epsilon\\) then give the higher-order derivatives.\nFor example, for a function \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) we have\n\\[f(x + \\epsilon v) = f(x) + \\epsilon \\left[ \\sum_i (\\partial_i f)(x) v_i \\right] + \\frac{1}{2}\\epsilon^2 \\left[ \\sum_i \\sum_j (\\partial_{i,j} f) v_i v_j \\right]\\]\nusing Dual numbers with a single \\(\\epsilon\\) component. In this way we can compute coefficients of the (symmetric) Hessian matrix."
  },
  {
    "objectID": "automatic_differentiation.html#application-solving-nonlinear-equations-using-the-newton-method",
    "href": "automatic_differentiation.html#application-solving-nonlinear-equations-using-the-newton-method",
    "title": "7  Forward-Mode Automatic Differentiation (AD) via High Dimensional Algebras",
    "section": "7.4 Application: solving nonlinear equations using the Newton method",
    "text": "7.4 Application: solving nonlinear equations using the Newton method\nAs an application, we will see how to solve nonlinear equations of the form \\[f(x) = 0\\] for functions \\(f: \\mathbb{R}^n \\to \\mathbb{R}^n\\).\nSince in general we cannot do anything with nonlinearity, we try to reduce it (approximate it) with something linear. Furthermore, in general we know that it is not possible to solve nonlinear equations in closed form (even for polynomials of degree \\(\\ge 5\\)), so we will need some kind of iterative method.\nWe start from an initial guess \\(x_0\\). The idea of the Newton method is to follow the tangent line to the function \\(f\\) at the point \\(x_0\\) and find where it intersects the \\(x\\)-axis; this will give the next iterate \\(x_1\\).\nAlgebraically, we want to solve \\(f(x_1) = 0\\). Suppose that \\(x_1 = x_0 + \\delta\\) for some \\(\\delta\\) that is currently unknown and which we wish to calculate.\nAssuming \\(\\delta\\) is small, we can expand:\n\\[f(x_1) = f(x_0 + \\delta) = f(x_0) + Df(x_0) \\cdot \\delta + \\mathcal{O}(\\| \\delta \\|^2).\\]\nSince we wish to solve\n\\[f(x_0 + \\delta) \\simeq 0,\\]\nwe put\n\\[f(x_0) + Df(x_0) \\cdot \\delta = 0,\\]\nso that mathematically we have\n\\[\\delta = -[Df(x_0)]^{-1} \\cdot f(x_0).\\]\nComputationally we prefer to solve the matrix equation\n\\[J \\delta = -f(x_0),\\]\nwhere \\(J := Df(x_0)\\) is the Jacobian of the function; Julia uses the syntax \\ (“backslash”) for solving linear systems in an efficient way:\n\nusing ForwardDiff, StaticArrays\n\nfunction newton_step(f, x0)\n    J = ForwardDiff.jacobian(f, x0)\n    δ = J \\ f(x0)\n\n    return x0 - δ\nend\n\nfunction newton(f, x0)\n    x = x0\n\n    for i in 1:10\n        x = newton_step(f, x)\n        @show x\n    end\n\n    return x\nend\n\nfsvec2(xx) = ( (x, y) = xx;  SVector(x^2 + y^2 - 1, x - y) )\n\nx0 = SVector(3.0, 5.0)\n\nx = newton(fsvec2, x0)\n\nx = [2.1875, 2.1875]\nx = [1.2080357142857143, 1.2080357142857143]\nx = [0.8109653811635519, 0.8109653811635519]\nx = [0.7137572554482892, 0.7137572554482892]\nx = [0.7071377642746832, 0.7071377642746832]\nx = [0.7071067818653062, 0.7071067818653062]\nx = [0.7071067811865475, 0.7071067811865475]\nx = [0.7071067811865476, 0.7071067811865476]\nx = [0.7071067811865475, 0.7071067811865475]\nx = [0.7071067811865476, 0.7071067811865476]\n\n\n2-element SVector{2, Float64} with indices SOneTo(2):\n 0.7071067811865476\n 0.7071067811865476"
  },
  {
    "objectID": "automatic_differentiation.html#conclusion",
    "href": "automatic_differentiation.html#conclusion",
    "title": "7  Forward-Mode Automatic Differentiation (AD) via High Dimensional Algebras",
    "section": "7.5 Conclusion",
    "text": "7.5 Conclusion\nTo make derivative calculations efficient and correct, we can move to higher dimensional numbers. In multiple dimensions, these then allow for multiple directional derivatives to be computed simultaneously, giving a method for computing the Jacobian of a function \\(f\\) on a single input. This is a direct application of using the compiler as part of a mathematical framework.\n\n7.5.1 References\n\nJohn L. Bell, An Invitation to Smooth Infinitesimal Analysis, http://publish.uwo.ca/~jbell/invitation%20to%20SIA.pdf\nBell, John L. A Primer of Infinitesimal Analysis\nNocedal & Wright, Numerical Optimization, Chapter 8\nGriewank & Walther, Evaluating Derivatives\n\nMany thanks to David Sanders for helping make these lecture notes."
  },
  {
    "objectID": "stiff_odes.html#youtube-video-link",
    "href": "stiff_odes.html#youtube-video-link",
    "title": "8  Solving Stiff Ordinary Differential Equations",
    "section": "8.1 Youtube Video Link",
    "text": "8.1 Youtube Video Link\nWe have previously shown how to solve non-stiff ODEs via optimized Runge-Kutta methods, but we ended by showing that there is a fundamental limitation of these methods when attempting to solve stiff ordinary differential equations. However, we can get around these limitations by using different types of methods, like implicit Euler. Let’s now go down the path of understanding how to efficiently implement stiff ordinary differential equation solvers, and its interaction with other domains like automatic differentiation.\nWhen one is solving a large-scale scientific computing problem with MPI, this is almost always the piece of code where all of the time is spent, so let’s understand how what it’s doing."
  },
  {
    "objectID": "stiff_odes.html#newtons-method-and-jacobians",
    "href": "stiff_odes.html#newtons-method-and-jacobians",
    "title": "8  Solving Stiff Ordinary Differential Equations",
    "section": "8.2 Newton’s Method and Jacobians",
    "text": "8.2 Newton’s Method and Jacobians\nRecall that the implicit Euler method is the following:\n\\[u_{n+1} = u_n + \\Delta t f(u_{n+1},p,t + \\Delta t)\\]\nIf we wanted to use this method, we would need to find out how to get the value \\(u_{n+1}\\) when only knowing the value \\(u_n\\). To do so, we can move everything to one side:\n\\[u_{n+1} - \\Delta t f(u_{n+1},p,t + \\Delta t) - u_n = 0\\]\nand now we have a problem\n\\[g(u_{n+1}) = 0\\]\nThis is the classic rootfinding problem \\[g(x)=0\\], find \\(x\\). The way that we solve the rootfinding problem is, once again, by replacing this problem about a continuous function \\(g\\) with a discrete dynamical system whose steady state is the solution to the \\[g(x)=0\\]. There are many methods for this, but some choices of the rootfinding method effect the stability of the ODE solver itself since we need to make sure that the steady state solution is a stable steady state of the iteration process, otherwise the rootfinding method will diverge (will be explored in the homework).\nThus for example, fixed point iteration is not appropriate for stiff differential equations. Methods which are used in the stiff case are either Anderson Acceleration or Newton’s method. Newton’s is by far the most common (and generally performs the best), so we can go down this route.\nLet’s use the syntax \\[g(x)=0\\]. Here we need some starting value \\(x_0\\) as our first guess for \\(u_{n+1}\\). The easiest guess is \\(u_{n}\\), though additional information about the equation can be used to compute a better starting value (known as a step predictor). Once we have a starting value, we run the iteration:\n\\[x_{k+1} = x_k - J(x_k)^{-1}g(x_k)\\]\nwhere \\(J(x_k)\\) is the Jacobian of \\(g\\) at the point \\(x_k\\). However, the mathematical formulation is never the syntax that you should use for the actual application! Instead, numerically this is two stages:\n\nSolve \\(Ja=g(x_k)\\) for \\(a\\)\nUpdate \\(x_{k+1} = x_k - a\\)\n\nBy doing this, we can turn the matrix inversion into a problem of a linear solve and then an update. The reason this is done is manyfold, but one major reason is because the inverse of a sparse matrix can be dense, and this Jacobian is in many cases (PDEs) a large and dense matrix.\nNow let’s break this down step by step.\n\n8.2.1 Some Quick Notes\nThe Jacobian of \\(g\\) can also be written as \\(J = I - \\gamma \\frac{df}{du}\\) for the ODE \\(u' = f(u,p,t)\\), where \\(\\gamma = \\Delta t\\) for the implicit Euler method. This general form holds for all other (SDIRK) implicit methods, changing the value of \\(\\gamma\\). Additionally, the class of Rosenbrock methods solves a linear system with exactly the same \\(J\\), meaning that essentially all implicit and semi-implicit ODE solvers have to do the same Newton iteration process on the same structure. This is the portion of the code that is generally the bottleneck.\nAdditionally, if one is solving a mass matrix ODE: \\(Mu' = f(u,p,t)\\), exactly the same treatment can be had with \\(J = M - \\gamma \\frac{df}{du}\\). This works even if \\(M\\) is singular, a case known as a differential-algebraic equation or a DAE. A DAE for example can be an ODE with constraint equations, and these structures can be represented as an ODE where these constraints lead to a singularity in the mass matrix (a row of all zeros is a term that is only the right hand side equals zero!)."
  },
  {
    "objectID": "stiff_odes.html#generation-of-the-jacobian",
    "href": "stiff_odes.html#generation-of-the-jacobian",
    "title": "8  Solving Stiff Ordinary Differential Equations",
    "section": "8.3 Generation of the Jacobian",
    "text": "8.3 Generation of the Jacobian\n\n8.3.1 Dense Finite Differences and Forward-Mode AD\nRecall that the Jacobian is the matrix of \\(\\frac{df_i}{dx_j}\\) for \\(f\\) a vector-valued function. The simplest way to generate the Jacobian is through finite differences. For each \\(h_j = h e_j\\) for \\(e_j\\) the basis vector of the \\(j\\)th axis and some sufficiently small \\(h\\), then we can compute column \\(j\\) of the Jacobian by:\n\\[\\frac{f(x+h_j)-f(x)}{h}\\]\nThus \\(m+1\\) applications of \\(f\\) are required to compute the full Jacobian.\nThis can be improved by using forward-mode automatic differentiation. Recall that we can formulate a multidimensional duel number of the form\n\\[d = x + v_1 \\epsilon_1 + \\ldots + v_m \\epsilon_m\\]\nWe can then seed the vectors \\(v_j = h_j\\) so that the differentiation directions are along the basis vectors, and then the output dual is the result:\n\\[f(d) = f(x) + J_1 \\epsilon_1 + \\ldots + J_m \\epsilon_m\\]\nwhere \\(J_j\\) is the \\(j\\)th column of the Jacobian. And thus with one calculation of the primal (f(x)) we have calculated the entire Jacobian.\n\n\n8.3.2 Sparse Differentiation and Matrix Coloring\nHowever, when the Jacobian is sparse we can compute it much faster. We can understand this by looking at the following system:\n\\[f(x)=\\left[\\begin{array}{c}\nx_{1}+x_{3}\\\\\nx_{2}x_{3}\\\\\nx_{1}\n\\end{array}\\right]\\]\nNotice that in 3 differencing steps we can calculate:\n\\[f(x+\\epsilon e_{1})=\\left[\\begin{array}{c}\nx_{1}+x_{3}+\\epsilon\\\\\nx_{2}x_{3}\\\\\nx_{1}+\\epsilon\n\\end{array}\\right]\\]\n\\[f(x+\\epsilon e_{2})=\\left[\\begin{array}{c}\nx_{1}+x_{3}\\\\\nx_{2}x_{3}+\\epsilon x_{3}\\\\\nx_{1}\n\\end{array}\\right]\\]\n\\[f(x+\\epsilon e_{3})=\\left[\\begin{array}{c}\nx_{1}+x_{3}+\\epsilon\\\\\nx_{2}x_{3}+\\epsilon x_{2}\\\\\nx_{1}\n\\end{array}\\right]\\]\nand thus:\n\\[\\frac{f(x+\\epsilon e_{1})-f(x)}{\\epsilon}=\\left[\\begin{array}{c}\n1\\\\\n0\\\\\n1\n\\end{array}\\right]\\]\n\\[\\frac{f(x+\\epsilon e_{2})-f(x)}{\\epsilon}=\\left[\\begin{array}{c}\n0\\\\\nx_{3}\\\\\n0\n\\end{array}\\right]\\]\n\\[\\frac{f(x+\\epsilon e_{3})-f(x)}{\\epsilon}=\\left[\\begin{array}{c}\n1\\\\\nx_{2}\\\\\n0\n\\end{array}\\right]\\]\nBut notice that the calculation of \\(e_1\\) and \\(e_2\\) do not interact. If we had done:\n\\[\\frac{f(x+\\epsilon e_{1}+\\epsilon e_{2})-f(x)}{\\epsilon}=\\left[\\begin{array}{c}\n1\\\\\nx_{3}\\\\\n1\n\\end{array}\\right]\\]\nwe would still get the correct value for every row because the \\(\\epsilon\\) terms do not collide (a situation known as perturbation confusion). If we knew the sparsity pattern of the Jacobian included a 0 at (2,1), (1,2), and (3,2), then we would know that the vectors would have to be \\([1 0 1]\\) and \\([0 x_3 0]\\), meaning that columns 1 and 2 can be computed simultaneously and decompressed. This is the key to sparse differentiation.\n\nWith forward-mode automatic differentiation, recall that we calculate multiple dimensions simultaneously by using a multidimensional dual number seeded by the vectors of the differentiation directions, that is:\n\\[d = x + v_1 \\epsilon_1 + \\ldots + v_m \\epsilon_m\\]\nInstead of using the primitive differentiation directions \\(e_j\\), we can instead replace this with the mixed values. For example, the Jacobian of the example function can be computed in one function call to \\(f\\) with the dual number input:\n\\[d = x + (e_1 + e_2) \\epsilon_1 + e_3 \\epsilon_2\\]\nand performing the decompression via the sparsity pattern. Thus the sparsity pattern gives a direct way to optimize the construction of the Jacobian.\nThis idea of independent directions can be formalized as a matrix coloring. Take \\(S_{ij}\\) the sparsity pattern of some Jacobian matrix \\(J_{ij}\\). Define a graph on the nodes 1 through m where there is an edge between \\(i\\) and \\(j\\) if there is a row where \\(i\\) and \\(j\\) are non-zero. This graph is the column connectivity graph of the Jacobian. What we wish to do is find the smallest set of differentiation directions such that differentiating in the direction of \\(e_i\\) does not collide with differentiation in the direction of \\(e_j\\). The connectivity graph is setup so that way this cannot be done if the two nodes are adjacent. If we let the subset of nodes differentiated together be a color, the question is, what is the smallest number of colors s.t. no adjacent nodes are the same color. This is the classic distance-1 coloring problem from graph theory. It is well-known that the problem of finding the chromatic number, the minimal number of colors for a graph, is generally NP-complete. However, there are heuristic methods for performing a distance-1 coloring quite quickly. For example, a greedy algorithm is as follows:\n\nPick a node at random to be color 1.\nMake all nodes adjacent to that be the lowest color that they can be (in this step that will be 2).\nNow look at all nodes adjacent to that. Make all nodes be the lowest color that they can be (either 1 or 3).\nRepeat by looking at the next set of adjacent nodes and color as conservatively as possible.\n\nThis can be visualized as follows:\n\nThe result will color the entire connected component. While not giving an optimal result, it will still give a result that is a sufficient reduction in the number of differentiation directions (without solving an NP-complete problem) and thus can lead to a large computational saving.\nAt the end, let \\(c_i\\) be the vector of 1’s and 0’s, where it’s 1 for every node that is color \\(i\\) and 0 otherwise. Sparse automatic differentiation of the Jacobian is then computed with:\n\\[d = x + c_1 \\epsilon_1 + \\ldots + c_k \\epsilon_k\\]\nthat is, the full Jacobian is computed with one dual number which consists of the primal calculation along with \\(k\\) dual dimensions, where \\(k\\) is the computed chromatic number of the connectivity graph on the Jacobian. Once this calculation is complete, the colored columns can be decompressed into the full Jacobian using the sparsity information, generating the original quantity that we wanted to compute.\nFor more information on the graph coloring aspects, find the paper titled “What Color Is Your Jacobian? Graph Coloring for Computing Derivatives” by Gebremedhin.\n\n8.3.2.1 Note on Sparse Reverse-Mode AD\nReverse-mode automatic differentiation can be though of as a method for computing one row of a Jacobian per seed, as opposed to one column per seed given by forward-mode AD. Thus sparse reverse-mode automatic differentiation can be done by looking at the connectivity graph of the column and using the resulting color vectors to seed the reverse accumulation process."
  },
  {
    "objectID": "stiff_odes.html#linear-solving",
    "href": "stiff_odes.html#linear-solving",
    "title": "8  Solving Stiff Ordinary Differential Equations",
    "section": "8.4 Linear Solving",
    "text": "8.4 Linear Solving\nAfter the Jacobian has been computed, we need to solve a linear equation \\(Ja=b\\). While mathematically you can solve this by computing the inverse \\(J^{-1}\\), this is not a good way to perform the calculation because even if \\(J\\) is sparse, then \\(J^{-1}\\) is in general dense and thus may not fit into memory (remember, this is \\(N^2\\) as many terms, where \\(N\\) is the size of the ordinary differential equation that is being solved, so if it’s a large equation it is very feasible and common that the ODE is representable but its full Jacobian is not able to fit into RAM). Note that some may say that this is done for numerical stability reasons: that is incorrect. In fact, under reasonable assumptions for how the inverse is computed, it will be as numerically stable as other techniques we will mention.\nThus instead of generating the inverse, we can instead perform a matrix factorization. A matrix factorization is a transformation of the matrix into a form that is more amenable to certain analyses. For our purposes, a general Jacobian within a Newton iteration can be transformed via the LU-factorization or (LU-decomposition), i.e.\n\\[J = LU\\]\nwhere \\(L\\) is lower triangular and \\(U\\) is upper triangular. If we write the linear equation in this form:\n\\[LUa = b\\]\nthen we see that we can solve it by first solving \\(L(Ua) = b\\). Since \\(L\\) is lower triangular, this is done by the backsubstitution algorithm. That is, in a lower triangular form, we can solve for the first value since we have:\n\\[L_{11} a_1 = b_1\\]\nand thus by dividing we solve. For the next term, we have that\n\\[L_{21} a_1 + L_{22} a_2 = b_2\\]\nand thus we plug in the solution to \\(a_1\\) and solve to get \\(a_2\\). The lower triangular form allows this to continue. This occurs in 1+2+3+…+n operations, and is thus O(n^2). Next, we solve \\(Ua = b\\), which once again is done by a backsubstitution algorithm but in the reverse direction. Together those two operations are O(n^2) and complete the inversion of \\(LU\\).\nSo is this an O(n^2) algorithm for computing the solution of a linear system? No, because the computation of \\(LU\\) itself is an O(n^3) calculation, and thus the true complexity of solving a linear system is still O(n^3). However, if we have already factorized \\(J\\), then we can repeatedly use the same \\(LU\\) factors to solve additional linear problems \\(Jv = u\\) with different vectors. We can exploit this to accelerate the Newton method. Instead of doing the calculation:\n\\[x_{k+1} = x_k - J(x_k)^{-1}g(x_k)\\]\nwe can instead do:\n\\[x_{k+1} = x_k - J(x_0)^{-1}g(x_k)\\]\nso that all of the Jacobians are the same. This means that a single O(n^3) factorization can be done, with multiple O(n^2) calculations using the same factorization. This is known as a Quasi-Newton method. While this makes the Newton method no longer quadratically convergent, it minimizes the large constant factor on the computational cost while retaining the same dynamical properties, i.e. the same steady state and thus the same overall solution. This makes sense for sufficiently large \\(n\\), but requires sufficiently large \\(n\\) because the loss of quadratic convergence means that it will take more steps to converge than before, and thus more \\(O(n^2)\\) backsolves are required, meaning that the difference between factorizations and backsolves needs to be large enough in order to offset the cost of extra steps.\n\n8.4.0.1 Note on Sparse Factorization\nNote that LU-factorization, and other factorizations, have generalizations to sparse matrices where a symbolic factorization is utilized to compute a sparse storage of the values which then allow for a fast backsubstitution. More details are outside the scope of this course, but note that Julia and MATLAB will both use the library SuiteSparse in the background when lu is called on a sparse matrix."
  },
  {
    "objectID": "stiff_odes.html#jacobian-free-newton-krylov-jfnk",
    "href": "stiff_odes.html#jacobian-free-newton-krylov-jfnk",
    "title": "8  Solving Stiff Ordinary Differential Equations",
    "section": "8.5 Jacobian-Free Newton Krylov (JFNK)",
    "text": "8.5 Jacobian-Free Newton Krylov (JFNK)\nAn alternative method for solving the linear system is the Jacobian-Free Newton Krylov technique. This technique is broken into two pieces: the jvp calculation and the Krylov subspace iterative linear solver.\n\n8.5.1 Jacobian-Vector Products as Directional Derivatives\nWe don’t actually need to compute \\(J\\) itself, since all that we actually need is the v = J*w. Is it possible to compute the Jacobian-Vector Product, or the jvp, without producing the Jacobian?\nTo see how this is done let’s take a look at what is actually calculated. Written out in the standard basis, we have that:\n\\[w_i = \\sum_{j}^{m} J_{ij} v_{j}\\]\nNow write out what \\(J\\) means and we see that:\n\\[w_i = \\sum_j^{m} \\frac{df_i}{dx_j} v_j = \\nabla f_i(x) \\cdot v\\]\nthat is, the \\(i\\)th component of \\(Jv\\) is the directional derivative of \\(f_i\\) in the direction \\(v\\). This means that in general, the jvp \\(Jv\\) is actually just the directional derivative in the direction of \\(v\\), that is:\n\\(Jv = \\nabla f \\cdot v\\)\nand therefore it has another mathematical representation, that is:\n\\[Jv = \\lim_{\\epsilon \\rightarrow 0} \\frac{f(x+v \\epsilon) - f(x)}{\\epsilon}\\]\nFrom this alternative form it is clear that we can always compute a jvp with a single computation. Using finite differences, a simple approximation is the following:\n\\[Jv \\approx \\frac{f(x+v \\epsilon) - f(x)}{\\epsilon}\\]\nfor non-zero \\(\\epsilon\\). Similarly, recall that in forward-mode automatic differentiation we can choose directions by seeding the dual part. Therefore, using the dual number with one partial component:\n\\[d = x + v \\epsilon\\]\nwe get that\n\\[f(d) = f(x) + Jv \\epsilon\\]\nand thus a single application with a single partial gives the jvp.\n\n8.5.1.1 Note on Reverse-Mode Automatic Differentiation\nAs noted earlier, reverse-mode automatic differentiation has its primitives compute rows of the Jacobian in the seeded direction. This means that the seeded reverse-mode call with the vector \\(v\\) computes \\(v^T J\\), that is the vector (transpose) Jacobian transpose, or vjp for short. When discussing parameter estimation and adjoints, this shorthand will be introduced as a way for using a traditionally machine learning tool to accelerate traditionally scientific computing tasks.\n\n\n\n8.5.2 Krylov Subspace Methods For Solving Linear Systems\n\n8.5.2.1 Basic Iterative Solver Methods\nNow that we have direct access to quick calculations of \\(Jv\\), how would we use this to solve the linear system \\(Jw = v\\) quickly? This is done through iterative linear solvers. These methods replace the process of solving for a factorization with, you may have guessed it, a discrete dynamical system whose solution is \\(w\\). To do this, what we want is some iterative process so that\n\\[Jw - b = 0\\]\nSo now let’s split \\(J = A - B\\), then if we are iterating the vectors \\(w_k\\) such that \\(w_k \\rightarrow w\\), then if we plug this into the previous (residual) equation we get\n\\[A w_{k+1} = Bw_k + b\\]\nsince when we plug in \\(w\\) we get zero (the sequence must be Cauchy so the difference \\(w_{k+1} - w_k \\rightarrow 0\\)). Thus if we can split our matrix \\(J\\) into a component \\(A\\) which is easy to invert and a part \\(B\\) that is just everything else, then we would have a bunch of easy linear systems to solve. There are many different choices that we can do. If we let \\(J = L + D + U\\), where \\(L\\) is the lower portion of \\(J\\), \\(D\\) is the diagonal, and \\(U\\) is the upper portion, then the following are well-known methods:\n\nRichardson: \\(A = \\omega I\\) for some \\(\\omega\\)\nJacobi: \\(A = D\\)\nDamped Jacobi: \\(A = \\omega D\\)\nGauss-Seidel: \\(A = D-L\\)\nSuccessive Over Relaxation: \\(A = \\omega D - L\\)\nSymmetric Successive Over Relaxation: \\(A = \\frac{1}{\\omega (2 - \\omega)}(D-\\omega L)D^{-1}(D-\\omega U)\\)\n\nThese decompositions are chosen since a diagonal matrix is easy to invert (it’s just the inversion of the scalars of the diagonal) and it’s easy to solve an upper or lower triangular linear system (once again, it’s backsubstitution).\nSince these methods give a a linear dynamical system, we know that there is a unique steady state solution, which happens to be \\(Aw - Bw = Jw = b\\). Thus we will converge to it as long as the steady state is stable. To see if it’s stable, take the update equation\n\\[w_{k+1} = A^{-1}(Bw_k + b)\\]\nand check the eigenvalues of the system: if they are within the unit circle then you have stability. Notice that this can always occur by bringing the eigenvalues of \\(A^{-1}\\) closer to zero, which can be done by multiplying \\(A\\) by a significantly large value, hence the \\(\\omega\\) quantities. While that always works, this essentially amounts to decreasing the stepsize of the iterative process and thus requiring more steps, thus making it take more computations. Thus the game is to pick the largest stepsize (\\(\\omega\\)) for which the steady state is stable. We will leave that as outside the topic of this course.\n\n\n8.5.2.2 Krylov Subspace Methods\nWhile the classical iterative solver methods give the background for understanding an alternative to direct inversion or factorization of a matrix, the problem with that approach is that it requires the ability to split the matrix \\(J\\), which we would like to avoid computing. Instead, we would like to develop an iterative solver technique which instead just uses the solution to \\(Jv\\). Indeed there are such methods, and these are the Krylov subspace methods. A Krylov subspace is the space spanned by:\n\\[\\mathcal{K}_k = \\text{span} \\{v,Jv,J^2 v, \\ldots, J^k v\\}\\]\nThere are a few nice properties about Krylov subspaces that can be exploited. For one, it is known that there is a finite maximum dimension of the Krylov subspace, that is there is a value \\(r\\) such that \\(J^{r+1} v \\in \\mathcal{K}_r\\), which means that the complete Krylov subspace can be computed in finitely many jvp, since \\(J^2 v\\) is just the jvp where the vector is the jvp. Indeed, one can show that \\(J^i v\\) is linearly independent for each \\(i\\), and thus that maximal value is \\(m\\), the dimension of the Jacobian. Therefore in \\(m\\) jvps the solution is guaranteed to live in the Krylov subspace, giving a maximal computational cost and a proof of convergence if the vector in there is the “optimal in the space”.\nThe most common method in the Krylov subspace family of methods is the GMRES method. Essentially, in step \\(i\\) one computes \\(\\mathcal{K}_i\\), and finds the \\(x\\) that is the closest to the Krylov subspace, i.e. finds the \\(x \\in \\mathcal{K}_i\\) such that \\(\\Vert Jx-v \\Vert\\) is minimized. At each step, it adds the new vector to the Krylov subspace after orthgonalizing it against the other vectors via Arnoldi iterations, leading to an orthogonal basis of \\(\\mathcal{K}_i\\) which makes it easy to express \\(x\\).\nWhile one has a guaranteed bound on the number of possible jvps in GMRES which is simply the number of ODEs (since that is what determines the size of the Jacobian and thus the total dimension of the problem), that bound is not necessarily a good one. For a large sparse matrix, it may be computationally impractical to ever compute 100,000 jvps. Thus one does not typically run the algorithm to conclusion, and instead stops when \\(\\Vert Jx-v \\Vert\\) is sufficiently below some user-defined error tolerance."
  },
  {
    "objectID": "stiff_odes.html#intermediate-conclusion",
    "href": "stiff_odes.html#intermediate-conclusion",
    "title": "8  Solving Stiff Ordinary Differential Equations",
    "section": "8.6 Intermediate Conclusion",
    "text": "8.6 Intermediate Conclusion\nLet’s take a step back and see what our intermediate conclusion is. In order to solve for the implicit step, it just boils down to doing Newton’s method on some \\(g(x)=0\\). If the Jacobian is small enough, one factorizes the Jacobian and uses Quasi-Newton iterations in order to utilize the stored LU-decomposition in multiple steps to reduce the computation cost. If the Jacobian is sparse, sparse automatic differentiation through matrix coloring is employed to directly fill the sparse matrix with less applications of \\(g\\), and then this sparse matrix is factorized using a sparse LU factorization.\nWhen the matrix is too large, then one resorts to using a Krylov subspace method, since this only requires being able to do \\(Jv\\) calculations. In general, \\(Jv\\) can be done matrix-free because it is simply the directional derivative in the direction of the vector \\(v\\), which can be computed through either numerical or forward-mode automatic differentiation. This is then used in the GMRES iterative process to find the solution in the Krylov subspace which is closest to the solution, exiting early when the residual error is small enough. If this is converging too slow, then preconditioning is used.\nThat’s the basic algorithm, but what are the other important details for getting this right?"
  },
  {
    "objectID": "stiff_odes.html#the-need-for-speed",
    "href": "stiff_odes.html#the-need-for-speed",
    "title": "8  Solving Stiff Ordinary Differential Equations",
    "section": "8.7 The Need for Speed",
    "text": "8.7 The Need for Speed\n\n8.7.1 Preconditioning\nHowever, the speed at GMRES convergences is dependent on the correlations between the vectors, which can be shown to be related to the condition number of the Jacobian matrix. A high condition number makes convergence slower (this is the case for the traditional iterative methods as well), which in turn is an issue because it is the high condition number on the Jacobian which leads to stiffness and causes one to have to use an implicit integrator in the first place!\nTo help speed up the convergence, a common technique is known as preconditioning. Preconditioning is the process of using a semi-inverse to the matrix in order to split the matrix so that the iterative problem that is being solved is one that has a smaller condition number. Mathematically, it involves decomposing \\(J = P_l A P_r\\) where \\(P_l\\) and \\(P_r\\) are the left and right preconditioners which have simple inverses, and thus instead of solving \\(Jx=v\\), we would solve:\n\\[P_l A P_r x = v\\]\nor\n\\[A P_r x = P_l^{-1}v\\]\nwhich then means that the Krylov subpace that needs to be solved for is that defined by \\(A\\): \\(\\mathcal{K} = \\text{span}\\{v,Av,A^2 v, \\ldots\\}\\). There are many possible choices for these preconditioners, but they are usually problem dependent. For example, for ODEs which come from parabolic and elliptic PDE discretizations, the multigrid method, such as a geometric multigrid or an algebraic multigrid, is a preconditioner that can accelerate the iterative solving process. One generic preconditioner that can generally be used is to divide by the norm of the vector \\(v\\), which is a scaling employed by both SUNDIALS CVODE and by DifferentialEquations.jl and can be shown to be almost always advantageous.\n\n\n8.7.2 Jacobian Re-use\nIf the problem is small enough such that the factorization is used and a Quasi-Newton technique is employed, it then holds that for most steps \\(J\\) is only approximate since it can be using an old LU-factorization. To push it even further, high performance codes allow for jacobian reuse, which is allowing the same Jacobian to be reused between different timesteps. If the Jacobian is too incorrect, it can cause the Newton iterations to diverge, which is then when one would calculate a new Jacobian and compute a new LU-factorization.\n\n\n8.7.3 Adaptive Timestepping\nIn simple cases, like partial differential equation discretizations of physical problems, the resulting ODEs are not too stiff and thus Newton’s iteration generally works. However, in cases like stiff biological models, Newton’s iteration can itself not always be stable enough to allow convergence. In fact, with many of the stiff biological models commonly used in benchmarks, no method is stable enough to pass without using adaptive timestepping! Thus one may need to adapt the timestep in order to improve the ability for the Newton method to converge (smaller timesteps increase the stability of the Newton stepping, see the homework).\nThis needs to be mixed with the Jacobian re-use strategy, since \\(J = I - \\gamma \\frac{df}{du}\\) where \\(\\gamma\\) is dependent on \\(\\Delta t\\) (and \\(\\gamma = \\Delta t\\) for implicit Euler) means that the Jacobian of the Newton method changes as \\(\\Delta t\\) changes. Thus one usually has a tiered algorithm for determining when to update the factorizations of \\(J\\) vs when to compute a new \\(\\frac{df}{du}\\) and then refactorize. This is generally dependent on estimates of convergence rates to heuristically guess how far off \\(\\frac{df}{du}\\) is from the current true value.\nSo how does one perform adaptivity? This is generally done through a rejection sampling technique. First one needs some estimate of the error in a step. This is calculated through an embedded method, which is a method that is able to be calculated without any extra \\(f\\) evaluations that is (usually) one order different from the true method. The difference between the true and the embedded method is then an error estimate. If this is greater than a user chosen tolerance, the step is rejected and re-ran with a smaller \\(\\Delta t\\) (possibly refactorizing, etc.). If this is less than the user tolerance, the step is accepted and \\(\\Delta t\\) is changed.\nThere are many schemes for how one can change \\(\\Delta t\\). One of the most common is known as the P-control, which stands for the proportional controller which is used throughout control theory. In this case, the control is to change \\(\\Delta t\\) in proportion to the current error ratio from the desired tolerance. If we let\n\\[q = \\frac{\\text{E}}{\\max(u_k,u_{k+1}) \\tau_r + \\tau_a}\\]\nwhere \\(\\tau_r\\) is the relative tolerance and \\(\\tau_a\\) is the absolute tolerance, then \\(q\\) is the ratio of the current error to the current tolerance. If \\(q&lt;1\\), then the error is less than the tolerance and the step is accepted, and vice versa for \\(q&gt;1\\). In either case, we let \\(\\Delta t_{new} = q \\Delta t\\) be the proportional update.\nHowever, proportional error control has many known features that are undesirable. For example, it happens to work in a “bang bang” manner, meaning that it can drastically change its behavior from step to step. One step may multiply the step size by 10x, then the next by 2x. This is an issue because it effects the stability of the ODE solver method (since the stability is not a property of a single step, but rather it’s a property of the global behavior over time)! Thus to smooth it out, one can use a PI-control, which modifies the control factor by a history value, i.e. the error in one step in the past. This of course also means that one can utilize a PID-controller for time stepping. And there are many other techniques that can be used, but many of the most optimized codes tend to use a PI-control mechanism."
  },
  {
    "objectID": "stiff_odes.html#methodological-summary",
    "href": "stiff_odes.html#methodological-summary",
    "title": "8  Solving Stiff Ordinary Differential Equations",
    "section": "8.8 Methodological Summary",
    "text": "8.8 Methodological Summary\nHere’s a quick summary of the methodologies in a hierarchical sense:\n\nAt the lowest level is the linear solve, either done by JFNK or (sparse) factorization. For large enough systems, this is the brunt of the work. This is thus the piece to computationally optimize as much as possible, and parallelize. For sparse factorizations, this can be done with a distributed sparse library implementation. For JFNK, the efficiency is simply due to the efficiency of your ODE function f.\nAn optional level for JFNK is the preconditioning level, where preconditioners can be used to decrease the total number of iterations required for Krylov subspace methods like GMRES to converge, and thus reduce the total number of f calls.\nAt the nonlinear solver level, different Newton-like techniques are utilized to minimize the number of factorizations/linear solves required, and maximize the stability of the Newton method.\nAt the ODE solver level, more efficient integrators and adaptive methods for stiff ODEs are used to reduce the cost by affecting the linear solves. Most of these calculations are dominated by the linear solve portion when it’s in the regime of large stiff systems. Jacobian reuse techniques, partial factorizations, and IMEX methods come into play as ways to reduce the cost per factorization and reduce the total number of factorizations."
  },
  {
    "objectID": "estimation_identification.html#youtube-video-link",
    "href": "estimation_identification.html#youtube-video-link",
    "title": "9  Basic Parameter Estimation, Reverse-Mode AD, and Inverse Problems",
    "section": "9.1 Youtube Video Link",
    "text": "9.1 Youtube Video Link\nHave a model. Have data. Fit model to data.\nThis is a problem that goes under many different names: parameter estimation, inverse problems, training, etc. In this lecture we will go through the methods for how that’s done, starting with the basics and bringing in the recent techniques from machine learning that can be used to improve the basic implementations."
  },
  {
    "objectID": "estimation_identification.html#the-shooting-method-for-parameter-fitting",
    "href": "estimation_identification.html#the-shooting-method-for-parameter-fitting",
    "title": "9  Basic Parameter Estimation, Reverse-Mode AD, and Inverse Problems",
    "section": "9.2 The Shooting Method for Parameter Fitting",
    "text": "9.2 The Shooting Method for Parameter Fitting\nAssume that we have some model \\(u = f(p)\\) where \\(p\\) is our parameters, where we put in some parameters and receive our simulated data \\(u\\). How should you choose \\(p\\) such that \\(u\\) best fits that data? The shooting method directly uses this high level definition of the model by putting a cost function on the output \\(C(p)\\). This cost function is dependent on a user-choice and it’s model-dependent. However, a common one is the L2-loss. If \\(y\\) is our expected data, then the L2-loss function against the data is simply:\n\\[C(p) = \\Vert f(p) - y \\Vert\\]\nwhere \\(C(p): \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) is a function that returns a scalar. The shooting method then directly optimizes this cost function by having the optimizer generate a data given new choices of \\(p\\).\n\n9.2.1 Methods for Optimization\nThere are many different nonlinear optimization methods which can be used for this purpose, and for a full survey one should look at packages like JuMP, Optim.jl, and NLopt.jl.\nThere are generally two sets of methods: global and local optimization methods. Local optimization methods attempt to find the best nearby extrema by finding a point where the gradient \\(\\frac{dC}{dp} = 0\\). Global optimization methods attempt to explore the whole space and find the best of the extrema. Global methods tend to employ a lot more heuristics and are extremely computationally difficult, and thus many studies focus on local optimization. We will focus strictly on local optimization, but one may want to look into global optimization for many applications of parameter estimation.\nMost local optimizers make use of derivative information in order to accelerate the solver. The simplest of which is the method of gradient descent. In this method, given a set of parameters \\(p_i\\), the next step of parameters one will try is:\n\\[p_{i+1} = p_i - \\alpha \\frac{dC}{dP}\\]\nthat is, update \\(p_i\\) by walking in the downward direction of the gradient. Instead of using just first order information, one may want to directly solve the rootfinding problem \\(\\frac{dC}{dp} = 0\\) using Newton’s method. Newton’s method in this case looks like:\n\\[p_{i+1} = p_i - (\\frac{d}{dp}\\frac{dC}{dp})^{-1} \\frac{dC}{dp}\\]\nBut notice that the Jacobian of the gradient is the Hessian, and thus we can rewrite this as:\n\\[p_{i+1} = p_i - H(p_i)^{-1} \\frac{dC(p_i)}{dp}\\]\nwhere \\(H(p)\\) is the Hessian matrix \\(H_{ij} = \\frac{dC}{dx_i dx_j}\\). However, solving a system of equations which involves the Hessian can be difficult (just like the Jacobian, but now with another layer of differentiation!), and thus many optimization techniques attempt to avoid the Hessian. A commonly used technique that is somewhat in the middle is the BFGS technique, which is a gradient-based optimization method that attempts to approximate the Hessian along the way to modify its stepping behavior. It uses the history of previously calculated points in order to build this quick Hessian approximate. If one keeps only a constant length history, say 5 time points, then one arrives at the l-BFGS technique, which is one of the most common large-scale optimization techniques.\n\n\n9.2.2 Connection Between Optimization and Differential Equations\nThere is actually a strong connection between optimization and differential equations. Let’s say we wanted to follow the gradient of the solution towards a local minimum. That would mean that the flow that we would wish to follow is given by an ODE, specifically the ODE:\n\\[p' = -\\frac{dC}{dp}\\]\nIf we apply the Euler method to this ODE, then we receive\n\\[p_{n+1} = p_n - \\alpha \\frac{dC(p_n)}{dp}\\]\nand we thus recover the gradient descent method. Now assume that you want to use implicit Euler. Then we would have the system\n\\[p_{n+1} = p_n - \\alpha \\frac{dC(p_{n+1})}{dp}\\]\nwhich we would then move to one side:\n\\[p_{n+1} - p_n + \\alpha \\frac{dC(p_{n+1})}{dp} = 0\\]\nand solve each step via a Newton method. For this Newton method, we need to take the Jacobian of this gradient function, and once again the Hessian arrives as the fundamental quantity.\n\n\n9.2.3 Neural Network Training as a Shooting Method for Functions\nA one layer dense neuron is traditionally written as the function:\n\\[layer(x) = \\sigma.(Wx + b)\\]\nwhere \\(x \\in \\mathbb{R}^n\\), \\(W \\in \\mathbb{R}^{m \\times n}\\), \\(b \\in \\mathbb{R}^{m}\\) and \\(\\sigma\\) is some choice of \\(\\mathbb{R}\\rightarrow\\mathbb{R}\\) nonlinear function, where the . is the Julia dot to signify element-wise operation.\nA traditional neural network, feed-forward network, or multi-layer perceptron is a 3 layer function, i.e.\n\\[NN(x) = W_3 \\sigma_2.(W_2\\sigma_1.(W_1x + b_1) + b_2) + b_3\\]\nwhere the first layer is called the input layer, the second is called the hidden layer, and the final is called the output layer. This specific function was seen as desirable because of the Universal Approximation Theorem, which is formally stated as follows:\nLet \\(\\sigma\\) be a nonconstant, bounded, and continuous function. Let \\(I_m = [0,1]^m\\). The space of real-valued continuous functions on \\(I_m\\) is denoted by \\(C(I_m)\\). For any \\(\\epsilon &gt;0\\) and any \\(f\\in C(I_m)\\), there exists an integer \\(N\\), real constants \\(W_i\\) and \\(b_i\\) s.t.\n\\[\\Vert NN(x) - f(x) \\Vert &lt; \\epsilon\\]\nfor all \\(x \\in I_m\\). Equivalently, \\(NN\\) given parameters is dense in \\(C(I_m)\\).\nHowever, it turns out that using only one hidden layer can require exponential growth in the size of said hidden layer, where the size is given by the number of columns in \\(W_1\\). To counteract this, deep neural networks were developed to be in the form of the recurrence relation:\n\\[v_{i+1} = \\sigma_i.(W_i v_{i} + b_i)\\] \\[v_1 = x\\] \\[DNN(x) = v_{n}\\]\nfor some \\(n\\) where \\(n\\) is the number of layers. Given a sufficient size of the hidden layers, this kind of function is a universal approximator (2017). Although it’s not quite known yet, some results have shown that this kind of function is able to fit high dimensional functions without the curse of dimensionality, i.e. the number of parameters does not grow exponentially with the input size. More mathematical results in this direction are still being investigated.\nHowever, this theory gives a direct way to transform the fitting of an arbitrary function into a parameter shooting problem. Given an unknown function \\(f\\) one wishes to fit, one can place the cost function\n\\[C(p) = \\Vert DNN(x;p) - f(x) \\Vert\\]\nwhere \\(DNN(x;p)\\) signifies the deep neural network given by the parameters \\(p\\), where the full set of parameters is the \\(W_i\\) and \\(b_i\\). To make the evaluation of that function be practical, we can instead say we wish to evaluate the difference at finitely many points:\n\\[C(p) = \\sum_k^N \\Vert DNN(x_k;p) - f(x_k) \\Vert\\]\nTraining a neural network is machine learning speak for finding the \\(p\\) which minimizes this cost function. Notice that this is then a shooting method problem, where a cost function is defined by direct evaluations of the model with some choice of parameters.\n\n\n9.2.4 Recurrent Neural Networks\nRecurrent neural networks are networks which are given by the recurrence relation:\n\\[x_{k+1} = x_k + DNN(x_k,k;p)\\]\nGiven our machinery, we can see this is equivalent to the Euler discretization with \\(\\Delta t = 1\\) on the neural ordinary differential equation defined by:\n\\[x' = DNN(x,t;p)\\]\nThus a recurrent neural network is a sequence of applications of a neural network (or possibly a neural network indexed by integer time)."
  },
  {
    "objectID": "estimation_identification.html#computing-gradients",
    "href": "estimation_identification.html#computing-gradients",
    "title": "9  Basic Parameter Estimation, Reverse-Mode AD, and Inverse Problems",
    "section": "9.3 Computing Gradients",
    "text": "9.3 Computing Gradients\nThis shows that many different problems, from training neural networks to fitting differential equations, all have the same underlying mathematical structure which requires the ability to compute the gradient of a cost function given model evaluations. However, this simply reduces to computing the gradient of the model’s output given the parameters. To see this, let’s take for example the L2 loss function, i.e.\n\\[C(p) = \\sum_i^N \\Vert f(x_i;p) - y_i \\Vert\\]\nfor some finite data points \\(y_i\\). In the ODE model, \\(y_i\\) are time series points. In the general neural network, \\(y_i = d(x_i)\\) for the function we wish to fit \\(d\\). In data science applications of machine learning, \\(y_i = d_i\\) the discrete data points we wish to fit. In any of these cases, we see that by the chain rule we have\n\\[\\frac{dC}{dp} = \\sum_i^N 2 \\left(f(x_i;p) - y_i \\right) \\frac{df(x_i)}{dp}\\]\nand therefore, knowing how to efficiently compute \\[\\frac{df(x_i)}{dp}\\] is the essential question for shooting-based parameter fitting.\n\n9.3.1 Forward-Mode Automatic Differentiation for Gradients\nLet’s recall the forward-mode method for computing gradients. For an arbitrary nonlinear function \\(f\\) with scalar output, we can compute derivatives by putting a dual number in. For example, with\n\\[d = d_0 + v_1 \\epsilon_1 + \\ldots + v_m \\epsilon_m\\]\nwe have that\n\\[f(d) = f(d_0) + f'(d_0)v_1 \\epsilon_1 + \\ldots + f'(d_0)v_m \\epsilon_m\\]\nwhere \\(f'(d_0)v_i\\) is the direction derivative in the direction of \\(v_i\\). To compute the gradient with respond to the input, we thus need to make \\(v_i = e_i\\).\nHowever, in this case we now do not want to compute the derivative with respect to the input! Instead, now we have \\(f(x;p)\\) and want to compute the derivatives with respect to \\(p\\). This simply means that we want to take derivatives in the directions of the parameters. To do this, let:\n\\[x = x_0 + 0 \\epsilon_1 + \\ldots + 0 \\epsilon_k\\] \\[P = p + e_1 \\epsilon_1 + \\ldots + e_k \\epsilon_k\\]\nwhere there are \\(k\\) parameters. We then have that\n\\[f(x;P) = f(x;p) + \\frac{df}{dp_1} \\epsilon_1 + \\ldots + \\frac{df}{dp_k} \\epsilon_k\\]\nas the output, and thus a \\(k+1\\)-dimensional number computes the gradient of the function with respect to \\(k\\) parameters.\nCan we do better?\n\n\n9.3.2 The Adjoint Technique and Reverse Accumulation\nThe fast method for computing gradients goes under many times. The adjoint technique, backpropagation, and reverse-mode automatic differentiation are in some sense all equivalent phrases given to this method from different disciplines. To understand the adjoint technique, we will look at the multivariate chain rule on a computation graph. Recall that for \\(f(x(t),y(t))\\) that we have:\n\\[\\frac{df}{dt} = \\frac{df}{dx}\\frac{dx}{dt} + \\frac{df}{dy}\\frac{dy}{dt}\\]\nWe can visualize our direct dependences as the computation graph:\n\ni.e. \\(t\\) directly determines \\(x\\) and \\(y\\) which then determines \\(f\\). To calculate Assume you’ve already evaluated \\(f(t)\\). If this has been done, then you’ve already had to calculate \\(x\\) and \\(y\\). Thus given the function \\(f\\), we can now calculate \\(\\frac{df}{dx}\\) and \\(\\frac{df}{dy}\\), and then calculate \\(\\frac{dx}{dt}\\) and \\(\\frac{dy}{dt}\\).\nNow let’s put another layer in the computation. Let’s make \\(f(x(v(t),w(t)),y(v(t),w(t))\\). We can write out the full expression for the derivative. Notice that even with this additional layer, the statement we wrote above still holds:\n\\[\\frac{df}{dt} = \\frac{df}{dx}\\frac{dx}{dt} + \\frac{df}{dy}\\frac{dy}{dt}\\]\nSo given an evaluation of \\(f\\), we can (still) directly calculate \\(\\frac{df}{dx}\\) and \\(\\frac{df}{dy}\\). But now, to calculate \\(\\frac{dx}{dt}\\) and \\(\\frac{dy}{dt}\\), we do the next step of the chain rule:\n\\[\\frac{dx}{dt} = \\frac{dx}{dv}\\frac{dv}{dt} + \\frac{dx}{dw}\\frac{dw}{dt}\\]\nand similar for \\(y\\). So plug it all in, and you see that our equations will grow wild if we actually try to plug it in! But it’s clear that, to calculate \\[\\frac{df}{dt}\\], we can first calculate \\(\\frac{df}{dx}\\), and then multiply that to \\(\\frac{dx}{dt}\\). If we had more layers, we could calculate the sensitivity (the derivative) of the output to the last layer, then and then the sensitivity to the second layer back is the sensitivity of the last layer multiplied to that, and the third layer back has the sensitivity of the second layer multiplied to it!\n\n\n9.3.3 Logistic Regression Example\nTo better see this structure, let’s write out a simple example. Let our forward pass through our function be:\n\\[\\begin{align}\nz &= wx + b\\\\\ny &= \\sigma(z)\\\\\n\\mathcal{L} &= \\frac{1}{2}(y-t)^2\\\\\n\\mathcal{R} &= \\frac{1}{2}w^2\\\\\n\\mathcal{L}_{reg} &= \\mathcal{L} + \\lambda \\mathcal{R}\\end{align}\\]\n\nThe formulation of the program here is called a Wengert list, tape, or graph. In this, \\(x\\) and \\(t\\) are inputs, \\(b\\) and \\(W\\) are parameters, \\(z\\), \\(y\\), \\(\\mathcal{L}\\), and \\(\\mathcal{R}\\) are intermediates, and \\(\\mathcal{L}_{reg}\\) is our output.\nThis is a simple univariate logistic regression model. To do logistic regression, we wish to find the parameters \\(w\\) and \\(b\\) which minimize the distance of \\(\\mathcal{L}_{reg}\\) from a desired output, which is done by computing derivatives.\nLet’s calculate the derivatives with respect to each quantity in reverse order. If our program is \\(f(x) = \\mathcal{L}_{reg}\\), then we have that\n\\[\\frac{df}{d\\mathcal{L}_{reg}} = 1\\]\nas the derivatives of the last layer. To computerize our notation, let’s write\n\\[\\overline{\\mathcal{L}_{reg}} = \\frac{df}{d\\mathcal{L}_{reg}}\\]\nfor our computed values. For the derivatives of the second to last layer, we have that:\n\\[\\begin{align}\n  \\overline{\\mathcal{R}} &= \\frac{df}{d\\mathcal{L}_{reg}} \\frac{d\\mathcal{L}_{reg}}{d\\mathcal{R}}\\\\\n                         &= \\overline{\\mathcal{L}_{reg}} \\lambda \\end{align}\\]\n\\[\\begin{align}\n\\overline{\\mathcal{L}} &= \\frac{df}{d\\mathcal{L}_{reg}} \\frac{d\\mathcal{L}_{reg}}{d\\mathcal{L}}\\\\\n                        &= \\overline{\\mathcal{L}_{reg}} \\end{align}\\]\nThis was our observation from before that the derivative of the second layer is the partial derivative of the current values times the sensitivity of the final layer. And then we keep multiplying, so now for our next layer we have that:\n\\[\\begin{align}\n  \\overline{y} &= \\overline{\\mathcal{L}} \\frac{d\\mathcal{L}}{dy}\\\\\n               &= \\overline{\\mathcal{L}} (y-t) \\end{align}\\]\nAnd notice that the chain rule holds since \\(\\overline{\\mathcal{L}}\\) implicitly already has the multiplication by \\(\\overline{\\mathcal{L}_{reg}}\\) inside of it. Then the next layer is:\n\\[\\begin{align}\n\\frac{df}{z} &= \\overline{y} \\frac{dy}{dz}\\\\\n              &= \\overline{y} \\sigma^\\prime(z) \\end{align}\\]\nThen the next layer. Notice that here, by the chain rule on \\(w\\) we have that:\n\\[\\begin{align}\n  \\overline{w} &= \\overline{z} \\frac{\\partial z}{\\partial w} + \\overline{\\mathcal{R}} \\frac{d \\mathcal{R}}{dw}\\\\\n               &= \\overline{z} x + \\overline{\\mathcal{R}} w\\end{align}\\]\n\\[\\begin{align}\n\\overline{b} &= \\overline{z} \\frac{\\partial z}{\\partial b}\\\\\n              &= \\overline{z} \\end{align}\\]\nThis completely calculates all derivatives. In conclusion, the rule is:\n\nYou sum terms from each outward arrow\nEach arrow has the derivative term of the end times the partial of the current term.\nRecurse backwards to build simple linear combination expressions.\n\nYou can thus think of the relations as a message passing relation in reverse to the forward pass:\n\nNote that the reverse-pass has the values of the forward pass, like \\(x\\) and \\(t\\), embedded within it.\n\n\n9.3.4 Backpropagation of a Neural Network\nNow let’s look at backpropagation of a deep neural network. Before getting to it in the linear algebraic sense, let’s write everything in terms of scalars. This means we can write a simple neural network as:\n\\[\\begin{align}\n  z_i &= \\sum_j W_{ij}^1 x_j + b_i^1\\\\\n  h_i &= \\sigma(z_i)\\\\\n  y_i &= \\sum_j W_{ij}^2 h_j + b_i^2\\\\\n  \\mathcal{L} &= \\frac{1}{2} \\sum_k \\left(y_k - t_k \\right)^2 \\end{align}\\]\nwhere I have chosen the L2 loss function. This is visualized by the computational graph:\n\nThen we can do the same process as before to get:\n\\[\\begin{align}\n  \\overline{\\mathcal{L}} &= 1\\\\\n  \\overline{y_i} &= \\overline{\\mathcal{L}} (y_i - t_i)\\\\\n  \\overline{w_{ij}^2} &= \\overline{y_i} h_j\\\\\n  \\overline{b_i^2} &= \\overline{y_i}\\\\\n  \\overline{h_i} &= \\sum_k (\\overline{y_k}w_{ki}^2)\\\\\n  \\overline{z_i} &= \\overline{h_i}\\sigma^\\prime(z_i)\\\\\n  \\overline{w_{ij}^1} &= \\overline{z_i} x_j\\\\\n  \\overline{b_i^1} &= \\overline{z_i}\\end{align}\\]\njust by examining the computation graph. Now let’s write this in linear algebraic form.\n\nThe forward pass for this simple neural network was:\n\\[\\begin{align}\n  z &= W_1 x + b_1\\\\\n  h &= \\sigma(z)\\\\\n  y &= W_2 h + b_2\\\\\n  \\mathcal{L} = \\frac{1}{2} \\Vert y-t \\Vert^2 \\end{align}\\]\nIf we carefully decode our scalar expression, we see that we get the following:\n\\[\\begin{align}\n  \\overline{\\mathcal{L}} &= 1\\\\\n  \\overline{y} &= \\overline{\\mathcal{L}}(y-t)\\\\\n  \\overline{W_2} &= \\overline{y}h^{T}\\\\\n  \\overline{b_2} &= \\overline{y}\\\\\n  \\overline{h} &= W_2^T \\overline{y}\\\\\n  \\overline{z} &= \\overline{h} .* \\sigma^\\prime(z)\\\\\n  \\overline{W_1} &= \\overline{z} x^T\\\\\n  \\overline{b_1} &= \\overline{z} \\end{align}\\]\nWe can thus decode the rules as:\n\nMultiplying by the matrix going forwards means multiplying by the transpose going backwards. A term on the left stays on the left, and a term on the right stays on the right.\nElement-wise operations give element-wise multiplication\n\nNotice that the summation is then easily encoded into this rule by the transpose operation.\nWe can write it in the general DNN form of:\n\\[r_i = W_i v_{i} + b_i\\] \\[v_{i+1} = \\sigma_i.(r_i)\\] \\[v_1 = x\\] \\[\\mathcal{L} = \\frac{1}{2} \\Vert v_{n} - t \\Vert\\]\n\\[\\begin{align}\n  \\overline{\\mathcal{L}} &= 1\\\\\n  \\overline{v_n} &= \\overline{\\mathcal{L}}(y-t)\\\\\n  \\overline{r_i} &= \\overline{v_i} .* \\sigma_i^\\prime (r_i)\\\\\n  \\overline{W_i} &= \\overline{v_i}r_{i-1}^{T}\\\\\n  \\overline{b_i} &= \\overline{v_i}\\\\\n  \\overline{v_{i-1}} &= W_{i}^{T} \\overline{v_i} \\end{align}\\]\n\n\n9.3.5 Reverse-Mode Automatic Differentiation and vjps\nBackpropagation of a neural network is thus a different way of accumulating derivatives. If \\(f\\) is a composition of \\(L\\) functions:\n\\[f = f^L \\circ f^{L-1} \\circ \\ldots \\circ f^1\\]\nThen the Jacobian matrix satisfies:\n\\[J = J_L J_{L-1} \\ldots J_1\\]\nA program is essentially a nice way of writing a function in composition form. Forward-mode automatic differentiation worked by propagating forward the actions of the Jacobians at every step of the program:\n\\[Jv = J_L (J_{L-1} (\\ldots (J_1 v) \\ldots ))\\]\neffectively calculating the Jacobian of the program by multiplying by the Jacobians from left to right at each step of the way. This means doing primitive \\(Jv\\) calculations on each underlying problem, and pushing that calculation through.\nBut what about reverse accumulation? This can be isolated to the simple expression graph:\n\nIn backpropagation, we just showed that when doing reverse accumulation, the rule is that multiplication forwards is multiplication by the transpose backwards. So if the forward way to compute the Jacobian in reverse is to replace the matrix by its transpose:\n\nWe can either look at it as \\(J^T v\\), or by transposing the equation \\(v^T J\\). It’s right there that we have a vector-transpose Jacobian product, or a vjp.\nWe can thus think of this as a different direction for the Jacobian accumulation. Reverse-mode automatic differentiation moves backwards through our composed Jacobian. For a value \\(v\\) at the end, we can push it backwards:\n\\[v^T J = (\\ldots ((v^T J_L) J_{L-1}) \\ldots ) J_1\\]\ndoing a vjp at every step of the way, which is simply doing reverse-mode AD of that function (and if it’s linear, then simply doing the matrix multiplication). Thus reverse-mode AD is just a grouping of vjps into a single larger expression, instead of linearizing every single step.\n\n\n9.3.6 Primitives of Reverse Mode\nFor forward-mode AD, we saw that we could define primitives in order to accelerate the calculation. For example, knowing that\n\\[exp(x+\\epsilon) = exp(x) + exp(x)\\epsilon\\]\nallows the program to skip autodifferentiating through the code for exp. This was simple with forward-mode since we could represent the operation on a Dual number. What’s the equivalent for reverse-mode AD? The answer is the pullback function. If \\(y = [y_1,y_2,\\ldots] = f(x_1,x_2, \\ldots)\\), then \\([\\overline{x_1},\\overline{x_2},\\ldots]=\\mathcal{B}_f^x(\\overline{y})\\) is the pullback of \\(f\\) at the point \\(x\\), defined for a scalar loss function \\(L(y)\\) as:\n\\[\\overline{x_i} = \\frac{\\partial L}{\\partial x_i} = \\sum_j \\frac{\\partial L}{\\partial y_j} \\frac{\\partial y_j}{\\partial x_i}\\]\nUsing the notation from earlier, \\(\\overline{y} = \\frac{\\partial L}{\\partial y}\\) is the derivative of the some intermediate w.r.t. the cost function, and thus\n\\[\\overline{x_i} = \\sum_j \\overline{y_j} \\frac{\\partial y_j}{\\partial x_i} = \\mathcal{B}_f^x(\\overline{y})\\]\nNote that \\(\\mathcal{B}_f^x(\\overline{y})\\) is a function of \\(x\\) because the reverse pass that is use embeds values from the forward pass, and the values from the forward pass to use are those calculated during the evaluation of \\(f(x)\\).\nBy the chain rule, if we don’t have a primitive defined for \\(y_i(x)\\), we can compute that by \\(\\mathcal{B}_{y_i}(\\overline{y})\\), and recursively apply this process until we hit rules that we know. The rules to start with are the scalar derivative rules with follow quite simply, and the multivariate rules which we derived above. For example, if \\(y=f(x)=Ax\\), then\n\\[\\mathcal{B}_{f}^x(\\overline{y}) = \\overline{y}^T A\\]\nwhich is simply saying that the Jacobian of \\(f\\) at \\(x\\) is \\(A\\), and so the vjp is to multiply the vector transpose by \\(A\\).\nLikewise, for element-wise operations, the Jacobian is diagonal, and thus the vjp is multiplying once again by a diagonal matrix against the derivative, deriving the same pullback as we had for backpropagation in a neural network. This then is a quicker encoding and derivation of backpropagation.\n\n\n9.3.7 Multivariate Derivatives from Reverse Mode\nSince the primitive of reverse mode is the vjp, we can understand its behavior by looking at a large primitive. In our simplest case, the function \\(f(x)=Ax\\) outputs a vector value, which we apply our loss function \\(L(y) = \\Vert y-t \\Vert\\) to get a scalar. Thus we seed the scalar output \\(v=1\\), and in the first step backwards we have a vector to scalar function, so the first pullback transforms from \\(1\\) to the vector \\(v_2 = 2|y-t|\\). Then we take that vector and multiply it like \\(v_2^T A\\) to get the derivatives w.r.t. \\(x\\).\nNow let \\(L(y)\\) be a vector function, i.e. we output a vector instead of a scalar from our loss function. Then \\(v\\) is the seed to this process. Let’s assume that \\(v = e_i\\), one of the basis vectors. Then\n\\[v_i^T J = e_i^T J\\]\npulls computes a row of the Jacobian. There, if we had a vector function \\(y=f(x)\\), the pullback \\(\\mathcal{B}_f^x(e_i)\\) is the row of the Jacobian \\(f'(x)\\). Concatenating these is thus a way to build a full Jacobian. The gradient is thus a special case where \\(y\\) is scalar, and thus the resulting Jacobian is just a single row, and therefore we set the seed equal to \\(1\\) to compute the unscaled gradient.\n\n\n9.3.8 Multi-Seeding\nSimilarly to forward-mode having a dual number with multiple simultaneous derivatives through partials \\(d = x + v_1 \\epsilon_1 + \\ldots + v_m \\epsilon_m\\), one can see that multi-seeding is an option in reverse-mode AD by, instead of pulling back a matrix instead of a row vector, where each row is a direction. Thus the matrix \\(A = [v_1 v_2 \\ldots v_n]^T\\) evaluated as \\(\\mathcal{B}_f^x(A)\\) is the equivalent operation to the forward-mode \\(f(d)\\) for generalized multivariate multiseeded reverse-mode automatic differentiation. One should take care to recognize the Jacobian as a generalized linear operator in this case and ensure that the shapes in the program correctly handle this storage of the reverse seed. When linear, this will automatically make use of BLAS3 operations, making it an efficient form for neural networks.\n\n\n9.3.9 Sparse Reverse Mode AD\nSince the Jacobian is built row-by-row with reverse mode AD, the sparse differentiation discussion from forward-mode AD applies similarly but to the transpose. Therefore, in order to perform sparse reverse mode automatic differentiation, one would build up a connectivity graph of the columns, and perform a coloring algorithm on this graph. The seeds of the reverse call, \\(v_i\\), would then be the color vectors, which would compute compressed rows, that are then decompressed similarly to the forward-mode case.\n\n\n9.3.10 Forward Mode vs Reverse Mode\nNotice that a pullback of a single scalar gives the gradient of a function, while the pushforward using forward-mode of a dual gives a directional derivative. Forward mode computes columns of a Jacobian, while reverse mode computes gradients (rows of a Jacobian). Therefore, the relative efficiency of the two approaches is based on the size of the Jacobian. If \\(f:\\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\), then the Jacobian is of size \\[m \\times n\\]. If \\(m\\) is much smaller than \\(n\\), then computing by each row will be faster, and thus use reverse mode. In the case of a gradient, \\(m=1\\) while \\(n\\) can be large, leading to this phenomena. Likewise, if \\(n\\) is much smaller than \\(m\\), then computing by each column will be faster. We will see shortly the reverse mode AD has a high overhead with respect to forward mode, and thus if the values are relatively equal (or \\(n\\) and \\(m\\) are small), forward mode is more efficient.\nHowever, since optimization needs gradients, reverse-mode definitely has a place in the standard toolchain which is why backpropagation is so central to machine learning.\n\n\n9.3.11 Side Note on Mixed Mode\nInterestingly, one can find cases where mixing the forward and reverse mode results would give an asymptotically better result. For example, if a Jacobian was non-zero in only the first 3 rows and first 3 columns, then sparse forward mode would still require N partials and reverse mode would require M seeds. However, one forward mode call of 3 partials and one reverse mode call of 3 seeds would calculate all three rows and columns with \\(\\mathcal{O}(1)\\) work, as opposed to \\(\\mathcal{O}(N)\\) or \\(\\mathcal{O}(M)\\). Exactly how to make use of this insight in an automated manner is an open research question.\n\n\n9.3.12 Forward-Over-Reverse and Hessian-Free Products\nUsing this knowledge, we can also develop quick ways for computing the Hessian. Recall from earlier in the discussion that Hessians are the Jacobian of the gradient. So let’s say for a scalar function \\(f\\) we want to compute the Hessian. To compute the gradient, we use the reverse-mode AD pullback \\(\\nabla f(x) = \\mathcal{B}_f^x(1)\\). Recall that the pullback is a function of \\(x\\) since that is the value at which the values from the forward pass are taken. Then since the Jacobian of the gradient vector is \\(n \\times n\\) (as many terms in the gradient as there are inputs!), it holds that we want to use forward-mode AD for this Jacobian. Therefore, using the dual number \\(x = x_0 + e_1 \\epsilon_1 + \\ldots + e_n \\epsilon_n\\) the reverse mode gradient function computes the full Hessian in one forward pass. What this amounts to is pushing forward the dual number forward sensitivities when building the pullback, and then when doing the pullback the dual portions, will be holding vectors for the columns of the Hessian.\nSimilarly, Hessian-vector products without computing the Hessian can be computed using the Jacobian-vector product trick on the function defined by the gradient. Here, \\(Hv\\) is equivalent to the dual part of\n\\(\\nabla f(x+v\\epsilon) = \\mathcal{B}_f^{x+v\\epsilon}(1)\\)\nThis means that our Newton method for optimization:\n\\[p_{i+1} = p_i - H(p_i)^{-1} \\frac{dC(p_i)}{dp}\\]\ncan be treated similarly to that for the nonlinear solving problem, where the linear system can be solved using Hessian-free vector products to build a Krylov subspace, giving rise to the Hessian-free Newton Krylov method for optimization.\n\n\n9.3.13 References\nWe thank Roger Grosse’s lecture notes for the amazing tikz graphs."
  },
  {
    "objectID": "adjoints.html#youtube-video-link",
    "href": "adjoints.html#youtube-video-link",
    "title": "10  Differentiable Programming and Neural Differential Equations",
    "section": "10.1 Youtube Video Link",
    "text": "10.1 Youtube Video Link\nOur last discussion focused on how, at a high mathematical level, one could in theory build programs which compute gradients in a fast manner by looking at the computational graph and performing reverse-mode automatic differentiation. Within the context of parameter identification, we saw many advantages to this approach because it did not scale multiplicatively in the number of parameters, and thus it is an efficient way to calculate Jacobians of objects where there are less rows than columns (think of the gradient as 1 row).\nMore precisely, this is seen to be more about sparsity patterns, with reverse-mode as being more efficient if there are “enough” less row seeds required than column partials (with mixed mode approaches sometimes being much better). However, to make reverse-mode AD realistically usable inside of a programming language instead of a compute graph, we need to do three things:\n\nWe need to have a way of implementing reverse-mode AD on a language.\nWe need a systematic way to derive “adjoint” relationships (pullbacks).\nWe need to see if there are better ways to fit parameters to data, rather than performing reverse-mode AD through entire programs!"
  },
  {
    "objectID": "adjoints.html#implementation-of-reverse-mode-ad",
    "href": "adjoints.html#implementation-of-reverse-mode-ad",
    "title": "10  Differentiable Programming and Neural Differential Equations",
    "section": "10.2 Implementation of Reverse-Mode AD",
    "text": "10.2 Implementation of Reverse-Mode AD\nForward-mode AD was implementable through operator overloading and dual number arithmetic. However, reverse-mode AD requires reversing a program through its computational structure, which is a much more difficult operation. This begs the question, how does one actually build a reverse-mode AD implementation?\n\n10.2.1 Static Graph AD\nThe most obvious solution is to use a static compute graph, since how we defined our differentiation structure was on a compute graph. Tensorflow is a modern example of this approach, where a user must define variables and operations in a graph language (that’s embedded into Python, R, Julia, etc.), and then execution on the graph is easy to differentiate. This has the advantage of being a simplified and controlled form, which means that not only differentiation transformations are possible, but also things like automatic parallelization. However, many see directly writing a (static) computation graph as a barrier for practical use since it requires completely rewriting all existing programs to this structure.\n\n\n10.2.2 Tracing-Based AD and Wengert Lists\nRecall that an alternative formulation of reverse-mode AD for composed functions\n\\[f = f^L \\circ f^{L-1} \\circ \\ldots \\circ f^1\\]\nis through pullbacks on the Jacobians:\n\\[v^T J = (\\ldots ((v^T J_L) J_{L-1}) \\ldots ) J_1\\]\nTherefore, if one can transform the program structure into a list of composed functions, then reverse-mode AD is the successive application of pullbacks going in the reverse direction:\n\\[\\mathcal{B}_{f}^{x}(A)=\\mathcal{B}_{f^{1}}^{x}\\left(\\ldots\\left(\\mathcal{\\mathcal{B}}_{f^{L-1}}^{f^{L-2}(f^{L-3}(\\ldots f^{1}(x)\\ldots))}\\left(\\mathcal{B}_{f^{L}}^{f^{L-1}(f^{L-2}(\\ldots f^{1}(x)\\ldots))}(A)\\right)\\right)\\ldots\\right)\\]\nRecall that the pullback \\(\\mathcal{B}_f^x(\\overline{y})\\) requires knowing:\n\nThe operation being performed\nThe value \\(x\\) of the forward pass\n\nThe idea is to then build a Wengert list that is from exactly the forward pass of a specific \\(x\\), also known as a trace, and thus giving rise to tracing-based reverse-mode AD. This is the basis of many reverse-mode implementations, such as Julia’s Tracker.jl (Flux.jl’s old AD), ReverseDiff.jl, PyTorch, Tensorflow Eager, Autograd, and Autograd.jl. It is widely adopted due to its simplicity in implementation.\n\n10.2.2.1 Inspecting Tracker.jl\nTracker.jl is a very simple implementation to inspect. The definition of its number and array types are as follows:\n\nstruct Call{F,As&lt;:Tuple}\n  func::F\n  args::As\nend\n\nmutable struct Tracked{T}\n  ref::UInt32\n  f::Call\n  isleaf::Bool\n  grad::T\n  Tracked{T}(f::Call) where T = new(0, f, false)\n  Tracked{T}(f::Call, grad::T) where T = new(0, f, false, grad)\n  Tracked{T}(f::Call{Nothing}, grad::T) where T = new(0, f, true, grad)\nend\n\nmutable struct TrackedReal{T&lt;:Real} &lt;: Real\n  data::T\n  tracker::Tracked{T}\nend\n\nstruct TrackedArray{T,N,A&lt;:AbstractArray{T,N}} &lt;: AbstractArray{T,N}\n  tracker::Tracked{A}\n  data::A\n  grad::A\n  TrackedArray{T,N,A}(t::Tracked{A}, data::A) where {T,N,A} = new(t, data)\n  TrackedArray{T,N,A}(t::Tracked{A}, data::A, grad::A) where {T,N,A} = new(t, data, grad)\nend\n\nAs expected, it replaces every single number and array with a value that will store not just perform the operation, but also build up a list of operations along with the values at every stage. Then pullback rules are implemented for primitives via the @grad macro. For example, the pullback for the dot product is implemented as:\n\n@grad dot(xs, ys) = dot(data(xs), data(ys)), Δ -&gt; (Δ .* ys, Δ .* xs)\n\nThis is read as: the value going forward is computed by using the Julia dot function on the arrays, and the pullback embeds the backs of the forward pass and uses Δ .* ys as the derivative with respect to x, and Δ .* xs as the derivative with respect to y. This element-wise nature makes sense given the diagonal-ness of the Jacobian.\nNote that this also allows utilizing intermediates of the forward pass within the reverse pass. This is seen in the definition of the pullback of meanpool:\n\n@grad function meanpool(x, pdims::PoolDims; kw...)\n  y = meanpool(data(x), pdims; kw...)\n  y, Δ -&gt; (nobacksies(:meanpool, NNlib.∇meanpool(data.((Δ, y, x))..., pdims; kw...)), nothing)\nend\n\nwhere the derivative makes use of not only x, but also y so that the meanpool does not need to be re-calculated.\nUsing this style, Tracker.jl moves forward, building up the value and closures for the backpass and then recursively pulls back the input Δ to receive the derivatve.\n\n\n\n10.2.3 Source-to-Source AD\nGiven our previous discussions on performance, you should be horrified with how this approach handles scalar values. Each TrackedReal holds as Tracked{T} which holds a Call, not a Call{F,As&lt;:Tuple}, and thus it’s not strictly typed. Because it’s not strictly typed, this implies that every single operation is going to cause heap allocations. If you measure this in PyTorch, TensorFlow Eager, Tracker, etc. you get around 500ns-2ms of overhead. This means that a 2ns + operation becomes… &gt;500ns! Oh my!\nThis is not the only issue with tracing. Another issue is that the trace is value-dependent, meaning that every new value can build a new trace. Thus one cannot easily JIT compile a trace because it’ll be different for every gradient calculation (you can compile it, but you better make sure the compile times are short!). Lastly, the Wengert list can be much larger than the code itself. For example, if you trace through a loop that is for i in 1:100000, then the trace will be huge, even if the function is relatively simple. This is directly demonstrated in the JAX “how it works” slide:\n\nTo avoid these issues, another version of reverse-mode automatic differentiation is source-to-source transformations. In order to do source code transformations, you need to know how to transform all language constructs via the reverse pass. This can be quite difficult (what is the “adjoint” of lock?), but when worked out this has a few benefits. First of all, you do not have to track values, meaning stack-allocated values can stay on the stack. Additionally, you can JIT compile one backpass because you have a single function used for all backpasses. Lastly, you don’t need to unroll your loops! Instead, which each branch you’d need to insert some data structure to recall the values used from the forward pass (in order to invert in the right directions). However, that can be much more lightweight than a tracking pass.\nThis can be a difficult problem to do on a general programming language. In general it needs a strong programmatic representation to use as a compute graph. Google’s engineers did an analysis when choosing Swift for TensorFlow and narrowed it down to either Swift or Julia due to their internal graph structures. Thus, it should be no surprise that the modern source-to-source AD systems are Zygote.jl for Julia, and Swift for TensorFlow in Swift. Additionally, older AD systems, like Tampenade, ADIFOR, and TAF, all for Fortran, were source-to-source AD systems."
  },
  {
    "objectID": "adjoints.html#derivation-of-reverse-mode-rules-adjoints-and-implicit-function-theorem",
    "href": "adjoints.html#derivation-of-reverse-mode-rules-adjoints-and-implicit-function-theorem",
    "title": "10  Differentiable Programming and Neural Differential Equations",
    "section": "10.3 Derivation of Reverse Mode Rules: Adjoints and Implicit Function Theorem",
    "text": "10.3 Derivation of Reverse Mode Rules: Adjoints and Implicit Function Theorem\nIn order to require the least amount of work from our AD system, we need to be able to derive the adjoint rules at the highest level possible. Here are a few well-known cases to start understanding. These next examples are from Steven Johnson’s resource.\n\n10.3.1 Adjoint of Linear Solve\nLet’s say we have the function \\(A(p)x=b(p)\\), i.e. this is the function that is given by the linear solving process, and we want to calculate the gradients of a cost function \\(g(x,p)\\). To evaluate the gradient directly, we’d calculate:\n\\[\\frac{dg}{dp} = g_p + g_x x_p\\]\nwhere \\(x_p\\) is the derivative of each value of \\(x\\) with respect to each parameter \\(p\\), and thus it’s an \\(M \\times P\\) matrix (a Jacobian). Since \\(g\\) is a small cost function, \\(g_p\\) and \\(g_x\\) are easy to compute, but \\(x_p\\) is given by:\n\\[x_{p_i} = A^{-1}(b_{p_i}-A_{p_i}x)\\]\nand so this is \\(P\\) \\(M \\times M\\) linear solves, which is expensive! However, if we multiply by\n\\[\\lambda^{T} = g_x A^{-1}\\]\nthen we obtain\n\\[\\frac{dg}{dp}\\vert_{f=0} = g_p - \\lambda^T f_p = g_p - \\lambda^T (A_p x - b_p)\\]\nwhich is an alternative formulation of the derivative at the solution value. However, in this case there is no computational benefit to this reformulation.\n\n\n10.3.2 Adjoint of Nonlinear Solve\nNow let’s look at some \\(f(x,p)=0\\) nonlinear solving. Differentiating by \\(p\\) gives us:\n\\[f_x x_p + f_p = 0\\]\nand thus \\(x_p = -f_x^{-1}f_p\\). Therefore, using our cost function we write:\n\\[\\frac{dg}{dp} = g_p + g_x x_p = g_p - g_x \\left(f_x^{-1} f_p \\right)\\]\nor\n\\[\\frac{dg}{dp} = g_p - \\left(g_x f_x^{-1} \\right) f_p\\]\nSince \\(g_x\\) is \\(1 \\times M\\), \\(f_x^{-1}\\) is \\(M \\times M\\), and \\(f_p\\) is \\(M \\times P\\), this grouping changes the problem gets rid of the size \\(MP\\) term.\nAs is normal with backpasses, we solve for \\(x\\) through the forward pass however we like, and then for the backpass solve for\n\\[f_x^T \\lambda = g_x^T\\]\nto obtain\n\\[\\frac{dg}{dp}\\vert_{f=0} = g_p - \\lambda^T f_p\\]\nwhich does the calculation without ever building the size \\(M \\times MP\\) term.\n\n\n10.3.3 Adjoint of Ordinary Differential Equations\nWe with to solve for some cost function \\(G(u,p)\\) evaluated throughout the differential equation, i.e.:\n\\[G(u,p) = G(u(p)) = \\int_{t_0}^T g(u(t,p))dt\\]\nTo derive this adjoint, introduce the Lagrange multiplier \\(\\lambda\\) to form:\n\\[I(p) = G(p) - \\int_{t_0}^T \\lambda^\\ast (u^\\prime - f(u,p,t))dt\\]\nSince \\(u^\\prime = f(u,p,t)\\), this is the mathematician’s trick of adding zero, so then we have that\n\\[\\frac{dG}{dp} = \\frac{dI}{dp} = \\int_{t_0}^T (g_p + g_u s)dt - \\int_{t_0}^T \\lambda^\\ast (s^\\prime - f_u s - f_p)dt\\]\nfor \\(s\\) being the sensitivity, \\(s = \\frac{du}{dp}\\). After applying integration by parts to \\(\\lambda^\\ast s^\\prime\\), we get that:\n\\[\\int_{t_{0}}^{T}\\lambda^{\\ast}\\left(s^{\\prime}-f_{u}s-f_{p}\\right)dt  =\\int_{t_{0}}^{T}\\lambda^{\\ast}s^{\\prime}dt-\\int_{t_{0}}^{T}\\lambda^{\\ast}\\left(f_{u}s-f_{p}\\right)dt\\] \\[=|\\lambda^{\\ast}(t)s(t)|_{t_{0}}^{T}-\\int_{t_{0}}^{T}\\lambda^{\\ast\\prime}sdt-\\int_{t_{0}}^{T}\\lambda^{\\ast}\\left(f_{u}s-f_{p}\\right)dt\\]\nTo see where we ended up, let’s re-arrange the full expression now:\n\\[\\frac{dG}{dp} =\\int_{t_{0}}^{T}(g_{p}+g_{u}s)dt+|\\lambda^{\\ast}(t)s(t)|_{t_{0}}^{T}-\\int_{t_{0}}^{T}\\lambda^{\\ast\\prime}sdt-\\int_{t_{0}}^{T}\\lambda^{\\ast}\\left(f_{u}s-f_{p}\\right)dt\\] \\[=\\int_{t_{0}}^{T}(g_{p}+\\lambda^{\\ast}f_{p})dt+|\\lambda^{\\ast}(t)s(t)|_{t_{0}}^{T}-\\int_{t_{0}}^{T}\\left(\\lambda^{\\ast\\prime}+\\lambda^\\ast f_{u}-g_{u}\\right)sdt\\]\nThat was just a re-arrangement. Now, let’s require that\n\\[\\lambda^\\prime = -\\frac{df}{du}^\\ast \\lambda + \\left(\\frac{dg}{du} \\right)^\\ast\\] \\[\\lambda(T) = 0\\]\nThis means that the boundary term of the integration by parts is zero, and also one of those integral terms are perfectly zero. Thus, if \\(\\lambda\\) satisfies that equation, then we get:\n\\[\\frac{dG}{dp} = \\lambda^\\ast(t_0)\\frac{dG}{du}(t_0) + \\int_{t_0}^T \\left(g_p + \\lambda^\\ast f_p \\right)dt\\]\nwhich gives us our adjoint derivative relation.\nIf \\(G\\) is discrete, then it can be represented via the Dirac delta:\n\\[G(u,p) = \\int_{t_0}^T \\sum_{i=1}^N \\Vert d_i - u(t_i,p)\\Vert^2 \\delta(t_i - t)dt\\]\nin which case\n\\[g_u(t_i) = 2(d_i - u(t_i,p))\\]\nat the data points \\((t_i,d_i)\\). Therefore, the derivative of an ODE solution with respect to a cost function is given by solving for \\(\\lambda^\\ast\\) using an ODE for \\(\\lambda^T\\) in reverse time, and then using that to calculate \\(\\frac{dG}{dp}\\). Note that \\(\\frac{dG}{dp}\\) can be calculated simultaneously by appending a single value to the reverse ODE, since we can simply define the new ODE term as \\(g_p + \\lambda^\\ast f_p\\), which would then calculate the integral on the fly (ODE integration is just… integration!).\n\n\n10.3.4 Complexities of Implementing ODE Adjoints\nThe image below explains the dilemma:\n\nEssentially, the whole problem is that we need to solve the ODE\n\\[\\lambda^\\prime = -\\frac{df}{du}^\\ast \\lambda - \\left(\\frac{dg}{du} \\right)^\\ast\\] \\[\\lambda(T) = 0\\]\nin reverse, but \\(\\frac{df}{du}\\) is defined by \\(u(t)\\) which is a value only computed in the forward pass (the forward pass is embedded within the backpass!). Thus we need to be able to retrieve the value of \\(u(t)\\) to get the Jacobian on-demand. There are three ways which this can be done:\n\nIf you solve the reverse ODE \\(u^\\prime = f(u,p,t)\\) backwards in time, mathematically it’ll give equivalent values. Computation-wise, this means that you can append \\(u(t)\\) to \\(\\lambda(t)\\) (to \\(\\frac{dG}{dp}\\)) to calculate all terms at the same time with a single reverse pass ODE. However, numerically this is unstable and thus not always recommended (ODEs are reversible, but ODE solver methods are not necessarily going to generate the same exact values or trajectories in reverse!)\nIf you solve the forward ODE and receive a continuous solution \\(u(t)\\), you can interpolate it to retrieve the values at any given the time reverse pass needs the \\(\\frac{df}{du}\\) Jacobian. This is fast but memory-intensive.\nEvery time you need a value \\(u(t)\\) during the backpass, you re-solve the forward ODE to \\(u(t)\\). This is expensive! Thus one can instead use checkpoints, i.e. save at finitely many time points during the forward pass, and use those as starting points for the \\(u(t)\\) calculation.\n\nAlternative strategies can be investigated, such as an interpolation which stores values in a compressed form.\n\n\n10.3.5 The vjp and Neural Ordinary Differential Equations\nIt is here that we can note that, if \\(f\\) is a function defined by a neural network, we arrive at the neural ordinary differential equation. This adjoint method is thus the backpropagation method for the neural ODE. However, the backpass\n\\[\\lambda^\\prime = -\\frac{df}{du}^\\ast \\lambda - \\left(\\frac{dg}{du} \\right)^\\ast\\] \\[\\lambda(T) = 0\\]\ncan be improved by noticing \\(\\frac{df}{du}^\\ast \\lambda\\) is a vjp, and thus it can be calculated using \\(\\mathcal{B}_f^{u(t)}(\\lambda^\\ast)\\), i.e. reverse-mode AD on the function \\(f\\). If \\(f\\) is a neural network, this means that the reverse ODE is defined through successive backpropagation passes of that neural network. The result is a derivative with respect to the cost function of the parameters defining \\(f\\) (either a model or a neural network), which can then be used to fit the data (“train”)."
  },
  {
    "objectID": "adjoints.html#alternative-training-strategies",
    "href": "adjoints.html#alternative-training-strategies",
    "title": "10  Differentiable Programming and Neural Differential Equations",
    "section": "10.4 Alternative “Training” Strategies",
    "text": "10.4 Alternative “Training” Strategies\nThose are the “brute force” training methods which simply use \\(u(t,p)\\) evaluations to calculate the cost. However, it is worth noting that there are a few better strategies that one can employ in the case of dynamical models.\n\n10.4.1 Multiple Shooting Techniques\nInstead of shooting just from the beginning, one can instead shoot from multiple points in time:\n\nOf course, one won’t know what the “initial condition in the future” is, but one can instead make that a parameter. By doing so, each interval can be solved independently, and one can then add to the cost function that the end of one interval must match up with the beginning of the other. This can make the integration more robust, since shooting with incorrect parameters over long time spans can give massive gradients which makes it hard to hone in on the correct values.\n\n\n10.4.2 Collocation Methods\nIf the data is dense enough, one can fit a curve through the points, such as a spline:\n\nIf that’s the case, one can use the fit spline in order to estimate the derivative at each point. Since the ODE is defined as \\(u^\\prime = f(u,p,t)\\), one then then use the cost function\n\\[C(p) = \\sum_{i=1}^N \\Vert\\tilde{u}^{\\prime}(t_i) - f(u(t_i),p,t)\\Vert\\]\nwhere \\(\\tilde{u}^{\\prime}(t_i)\\) is the estimated derivative at the time point \\(t_i\\). Then one can fit the parameters to ensure this holds. This method can be extremely fast since the ODE doesn’t ever have to be solved! However, note that this is not able to compensate for error accumulation, and thus early errors are not accounted for in the later parts of the data. This means that the integration won’t necessarily match the data even if this fit is “good” if the data points are too far apart, a property that is not true with fitting. Thus, this is usually done as part of a two-stage method, where the starting stage uses collocation to get initial parameters which is then completed with a shooting method."
  },
  {
    "objectID": "mpi.html",
    "href": "mpi.html",
    "title": "11  Introduction to MPI.jl",
    "section": "",
    "text": "Description of MPI and MPI.jl, including demonstrations on its use.\n\nMPI.jl slides\nMPI.jl video"
  },
  {
    "objectID": "gpus.html#youtube-video-part-1",
    "href": "gpus.html#youtube-video-part-1",
    "title": "12  GPU programming",
    "section": "12.1 Youtube Video Part 1",
    "text": "12.1 Youtube Video Part 1"
  },
  {
    "objectID": "gpus.html#youtube-video-part-2",
    "href": "gpus.html#youtube-video-part-2",
    "title": "12  GPU programming",
    "section": "12.2 Youtube Video Part 2",
    "text": "12.2 Youtube Video Part 2"
  },
  {
    "objectID": "gpus.html#levels-of-parallelism-in-hardware",
    "href": "gpus.html#levels-of-parallelism-in-hardware",
    "title": "12  GPU programming",
    "section": "12.3 Levels of parallelism in hardware",
    "text": "12.3 Levels of parallelism in hardware\n\nInstruction-Level Parallelism\nData-Level Parallelism\n\nSIMD/Vector\nGPUs\n\nThread-Level Parallelism\n\n\n12.3.1 Instruction-Level Parallelism\nInstruction-Level parallelism is used by your compiler and CPU to speed up serial programs. To signify that you are not expected to write code that considers ILP, the following code-snippets are in a very explicit low-level Julia dialect that approximates 1 RISC instruction per line.\n\nfunction f(A, x)\n    i = length(A)\n\n    @label Loop\n    a = A[i]    # Load\n    c = a + x   # Add\n    A[i] = c    # Store\n    i = i - 1   # Decrement\n    i &gt; 0 && @goto Loop\n\n    return A\nend\n\nThe same code in RISC-V would have been:\n...\nLoop: fld    f0, 0(x1)\n      fadd.d f4, f0, f2\n      fsd    f4, 0(x1)\n      addi   x1, x1, -8\n      bnez   x1, Loop\n...\n** What are the data-dependencies in this loop? **\n\n\n12.3.2 Pipeline Scheduling and loop unrolling\n\n12.3.2.1 Latency\n\nLoad latency: 1 cycle\nFloat arithmetic latency: 2 cycle\nInteger arithmetic latency: 0 cycle\n\n\n    @label Loop\n    a = A[i]             # Cycle 1\n    # Stall              # Cycle 2\n    c = a + x            # Cycle 3\n    # Stall              # Cycle 4\n    # Stall              # Cycle 5\n    A[i] = c             # Cycle 6\n    i = i - 1            # Cycle 7\n    i &gt; 0 && @goto Loop  # Cycle 8\n\nWith our given latencies and issueing one operation per cycle, we can execute the loop in 8 cycles. By reordering we can execute it in 7 cycles. Can we do better?\n\n    @label Loop\n    a = A[i]             # Cycle 1\n    i = i - 1            # Cycle 2\n    c = a + x            # Cycle 3\n    # Stall              # Cycle 4\n    # Stall              # Cycle 5\n    A[i+1] = c             # Cycle 6\n    i &gt; 0 && @goto Loop  # Cycle 7\n\nBy reordering the decrement we can hide the load latency.\n\nHow many cylces are overhead: 2\nHow many stall cycles: 2\nHow many cycles are actually work: 3\n\nIn order to improve the performance of this code we want to reduce the overhead of the loop in relative to the work. One technique compilers will use is loop-unrolling. Unrolling replicates the loop body multiple times, changing the loop exit condition accordingly. This requires duplicating the loop so that we can handle iteration lengths that are not a multiple of the unrolling factor.\nNote: A[i+1] is free since it can be precomputed relative to A[i]\n\n\n12.3.2.2 Unrolling by a factor of 4\n\n    @label Loop\n    a = A[i]\n    c = a + x\n    A[i] = c\n    a1 = A[i-1]\n    c1 = a1 + x\n    A[i-1] = c1\n    a2 = A[i-2]\n    c2 = a2 + x\n    A[i-2] = c2\n    a3 = A[i-3]\n    c3 = a3 + x\n    A[i-3] = c3\n    i = i - 4\n    i &gt; 4 && @goto Loop\n\nBy unrolling with a factor of 4, we have reduced the overhead to 2 cycles (ignoring stalls for now). Note that A[i-3] can be precomputed relative to A and is therefore free on most architectures.\n\nDo we still have stalls?: Yes\nHow many cycles are overhead: 2\nHow many stall cycles: 12\nHow many cycles are actually work: 12\n\n\n    @label Loop\n    a = A[i]\n    # Stall\n    c = a + x\n    # Stall\n    # Stall\n    A[i] = c\n    a1 = A[i-1]\n    # Stall\n    c1 = a1 + x\n    # Stall\n    # Stall\n    A[i-1] = c1\n    a2 = A[i-2]\n    # Stall\n    c2 = a2 + x\n    # Stall\n    # Stall\n    A[i-2] = c2\n    a3 = A[i-3]\n    # Stall\n    c3 = a3 + x\n    # Stall\n    # Stall\n    A[i-3] = c3\n    i = i - 4\n    i &gt; 4 && @goto Loop\n\nCan we re-order to reduce stalls?\n\n    @label Loop\n    a  = A[i]\n    a1 = A[i-1]\n    a2 = A[i-2]\n    a3 = A[i-3]\n    c  = a  + x\n    c1 = a1 + x\n    c2 = a2 + x\n    c3 = a3 + x\n    A[i]   = c\n    A[i-1] = c1\n    A[i-2] = c2\n    A[i-3] = c3\n    i = i - 4\n    i &gt; 4 && @goto Loop\n\n\nHow many cycles are overhead (this includes stalls): 2\nHow many cycles are actually work: 12\n\nThis is also called interleaving and one should note that we started to cluster operations together. Instead of expressing operations like this that are inherently data-parallel in a serial manner and expecting the compiler and the underlying architecture to pick up the slack, we can also also explicitly express the data-parallelism. The two big avenues of doing so are: explicit vector programming and GPU programming.\n\n\n\n12.3.3 Data-parallelism\nNot all programs are data parallel programs, but many in scientific computing are and this has caused the introduction of hardware specialised to perform data-parallel operations. As an example many modern CPUs include vector extensions that enable Single-Instruction-Multiple-Data (SIMD) programming.\n\n12.3.3.1 SIMD (explicit vectorized)\nTerms: Each vector element is processed by a vector lane.\n\nusing SIMD\nA = rand(Float64, 64)\nT = Vec{4, Float64}\nx = 1.0\n\nfor i in 1:4:length(A)\n    a = vload(T, A, i)\n    c = a + x\n    vstore(c, A, i)\nend\n\n\nStalls are only per instruction, and not per element\nreduced overhead processing of 3*&lt;4xFloat64&gt; per iteration with only 2 overhead instructions (excluding stalls), so the overhead is amortized across 4 elements.\n\nNote: - We can remove stalls similar to what we did for the serial code: - pipelining - interleaving and unrolling - Latencies will be higher\n\n12.3.3.1.1 How do we handle branching\nTranslating serial code to a vector processor is tricky if there are data or index dependent control-flow. There are some architectures (see the NEC Aurora VX) that have support for vector predication and there are also masked load and store instructions for SIMD on Intel CPUs. In general though one has to do a manual transform that computes both sides of the branch and then merges the results together.\n\nA = rand(Int64, 64)\nfor i in 1:length(A)\n    a = A[i]\n    if a % 2 == 0\n        A[i] = -a\n    end\nend\n\n\nusing SIMD\nA = rand(Int64, 64)\nT = Vec{4, Int64}\n\nfor i in 1:4:length(A)\n    a = vload(T, A, i)\n    mask = a % 2 == 0        # calculate mask\n    b = -a                   # If branch\n    c = vifelse(mask, b, a)  # merge results\n    vstore(c, A, i)\nend\n\n\n\n\n12.3.3.2 GPU (implicit vectorized)\nInstead of using explicit vectorization, GPUs change the programming model so that the programmer writes a kernel which operates over each element of the data. In effect the programmer is writing a program that is executed for each vector lane. It is important to remember that the hardware itself still operates on vectors (CUDA calls this warp-size and it is 32 elements).\nAt this point please refer to the lecture slides"
  },
  {
    "objectID": "pdes_and_convolutions.html#youtube-video",
    "href": "pdes_and_convolutions.html#youtube-video",
    "title": "13  PDEs, Convolutions, and the Mathematics of Locality",
    "section": "13.1 Youtube Video",
    "text": "13.1 Youtube Video\nAt this point we have identified how the worlds of machine learning and scientific computing collide by looking at the parameter estimation problem. Training neural networks is parameter estimation of a function f where f is a neural network. Backpropagation of a neural network is simply the adjoint problem for f, and it falls under the class of methods used in reverse-mode automatic differentiation. But this story also extends to structure. Recurrent neural networks are the Euler discretization of a continuous recurrent neural network, also known as a neural ordinary differential equation.\nGiven all of these relations, our next focus will be on the other class of commonly used neural networks: the convolutional neural network (CNN). It turns out that in this case there is also a clear analogue to convolutional neural networks in traditional scientific computing, and this is seen in discretizations of partial differential equations. To see this, we will first describe the convolution operation that is central to the CNN and see how this object naturally arises in numerical partial differential equations."
  },
  {
    "objectID": "pdes_and_convolutions.html#convolutional-neural-networks",
    "href": "pdes_and_convolutions.html#convolutional-neural-networks",
    "title": "13  PDEs, Convolutions, and the Mathematics of Locality",
    "section": "13.2 Convolutional Neural Networks",
    "text": "13.2 Convolutional Neural Networks\nThe purpose of a convolutional neural network is to be a network which makes use of the spatial structure of an image. An image is a 3-dimensional object: width, height, and 3 color channels. The convolutional operations keeps this structure intact and acts against this object is a 3-tensor. A convolutional layer is a function that applies a stencil to each point. This is illustrated by the following animation:\n\n\n\nconvolution\n\n\nThis is the 2D stencil:\n1  0  1\n0  1  0\n1  0  1\nwhich is then applied to the matrix at each inner point to go from an NxNx3 matrix to an (N-2)x(N-2)x3 matrix.\nAnother operation used with convolutions is the pooling layer. For example, the maxpool layer is stencil which takes the maximum of the the value and its neighbor, and the meanpool takes the mean over the nearby values, i.e. it is equivalent to the stencil:\n1/9 1/9 1/9\n1/9 1/9 1/9\n1/9 1/9 1/9\nA convolutional neural network is then composed of layers of this form. We can express this mathematically by letting \\(conv(x;S)\\) as the convolution of \\(x\\) given a stencil \\(S\\). If we let \\(dense(x;W,b,σ) = σ(W*x + b)\\) as a layer from a standard neural network, then deep convolutional neural networks are of forms like:\n\\[CNN(x) = dense(conv(maxpool(conv(x))))\\]\nwhich can be expressed in Flux.jl syntax as:\n\nm = Chain(\n  Conv((2,2), 1=&gt;16, relu),\n  x -&gt; maxpool(x, (2,2)),\n  Conv((2,2), 16=&gt;8, relu),\n  x -&gt; maxpool(x, (2,2)),\n  x -&gt; reshape(x, :, size(x, 4)),\n  Dense(288, 10), softmax) |&gt; gpu"
  },
  {
    "objectID": "pdes_and_convolutions.html#discretizations-of-partial-differential-equations",
    "href": "pdes_and_convolutions.html#discretizations-of-partial-differential-equations",
    "title": "13  PDEs, Convolutions, and the Mathematics of Locality",
    "section": "13.3 Discretizations of Partial Differential Equations",
    "text": "13.3 Discretizations of Partial Differential Equations\nNow let’s investigate discertizations of partial differential equations. A canonical differential equation to start with is the Poisson equation. This is the equation:\n\\[u_{xx} = f(x)\\]\nwhere here we have that subscripts correspond to partial derivatives, i.e. this syntax stands for the partial differential equation:\n\\[\\frac{d^2u}{dx^2} = f(x)\\]\nIn this case, \\(f\\) is some given data and the goal is to find the \\(u\\) that satisfies this equation. There are two ways this is generally done:\n\nExpand out the derivative in terms of Taylor series approximations.\nExpand out \\(u\\) in terms of some function basis.\n\n\n13.3.1 Finite Difference Discretizations\nLet’s start by looking at Taylor series approximations to the derivative. In this case, we will use what’s known as finite differences. The simplest finite difference approximation is known as the first order forward difference. This is commonly denoted as\n\\[\\delta_{+}u=\\frac{u(x+\\Delta x)-u(x)}{\\Delta x}\\]\nThis looks like a derivative, and we think it’s a derivative as \\(\\Delta x\\rightarrow 0\\), but let’s show that this approximation is meaningful. Assume that \\(u\\) is sufficiently nice. Then from a Taylor series we have that\n\\[u(x+\\Delta x)=u(x)+\\Delta xu^{\\prime}(x)+\\mathcal{O}(\\Delta x^{2})\\]\n(here I write \\(\\left(\\Delta x\\right)^{2}\\) as \\(\\Delta x^{2}\\) out of convenience, note that those two terms are not necessarily the same). That term on the end is called “Big-O Notation”. What is means is that those terms are asymptotically like \\(\\Delta x^{2}\\). If \\(\\Delta x\\) is small, then \\(\\Delta x^{2}\\ll\\Delta x\\) and so we can think of those terms as smaller than any of the terms we show in the expansion. By simplification notice that we get\n\\[\\frac{u(x+\\Delta x)-u(x)}{\\Delta x}=u^{\\prime}(x)+\\mathcal{O}(\\Delta x)\\] This means that \\(\\delta_{+}\\) is correct up to first order, where the \\(\\mathcal{O}(\\Delta x)\\) portion that we dropped is the error. Thus \\(\\delta_{+}\\) is a first order approximation.\nNotice that the same proof shows that the backwards difference,\n\\[\\delta_{-}u=\\frac{u(x)-u(x-\\Delta x)}{\\Delta x}\\]\nis first order.\n\n13.3.1.1 Second Order Approximations to the First Derivative\nNow let’s look at the following:\n\\[\\delta_{0}u=\\frac{u(x+\\Delta x)-u(x-\\Delta x)}{2\\Delta x}.\\]\nThe claim is this differencing scheme is second order. To show this, we once again turn to Taylor Series. Let’s do this for both terms:\n\\[u(x+\\Delta x) =u(x)+\\Delta xu^{\\prime}(x)+\\frac{\\Delta x^{2}}{2}u^{\\prime\\prime}(x)+\\mathcal{O}(\\Delta x^{3})\\] \\[u(x-\\Delta x) =u(x)-\\Delta xu^{\\prime}(x)+\\frac{\\Delta x^{2}}{2}u^{\\prime\\prime}(x)+\\mathcal{O}(\\Delta x^{3})\\]\nNow we subtract the two:\n\\[u(x+\\Delta x)-u(x-\\Delta x)=2\\Delta xu^{\\prime}(x)+\\mathcal{O}(\\Delta x^{3})\\]\nand thus we move tems around to get\n\\[\\delta_{0}u=\\frac{u(x+\\Delta x)-u(x-\\Delta x)}{2\\Delta x}=u^{\\prime}(x)+\\mathcal{O}\\left(\\Delta x^{2}\\right)\\]\nWhat does this improvement mean? Let’s say we go from \\(\\Delta x\\) to \\(\\frac{\\Delta x}{2}\\). Then while the error from the first order method is around \\(\\frac{1}{2}\\) the original error, the error from the central differencing method is \\(\\frac{1}{4}\\) the original error! When trying to get an accurate solution, this quadratic reduction can make quite a difference in the number of required points.\n\n\n13.3.1.2 Second Derivative Central Difference\nNow we want a second derivative approximation. Let’s show the classic central difference formula for the second derivative:\n\\[\\delta_{0}^{2}u=\\frac{u(x+\\Delta x)-2u(x)+u(x-\\Delta x)}{\\Delta x^{2}}\\]\nis second order. To do so, we expand out the two terms:\n\\[u(x+\\Delta x) =u(x)+\\Delta xu^{\\prime}(x)+\\frac{\\Delta x^{2}}{2}u^{\\prime\\prime}(x)+\\frac{\\Delta x^{3}}{6}u^{\\prime\\prime\\prime}(x)+\\mathcal{O}\\left(\\Delta x^{4}\\right)\\] \\[u(x-\\Delta x) =u(x)-\\Delta xu^{\\prime}(x)+\\frac{\\Delta x^{2}}{2}u^{\\prime\\prime}(x)-\\frac{\\Delta x^{3}}{6}u^{\\prime\\prime\\prime}(x)+\\mathcal{O}\\left(\\Delta x^{4}\\right)\\]\nand now plug it in. It’s clear the \\(u(x)\\) cancels out. The opposite signs makes \\(u^{\\prime}(x)\\) cancel out, and then the same signs and cancellation makes the \\(u^{\\prime\\prime}\\) term have a coefficient of 1. But, the opposite signs makes the \\(u^{\\prime\\prime\\prime}\\) term cancel out. Thus when we simplify and divide by \\(\\Delta x^{2}\\) we get\n\\[\\frac{u(x+\\Delta x)-2u(x)+u(x-\\Delta x)}{\\Delta x^{2}}=u^{\\prime\\prime}(x)+\\mathcal{O}\\left(\\Delta x^{2}\\right).\\]\n\n\n13.3.1.3 Finite Differencing from Polynomial Interpolation\nFinite differencing can also be derived from polynomial interpolation. Draw a line between two points. What is the approximation for the first derivative?\n\\[\\delta_{+}u=\\frac{u(x+\\Delta x)-u(x)}{\\Delta x}\\]\nNow draw a quadratic through three points. i.e., given \\(u_{1}\\), \\(u_{2}\\), and \\(u_{3}\\) at \\(x=0\\), \\(\\Delta x\\), \\(2\\Delta x\\), we want to find the interpolating polynomial\n\\[g(x)=a_{1}x^{2}+a_{2}x+a_{3}\\].\nSetting \\(g(0)=u_{1}\\), \\(g(\\Delta x)=u_{2}\\), and \\(g(2\\Delta x)=u_{3}\\), we get the following relations:\n\\[u_{1} =g(0)=a_{3}\\] \\[u_{2} =g(\\Delta x)=a_{1}\\Delta x^{2}+a_{2}\\Delta x+a_{3}\\] \\[u_{3} =g(2\\Delta x)=4a_{1}\\Delta x^{2}+2a_{2}\\Delta x+a_{3}\\]\nwhich when we write in matrix form is:\n\\[\\left(\\begin{array}{ccc}\n0 & 0 & 1\\\\\n\\Delta x^{2} & \\Delta x & 1\\\\\n4\\Delta x^{2} & 2\\Delta x & 1\n\\end{array}\\right)\\left(\\begin{array}{c}\na_{1}\\\\\na_{2}\\\\\na_{3}\n\\end{array}\\right)=\\left(\\begin{array}{c}\nu_{1}\\\\\nu_{2}\\\\\nu_{3}\n\\end{array}\\right)\\]\nand thus we can invert the matrix to get the a’s:\n\\[a_{1} =\\frac{u_{3}-2u_{2}+u_{1}}{2\\Delta x^{2}}\\] \\[a_{2} =\\frac{-u_{3}+4u_{2}-3u_{1}}{2\\Delta x}\\] \\[a_{3} =u_{1}\\text{ or }g(x)=\\frac{u_{3}-2u_{2}-u_{1}}{2\\Delta x^{2}}x^{2}+\\frac{-u_{3}+4u_{2}-3u_{1}}{2\\Delta x}x+u_{1}\\]\nNow we can get derivative approximations from this. Notice for example that\n\\[g^{\\prime}(x)=\\frac{u_{3}-2u_{2}+u_{1}}{\\Delta x^{2}}x+\\frac{-u_{3}+4u_{2}-3u_{1}}{2\\Delta x}\\]\nNow what’s the derivative at the middle point?\n\\[g^{\\prime}\\left(\\Delta x\\right)=\\frac{u_{3}-2u_{2}+u_{1}}{\\Delta x}+\\frac{-u_{3}+4u_{2}-3u_{1}}{2\\Delta x}=\\frac{u_{3}-u_{1}}{2\\Delta x}.\\]\nAnd now check\n\\[g^{\\prime\\prime}(\\Delta x)=\\frac{u_{3}-2u_{2}+u_{1}}{\\Delta x^{2}}\\] which is the central derivative formula. This gives a systematic way of deriving higher order finite differencing formulas. In fact, this formulation allows one to derive finite difference formulae for non-evenly spaced grids as well! The algorithm which automatically generates stencils from the interpolating polynomial forms is the Fornberg algorithm.\n\n\n13.3.1.4 Multidimensional Finite Difference Operations\nNow let’s look at the multidimensional Poisson equation, commonly written as:\n\\[\\Delta u = f(x,y)\\]\nwhere \\(\\Delta u = u_{xx} + u_{yy}\\). Using the logic of the previous sections, we can approximate the two derivatives to have:\n\\[\\frac{u(x+\\Delta x,y)-2u(x,y)+u(x-\\Delta x,y)}{\\Delta x^{2}} + \\frac{u(x,y+\\Delta y)-2u(x,y)+u(x-x,y-\\Delta y)}{\\Delta y^{2}}=u^{\\prime\\prime}(x)+\\mathcal{O}\\left(\\Delta x^{2}\\right) + \\mathcal{O}\\left(\\Delta y^{2}\\right).\\]\nNotice that this is the stencil operation:\n0  1 0\n1 -4 1\n0  1 0\nThis means that derivative discretizations are stencil or convolutional operations."
  },
  {
    "objectID": "pdes_and_convolutions.html#representation-and-implementation-of-stencil-operations",
    "href": "pdes_and_convolutions.html#representation-and-implementation-of-stencil-operations",
    "title": "13  PDEs, Convolutions, and the Mathematics of Locality",
    "section": "13.4 Representation and Implementation of Stencil Operations",
    "text": "13.4 Representation and Implementation of Stencil Operations\n\n13.4.1 Stencil Operations as Sparse Matrices\nStencil operations are linear operators, i.e. \\(S[x+\\alpha y] = S[x] + \\alpha S[y]\\) for any sufficiently nice stencil operation \\(S\\) (note “sufficiently nice”: there is a “stencil” operation mentioned in the convolutional neural networks section which was not linear: which operation was it?). Now we write these operators as matrices. Notice that for the vector:\n\\[U=\\left(\\begin{array}{c}\nu_{1}\\\\\n\\vdots\\\\\nu_{n}\n\\end{array}\\right),\\] we have that\n\\[\\delta_{+}U=\\left(\\begin{array}{c}\nu_{2}-u_{1}\\\\\n\\vdots\\\\\nu_{n}-u_{n-1}\n\\end{array}\\right)\\]\nand so\n\\[\\delta_{+}=\\left(\\begin{array}{ccccc}\n-1 & 1\\\\\n& -1 & 1\\\\\n&  & \\ddots & \\ddots\\\\\n&  &  & -1 & 1\n\\end{array}\\right)\\]\nWe can do the same to understand the other operators. But notice something: this operator isn’t square! In order for this to be square, in order to know what happens at the endpoint, we need to know the boundary conditions. I.e., an assumption on the value or derivative at \\(u(0)\\) or \\(u(1)\\) is required in order to get the first/last rows of the matrix!\nSimilarly, \\(\\delta_{0}^{2}\\) can be represented as the tridiagonal matrix of [1 -2 1], also known as the Strang matrix.\nNow let’s think about the higher dimensional forms as a vector, i.e. vec(u). In this case, what is the matrix A for which reshape(A*vec(u),size(u)...) performs the higher dimensional Laplacian, i.e. \\(u_{xx} + u_{yy}\\)? The answer is that it discretizes via Kronecker products to:\n\\[A=I_{y}\\otimes A_{x}+A_{y}\\otimes I_{x}\\]\nor:\n\\[\\frac{\\partial^{2}}{\\partial x^{2}}=\\left(\\begin{array}{cccc}\nA_{x}\\\\\n& A_{x}\\\\\n&  & \\ddots\\\\\n&  &  & A_{x}\n\\end{array}\\right)\\]\nand\n\\[\\frac{\\partial^{2}}{\\partial y^{2}}=\\left(\\begin{array}{cccc}\n-2I_{x} & I_{x}\\\\\nI_{x} & -2I_{x} & I_{x}\\\\\n&  & \\ddots\\\\\n\\\\\n\\end{array}\\right)\\]\nTo see why this is the case, understand it again as the stencil operation\n0  1 0\n1 -4 1\n0  1 0\nIn this operation, at a point you still use the up and down neighbors, and thus this has a tridiagonal form since those are the immediate neighbors, but the next \\(y\\) value is \\(N\\) over, so this is where the block tridiagonal form comes for the stencil in the \\(y\\) terms. When these are added together one receives the appropriate matrix. The Kronecker product effectively encodes this “N over” behavior. It also readily generalizes to \\(N\\) dimensions. To see this for 3-dimensional Laplacians, \\(u_{xx} + u_{yy} + u_{zz}\\), notice that\n\\[A=I_z \\otimes I_{y}\\otimes A_{x} + I_z \\otimes A_{y}\\otimes I_{x} + A_z \\otimes I_y \\otimes I_x\\]\nusing the same reasoning about “N” over and “N^2 over”, and from this formulation it’s clear how to generalize to arbitrary dimensions.\nWe note that there is an alternative representation as well for 2D forms. We can represent them with left and right matrix operations. When \\(u\\) is represented as a matrix, notice that\n\\[A(u) = A_y u + u A_x\\]\nwhere \\(A_y\\) and \\(A_x\\) are both the [1 -2 1] 1D tridiagonal stencil matrix, but by right multiplying it’s occurring along the columns and left multiplying occurs along the rows. This then gives a semi-dense formulation of the stencil operation.\n\n\n13.4.2 Implementation via Stencil Compilers\nSparse matrix implementations of stencils are fairly inefficient given the way that sparse matrices are represented (lists of (i,j,v) pairs, which are then compressed into CSR or CSC formats). However, it moves in the right direction by noticing that the operation\nu[i+1,j] + u[i,j+1] + u[i-1,j] + u[i,j-1] - 4u[i,j]\nis an inefficient way to walk through the data. The reason is because u[i,j+1] is using values that are far away from u[i,j], and thus they may not necessarily be in the cache.\nThus what is generally used is a stencil compiler which generates functions for stencil operations. These work by dividing the tensor into blocks on which the stencil is applied, where the blocks are small enough to allow the cache lines to fit the future points. This is a very deep computational topic that is beyond the scope of this course. Note one of the main reasons why NVIDA’s CUDA dominates machine learning is because of its cudnn library, which is a very efficient GPU stencil computation library that is specifically tuned to NVIDIA’s GPUs."
  },
  {
    "objectID": "pdes_and_convolutions.html#cross-discipline-learning",
    "href": "pdes_and_convolutions.html#cross-discipline-learning",
    "title": "13  PDEs, Convolutions, and the Mathematics of Locality",
    "section": "13.5 Cross-Discipline Learning",
    "text": "13.5 Cross-Discipline Learning\nGiven these relations, there is a lot each of the disciplines can learn from one another about stencil computations.\n\n13.5.1 What ML can learning from SciComp: Stability of Convolutional RNNs\nStability of time-dependent partial differential equations is a long-known problem. Stability of an RNN defined by stencil computations is then stability of Euler discretizations of PDEs. Let’s take a look at Von Neumann analysis of PDE stability.\nLet’s look at the error update equation. Write\n\\[e_{i}^{n}=u(x_{j},t_{n})-u_{j}^{n}\\]\nFor \\(e_{i}^{n}\\), as before, plug it in, add and subtract \\(u(x_{j},t^{n})=u_{j}^{n}\\), and then we get\n\\[e_{i}^{n+1}=e_{i}^{n}+\\mu\\left(e_{i+1}^{n}-2e_{i}^{n}+e_{i-1}^{n}\\right)+\\Delta t\\tau_{i}^{n}\\]\nwhere\n\\[\\tau_{i}^{n}\\sim\\mathcal{O}(\\Delta t)+\\mathcal{O}(\\Delta x^{2}).\\]\nStability requires that the homogenous equation goes to zero. Another way of saying that is that the propagation of errors has errors decrease their influence over time. Thus we look at:\n\\[e_{i}^{n+1}   =e_{i}^{n}+\\mu\\left(e_{i+1}^{n}-2e_{i}^{n}+e_{i-1}^{n}\\right) =\\left(1-2\\mu\\right)e_{i}^{n}+\\mu e_{i+1}^{n}+\\mu e_{i-1}^{n}\\]\nA necessary condition for decreasing is then for all coefficients to be positive\n\\[1-2\\mu\\geq0\\] or\n\\[\\mu\\leq\\frac{1}{2}\\]\nA more satisfying way may be to look at the generated ODE\n\\[u^{\\prime}=Au\\]\nwhere A is the matrix \\(\\left[\\mu,1-2\\mu,\\mu\\right].\\)\nBut finding the maximum eigenvalue is non-trivial. But for linear PDEs, one nice way to analyze the stability directly is to use the Fourier mode decomposition. This is known as Van Neumann stability analysis. To do this, decompose \\(U\\) into the Fourier modes:\n\\[U(x,t)=\\sum_{k}\\hat{U}(t)e^{ikx}\\]\nSince\n\\[x_{j}=j\\Delta x,\\]\nwe can write this out as\n\\[U_{j}^{n}=\\hat{U}^{n}e^{ikj\\Delta x}\\]\nand then plugging this into the FTCS scheme we get\n\\[\\frac{\\hat{U}^{n+1}e^{ikj\\Delta x}-\\hat{U}^{n}e^{ikj\\Delta x}}{\\Delta t}=\\frac{\\hat{U}^{n}e^{ik(j+1)\\Delta x}-2\\hat{U}^{n}e^{ikj\\Delta x}+\\hat{U}^{n}e^{ik(j-1)\\Delta x}}{\\Delta x^{2}}\\]\nLet G be the growth factor, defined as\n\\[G=\\frac{\\hat{U}^{n+1}}{\\hat{U}^{n}}\\]\nand thus after cancelling we get\n\\[\\frac{G-1}{\\Delta t}=\\frac{e^{ik\\Delta x}-2+e^{-ik\\Delta x}}{\\Delta x^{2}}\\]\nSince\n\\[e^{ik\\Delta x}+e^{-ik\\Delta x}=2\\cos\\left(k\\Delta x\\right),\\]\nthen we get\n\\[G=1-\\mu\\left(2\\cos\\left(k\\Delta x\\right)-2\\right)\\]\nand using the half angle formula\n\\[G=1-4\\mu\\sin^{2}\\left(\\frac{k\\Delta x}{2}\\right)\\]\nIn order to be stable, we require\n\\[\\left|G\\right|\\leq1,\\]\nwhich means\n\\[-1\\leq1-4\\mu\\sin^{2}\\left(\\frac{k\\Delta x}{2}\\right)\\leq1 \\mu&gt;0\\]\nand so \\(\\leq1\\) is simple. Since \\(\\sin^{2}(x)\\leq1\\), then we can simplify this to\n\\[-1\\leq1-4\\mu\\]\nand thus \\(\\mu\\leq\\frac{1}{2}\\). With backwards Euler we get\n\\[\\frac{G-1}{\\Delta t}=\\frac{G}{\\Delta x^{2}}\\left(e^{ik\\Delta x}-2+e^{-ik\\Delta x}\\right)\\]\nand thus get\n\\[G+4G\\mu\\sin^{2}\\left(\\frac{k\\Delta x}{2}\\right)=1\\]\nand thus\n\\[G=\\frac{1}{1+4\\mu\\sin^{2}\\left(\\frac{k\\Delta x}{2}\\right)}\\leq1.\\]\n\n\n13.5.2 What SciComp can learn from ML: Moderate Generalizations to Partial Differential Equation Models\nInstead of using\n\\[\\Delta u = f\\]\nwe can start with\n\\[S[u] = f\\]\na stencil computation, predefined to match a known partial differential equation operator, and then transfer learn the stencil to better match data. This is an approach which is starting to move down the lines of physics-informed machine learning that will be further explored in future lectures."
  },
  {
    "objectID": "diffeq_machine_learning.html#youtube-video",
    "href": "diffeq_machine_learning.html#youtube-video",
    "title": "14  Mixing Differential Equations and Neural Networks for Physics-Informed Learning",
    "section": "14.1 Youtube Video",
    "text": "14.1 Youtube Video\nGiven this background in both neural network and differential equation modeling, let’s take a moment to survey some methods which integrate the two ideas. In this course we have fully described how Physics-Informed Neural Networks (PINNs) and neural ordinary differential equations are both trained and used. There are many other methods which utilize the composition of these ideas.\nJulia codes for these methods are being developed, optimized, and tested in the SciML organization. Some packages to note are\n\nNeuralPDE.jl\nDiffEqFlux.jl\nDataDrivenDiffEq.jl\nSurrogates.jl\nReservoirComputing.jl\n\nand many more collaborations with scientists around the world (too many to note). And there are some scattered packages in other languages to note too, such as:\n\ndeepxde\npysindy\nADCME.jl\n\nand many more. This lecture is a quick survey on different directions that people have taken so far in this field. It is by no means comprehensive."
  },
  {
    "objectID": "diffeq_machine_learning.html#the-augmented-neural-ordinary-differential-equation",
    "href": "diffeq_machine_learning.html#the-augmented-neural-ordinary-differential-equation",
    "title": "14  Mixing Differential Equations and Neural Networks for Physics-Informed Learning",
    "section": "14.2 The Augmented Neural Ordinary Differential Equation",
    "text": "14.2 The Augmented Neural Ordinary Differential Equation\nNote that not every function can be represented by an ordinary differential equation. Specifically, \\(u(t)\\) is an \\(\\mathbb{R} \\rightarrow \\mathbb{R}^n\\) function which cannot loop over itself except when the solution is cyclic. The reason is because the flow of the ODE’s solution is unique from every time point, and for it to have “two directions” at a point \\(u_i\\) in phase space would have two solutions to the problem\n\\[u' = f(u,p,t)\\]\nwhere \\(u(0)=u_i\\), and thus this cannot happen (with \\(f\\) sufficiently nice). However, if we have another degree of freedom we can ensure that the ODE does not overlap with itself. This is the augmented neural ordinary differential equation.\nWe only need one degree of freedom in order to not collide, so we can do the following. We can add a fake state to the ODE which is zero at every single data point. This then allows this extra dimension to “bump around” as necessary to let the function be a universal approximator. In code this looks like:\n\ndudt = Chain(...) # Flux neural network\np,re = Flux.destructure(dudt)\ndudt_(u,p,t) = re(p)(u)\nprob = ODEProblem(dudt_,[u0,0f0],tspan,p)\naugmented_data = vcat(ode_data,zeros(1,size(ode_data,2)))"
  },
  {
    "objectID": "diffeq_machine_learning.html#extensions-to-other-differential-equations",
    "href": "diffeq_machine_learning.html#extensions-to-other-differential-equations",
    "title": "14  Mixing Differential Equations and Neural Networks for Physics-Informed Learning",
    "section": "14.3 Extensions to other Differential Equations",
    "text": "14.3 Extensions to other Differential Equations\nWhile our previous lectures focused on ordinary differential equations, the larger classes of differential equations can also have neural networks, for example:\n\nstochastic differential equations\ndelay differential equations\npartial differential equations\njump stochastic differential equations\nHybrid differential equations (DEs with event handling)\n\nFor each of these equations, one can come up with an adjoint definition in order to define a backpropagation, or perform direct automatic differentiation of the solver code. One such paper in this area includes neural stochastic differential equations\n\n14.3.1 The Universal Ordinary Differential Equation\nThis formulation of the neural differential equation in terms of a “knowledge-embedded” structure is leading. If we already knew something about the differential equation, could we use that information in the differential equation definition itself? This leads us to the idea of the universal differential equation, which is a differential equation that embeds universal approximators in its definition to allow for learning arbitrary functions as pieces of the differential equation.\nThe best way to describe this object is to code up an example. As our example, let’s say that we have a two-state system and know that the second state is defined by a linear ODE. This means we want to write:\n\\[x' = NN(x,y)\\] \\[y' = p_1 x + p_2 y\\]\nWe can code this up as follows:\n\nu0 = Float32[0.8; 0.8]\ntspan = (0.0f0,25.0f0)\n\nann = Chain(Dense(2,10,tanh), Dense(10,1))\n\np1,re = Flux.destructure(ann)\np2 = Float32[-2.0,1.1]\np3 = [p1;p2]\nps = Flux.params(p3)\n\nfunction dudt_(du,u,p,t)\n    x, y = u\n    du[1] = re(p[1:41])(u)[1]\n    du[2] = p[end-1]*y + p[end]*x\nend\nprob = ODEProblem(dudt_,u0,tspan,p3)\nconcrete_solve(prob,Tsit5(),u0,p3,abstol=1e-8,reltol=1e-6)\n\nand we can train the system to be stable at 1 as follows:\n\nfunction predict_adjoint()\n  Array(concrete_solve(prob,Tsit5(),u0,p3,saveat=0.0:0.1:25.0))\nend\nloss_adjoint() = sum(abs2,x-1 for x in predict_adjoint())\nloss_adjoint()\n\ndata = Iterators.repeated((), 300)\nopt = ADAM(0.01)\niter = 0\ncb = function ()\n  global iter += 1\n  if iter % 50 == 0\n    display(loss_adjoint())\n    display(plot(solve(remake(prob,p=p3,u0=u0),Tsit5(),saveat=0.1),ylim=(0,6)))\n  end\nend\n\n# Display the ODE with the current parameter values.\ncb()\n\nFlux.train!(loss_adjoint, ps, data, opt, cb = cb)\n\nDiffEqFlux.jl supports the wide gambit of possible universal differential equations with combinations of stiffness, delays, stochasticity, etc. It does so by using Julia’s language-wide AD tooling, such as ReverseDiff.jl, Tracker.jl, ForwardDiff.jl, and Zygote.jl, along with specializations available whenever adjoint methods are known (and the choice between the two is given to the user).\nMany of the methods below can be encapsulated as a choice of a universal differential equation and trained with higher order, adaptive, and more efficient methods with DiffEqFlux.jl."
  },
  {
    "objectID": "diffeq_machine_learning.html#deep-bsde-methods-for-high-dimensional-partial-differential-equations",
    "href": "diffeq_machine_learning.html#deep-bsde-methods-for-high-dimensional-partial-differential-equations",
    "title": "14  Mixing Differential Equations and Neural Networks for Physics-Informed Learning",
    "section": "14.4 Deep BSDE Methods for High Dimensional Partial Differential Equations",
    "text": "14.4 Deep BSDE Methods for High Dimensional Partial Differential Equations\nThe key paper on deep BSDE methods is this article from PNAS by Jiequn Han, Arnulf Jentzen, and Weinan E. Follow up papers like this one have identified a larger context in the sense of forward-backwards SDEs for a large class of partial differential equations.\n\n14.4.1 Understanding the Setup for Terminal PDEs\nWhile this setup may seem a bit contrived given the “very specific” partial differential equation form (you know the end value? You have some parabolic form?), it turns out that there is a large class of problems in economics and finance that satisfy this form. The reason is because in these problems you may know the value of something at the end, when you’re going to sell it, and you want to evaluate it right now. The classic example is in options pricing. An option is a contract to be able to solve a stock at a given value. The simplest case is a contract that can only be executed at a pre-determined time in the future. Let’s say we have an option to sell a stock at 100 no matter what. This means that, if the stock at the strike time (the time the option can be sold) is 70, we will make 30 from this option, and thus the option itself is worth 30. The question is, if I have this option today, the strike time is 3 months in the future, and the stock price is currently 70, how much should I value the option today?\nTo solve this, we need to put a model on how we think the stock price will evolve. One simple version is a linear stochastic differential equation, i.e. the stock price will evolve with a constant interest rate \\(r\\) with some volatility (randomness) \\(\\sigma\\), in which case:\n\\[dX_t = r X_t dt + \\sigma X_t dW_t.\\]\nFrom this model, we can evaluate the probability that the stock is going to be at given values, which then gives us the probability that the option is worth a given value, which then gives us the expected (or average) value of the option. This is the Black-Scholes problem. However, a more direct way of calculating this result is writing down a partial differential equation for the evolution of the value of the option \\(V\\) as a function of time \\(t\\) and the current stock price \\(x\\). At the final time point, if we know the stock price then we know the value of the option, and thus we have a terminal condition \\(V(T,x) = g(x)\\) for some known value function \\(g(x)\\). The question is, given this value at time \\(T\\), what is the value of the option at time \\(t=0\\) given that the stock currently has a value \\(x = \\zeta\\). Why is this interesting? This will tell you what you think the option is currently valued at, and thus if it’s cheaper than that, you can gain money by buying the option right now! This means that the “solution” to the PDE is the value \\(V(0,\\zeta)\\), where we know the final points \\(V(T,x) = g(x)\\). This is precisely the type of problem that is solved by the deep BSDE method.\n\n\n14.4.2 The Deep BSDE Method\nConsider the class of semilinear parabolic PDEs, in finite time \\(t\\in[0, T]\\) and \\(d\\)-dimensional space \\(x\\in\\mathbb R^d\\), that have the form\n\\[\\begin{align}\n  \\frac{\\partial u}{\\partial t}(t,x)    &+\\frac{1}{2}\\text{trace}\\left(\\sigma\\sigma^{T}(t,x)\\left(\\text{Hess}_{x}u\\right)(t,x)\\right)\\\\\n    &+\\nabla u(t,x)\\cdot\\mu(t,x) \\\\\n    &+f\\left(t,x,u(t,x),\\sigma^{T}(t,x)\\nabla u(t,x)\\right)=0,\\end{align}\\]\nwith a terminal condition \\(u(T,x)=g(x)\\). In this equation, \\(\\text{trace}\\) is the trace of a matrix, \\(\\sigma^T\\) is the transpose of \\(\\sigma\\), \\(\\nabla u\\) is the gradient of \\(u\\), and \\(\\text{Hess}_x u\\) is the Hessian of \\(u\\) with respect to \\(x\\). Furthermore, \\(\\mu\\) is a vector-valued function, \\(\\sigma\\) is a \\(d \\times d\\) matrix-valued function and \\(f\\) is a nonlinear function. We assume that \\(\\mu\\), \\(\\sigma\\), and \\(f\\) are known. We wish to find the solution at initial time, \\(t=0\\), at some starting point, \\(x = \\zeta\\).\nLet \\(W_{t}\\) be a Brownian motion and take \\(X_t\\) to be the solution to the stochastic differential equation\n\\[dX_t = \\mu(t,X_t) dt + \\sigma (t,X_t) dW_t\\]\nwith initial condition \\(X(0)=\\zeta\\). Previous work has shown that the solution satisfies the following BSDE:\n\\[\\begin{align}\nu(t, &X_t) - u(0,\\zeta) = \\\\\n& -\\int_0^t f(s,X_s,u(s,X_s),\\sigma^T(s,X_s)\\nabla u(s,X_s)) ds \\\\\n& + \\int_0^t \\left[\\nabla u(s,X_s) \\right]^T \\sigma (s,X_s) dW_s,\\end{align}\\]\nwith terminating condition \\(g(X_T) = u(X_T,W_T)\\).\nAt this point, the authors approximate \\(\\left[\\nabla u(s,X_s) \\right]^T \\sigma (s,X_s)\\) and \\(u(0,\\zeta)\\) as neural networks. Using the Euler-Maruyama discretization of the stochastic differential equation system, one arrives at a recurrent neural network:\n\n\n\nDeep BSDE\n\n\n\n\n14.4.3 Julia Implementation\nA Julia implementation for the deep BSDE method can be found at NeuralPDE.jl. The examples considered below are part of the standard test suite.\n\n\n14.4.4 Financial Applications of Deep BSDEs: Nonlinear Black-Scholes\nNow let’s look at a few applications which have PDEs that are solved by this method. One set of problems that are solved, given our setup, are Black-Scholes types of equations. Unlike a lot of previous literature, this works for a wide class of nonlinear extensions to Black-Scholes with large portfolios. Here, the dimension of the PDE for \\(V(t,x)\\) is the dimension of \\(x\\), where the dimension is the number of stocks in the portfolio that we want to consider. If we want to track 1000 stocks, this means our PDE is 1000 dimensional! Traditional PDE solvers would need around \\(N^{1000}\\) points evolving over time in order to arrive at the solution, which is completely impractical.\nOne example of a nonlinear Black-Scholes equation in this form is the Black-Scholes equation with default risk. Here we are adding to the standard model the idea that the companies that we are buying stocks for can default, and thus our valuation has to take into account this default probability as the option will thus become value-less. The PDE that is arrived at is:\n\\[\\frac{\\partial u}{\\partial t}(t,x) + \\bar{\\mu}\\cdot \\nabla u(t, x) + \\frac{\\bar{\\sigma}^{2}}{2} \\sum_{i=1}^{d} \\left |x_{i}  \\right |^{2} \\frac{\\partial^2 u}{\\partial {x_{i}}^2}(t,x) \\\\ - (1 -\\delta )Q(u(t,x))u(t,x) - Ru(t,x) = 0\\]\nwith terminating condition \\(g(x) = \\min_{i} x_i\\) for \\(x = (x_{1}, . . . , x_{100}) \\in R^{100}\\), where \\(\\delta \\in [0, 1)\\), \\(R\\) is the interest rate of the risk-free asset, and Q is a piecewise linear function of the current value with three regions \\((v^{h} &lt; v ^{l}, \\gamma^{h} &gt; \\gamma^{l})\\),\n\\[\\begin{align}\nQ(y) &= \\mathbb{1}_{(-\\infty,\\upsilon^{h})}(y)\\gamma ^{h}\n+ \\mathbb{1}_{[\\upsilon^{l},\\infty)}(y)\\gamma ^{l}\n\\\\ &+ \\mathbb{1}_{[\\upsilon^{h},\\upsilon^{l}]}(y)\n\\left[ \\frac{(\\gamma ^{h} - \\gamma ^{l})}{(\\upsilon ^{h}- \\upsilon ^{l})}\n(y - \\upsilon ^{h}) + \\gamma ^{h}  \\right  ].\n\\end{align}\\]\nThis PDE can be cast into the form of the deep BSDE method by setting:\n\\[\\begin{align}\n    \\mu &= \\overline{\\mu} X_{t} \\\\\n    \\sigma &= \\overline{\\sigma} \\text{diag}(X_{t}) \\\\\n    f &= -(1 -\\delta )Q(u(t,x))u(t,x) - R u(t,x)\n\\end{align}\\]\nThe Julia code for this exact problem in 100 dimensions can be found here\n\n\n14.4.5 Stochastic Optimal Control as a Deep BSDE Application\nAnother type of problem that fits into this terminal PDE form is the stochastic optimal control problem. The problem is a generalized context to what motivated us before. In this case, there are a set of agents which undergo some known stochastic model. What we want to do is apply some control (push them in some direction) at every single timepoint towards some goal. For example, we have the physics for the dynamics of drone flight, but there’s randomness in the wind condition, and so we want to control the engine speeds to move in a certain direction. However, there is a cost associated with controlling, and thus the question is how to best balance the use of controls with the natural stochastic evolution.\nIt turns out this is in the same form as the Black-Scholes problem. There is a model evolving forwards, and when we get to the end we know how much everything “cost” because we know if the drone got to the right location and how much energy it took. So in the same sense as Black-Scholes, we can know the value at the end and try and propagate it backwards given the current state of the system \\(x\\), to find out \\(u(0,\\zeta)\\), i.e. how should we control right now given the current system is in the state \\(x = \\zeta\\). It turns out that the solution of \\(u(t,x)\\) where \\(u(T,x)=g(x)\\) and we want to find \\(u(0,\\zeta)\\) is given by a partial differential equation which is known as the Hamilton-Jacobi-Bellman equation, which is one of these terminal PDEs that is representable by the deep BSDE method.\nTake the classical linear-quadratic Gaussian (LQG) control problem in 100 dimensions\n\\[dX_t = 2\\sqrt{\\lambda} c_t dt + \\sqrt{2} dW_t\\]\nwith \\(t\\in [0,T]\\), \\(X_0 = x\\), and with a cost function\n\\[C(c_t) = \\mathbb{E}\\left[\\int_0^T \\Vert c_t \\Vert^2 dt + g(X_t) \\right]\\]\nwhere \\(X_t\\) is the state we wish to control, \\(\\lambda\\) is the strength of the control, and \\(c_t\\) is the control process. To minimize the control, the Hamilton–Jacobi–Bellman equation:\n\\[\\frac{\\partial u}{\\partial t}(t,x) + \\Delta u(t,x) - \\lambda \\Vert \\nabla u(t,x) \\Vert^2 = 0\\]\nhas a solution \\(u(t,x)\\) which at \\(t=0\\) represents the optimal cost of starting from \\(x\\).\nThis PDE can be rewritten into the canonical form of the deep BSDE method by setting:\n\\[\\begin{align}\n    \\mu &= 0, \\\\\n    \\sigma &= \\overline{\\sigma} I, \\\\\n    f &= -\\alpha \\left \\| \\sigma^T(s,X_s)\\nabla u(s,X_s)) \\right \\|^{2},\n\\end{align}\\]\nwhere \\(\\overline{\\sigma} = \\sqrt{2}\\), T = 1 and \\(X_0 = (0,. . . , 0) \\in R^{100}\\).\nThe Julia code for solving this exact problem in 100 dimensions can be found here"
  },
  {
    "objectID": "diffeq_machine_learning.html#connections-of-reservoir-computing-to-scientific-machine-learning",
    "href": "diffeq_machine_learning.html#connections-of-reservoir-computing-to-scientific-machine-learning",
    "title": "14  Mixing Differential Equations and Neural Networks for Physics-Informed Learning",
    "section": "14.5 Connections of Reservoir Computing to Scientific Machine Learning",
    "text": "14.5 Connections of Reservoir Computing to Scientific Machine Learning\nReservoir computing techniques are an alternative to the “full” neural network techniques we have previously discussed. However, the process of training neural networks has a few caveats which can cause difficulties in real systems:\n\nThe tangent space diverges exponentially fast when the system is chaotic, meaning that results of both forward and reverse automatic differentiation techniques (and the related adjoints) are divergent on these kinds of systems.\nIt is hard for neural networks to represent stiff systems. There are many reasons for this, one being that neural networks tend to drop high frequency behavior.\n\nThere are ways being investigated to alleviate these issues. For example, shadow adjoints can give a non-divergent average sense of a derivative on ergodic chaotic systems, but is significantly more expensive than the traditional adjoint.\nTo get around these caveats, some research teams have investigated alternatives which do not require gradient-based optimization. The clear frontrunner in this field is a type of architecture called echo state networks. A simplified formulation of an echo state network essentially fixes a neural network that defines a reservoir, i.e.\n\\[x_{n+1} = \\sigma(W x_n + W_{fb} y_n)\\] \\[y_n = g(W_{out} x_n)\\]\nwhere \\(W\\) and \\(W_{fb}\\) are fixed random matrices that are chosen before the training process, \\(x_n\\) is called the reservoir state, and \\(y_n\\) is the output state for the observables. The idea is to find a projection \\(W_{out}\\) from the high dimensional random reservoir \\(x\\) to model the timeseries by \\(y\\). If the reservoir is a big enough and nonlinear enough random system, there should in theory exist a projection from that random system that matches any potential timeseries. Indeed, one can prove that echo state networks are universal adaptive filters under certain conditions.\nIf \\(g\\) is invertible (and in many cases \\(g\\) is taken to be the identity), then one can directly apply the inversion of \\(g\\) to the data. This turns the training of \\(W_{out}\\), the only non-fixed portion, into a standard least squares regression between the reservoir and the observation series. This is then solved by classical means like SVD factorizations which can be stable in ill-conditioned cases.\nEcho state networks have been shown to accurately reproduce chaotic attractors which are shown to be hard to train RNNs against. A demonstration via ReservoirComputing.jl clearly highlights this prediction ability:\n \nHowever, this methodology still is not tailored to the continuous nature of dynamical systems found in scientific computing. Recent work has extended this methodolgy to allow for a continuous reservoir, i.e. a continuous-time echo state network. It is shown that using the adaptive points of a stiff ODE integrator gives a non-uniform sampling in time that makes it easier to learn stiff equations from less training points, and demonstrates the ability to learn equations where standard physics-informed neural network (PINN) training techniques fail.\n\nThis area of research is still far less developed than PINNs and neural differential equations but shows promise to more easily learn highly stiff and chaotic systems which are seemingly out of reach for these other methods."
  },
  {
    "objectID": "diffeq_machine_learning.html#automated-equation-discovery-outputting-latex-for-dynamical-systems-from-data",
    "href": "diffeq_machine_learning.html#automated-equation-discovery-outputting-latex-for-dynamical-systems-from-data",
    "title": "14  Mixing Differential Equations and Neural Networks for Physics-Informed Learning",
    "section": "14.6 Automated Equation Discovery: Outputting LaTeX for Dynamical Systems from Data",
    "text": "14.6 Automated Equation Discovery: Outputting LaTeX for Dynamical Systems from Data\nThe SINDy algorithm enables data-driven discovery of governing equations from data. It leverages the fact that most physical systems have only a few relevant terms that define the dynamics, making the governing equations sparse in a high-dimensional nonlinear function space. Given a set of observations\n\\[\\begin{array}{c}\n\\mathbf{X}=\\left[\\begin{array}{c}\n\\mathbf{x}^{T}\\left(t_{1}\\right) \\\\\n\\mathbf{x}^{T}\\left(t_{2}\\right) \\\\\n\\vdots \\\\\n\\mathbf{x}^{T}\\left(t_{m}\\right)\n\\end{array}\\right]=\\left[\\begin{array}{cccc}\nx_{1}\\left(t_{1}\\right) & x_{2}\\left(t_{1}\\right) & \\cdots & x_{n}\\left(t_{1}\\right) \\\\\nx_{1}\\left(t_{2}\\right) & x_{2}\\left(t_{2}\\right) & \\cdots & x_{n}\\left(t_{2}\\right) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{1}\\left(t_{m}\\right) & x_{2}\\left(t_{m}\\right) & \\cdots & x_{n}\\left(t_{m}\\right)\n\\end{array}\\right] \\\\\n\\end{array}\\]\nand a set of derivative observations\n\\[\\begin{array}{c}\n\\dot{\\mathbf{X}}=\\left[\\begin{array}{c}\n\\mathbf{x}^{T}\\left(t_{1}\\right) \\\\\n\\dot{\\mathbf{x}}^{T}\\left(t_{2}\\right) \\\\\n\\vdots \\\\\n\\mathbf{x}^{T}\\left(t_{m}\\right)\n\\end{array}\\right]=\\left[\\begin{array}{cccc}\n\\dot{x}_{1}\\left(t_{1}\\right) & \\dot{x}_{2}\\left(t_{1}\\right) & \\cdots & \\dot{x}_{n}\\left(t_{1}\\right) \\\\\n\\dot{x}_{1}\\left(t_{2}\\right) & \\dot{x}_{2}\\left(t_{2}\\right) & \\cdots & \\dot{x}_{n}\\left(t_{2}\\right) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\dot{x}_{1}\\left(t_{m}\\right) & \\dot{x}_{2}\\left(t_{m}\\right) & \\cdots & \\dot{x}_{n}\\left(t_{m}\\right)\n\\end{array}\\right]\n\\end{array}\\]\nwe can evaluate the observations in a basis \\(\\Theta(X)\\):\n\\[\\Theta(\\mathbf{X})=\\left[\\begin{array}{llllllll}\n1 & \\mathbf{X} & \\mathbf{X}^{P_{2}} & \\mathbf{X}^{P_{3}} & \\cdots & \\sin (\\mathbf{X}) & \\cos (\\mathbf{X}) & \\cdots\n\\end{array}\\right]\\]\nwhere \\(X^{P_i}\\) stands for all \\(P_i\\)th order polynomial terms. For example,\n\\[\\mathbf{X}^{P_{2}}=\\left[\\begin{array}{cccccc}\nx_{1}^{2}\\left(t_{1}\\right) & x_{1}\\left(t_{1}\\right) x_{2}\\left(t_{1}\\right) & \\cdots & x_{2}^{2}\\left(t_{1}\\right) & \\cdots & x_{n}^{2}\\left(t_{1}\\right) \\\\\nx_{1}^{2}\\left(t_{2}\\right) & x_{1}\\left(t_{2}\\right) x_{2}\\left(t_{2}\\right) & \\cdots & x_{2}^{2}\\left(t_{2}\\right) & \\cdots & x_{n}^{2}\\left(t_{2}\\right) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\nx_{1}^{2}\\left(t_{m}\\right) & x_{1}\\left(t_{m}\\right) x_{2}\\left(t_{m}\\right) & \\cdots & x_{2}^{2}\\left(t_{m}\\right) & \\cdots & x_{n}^{2}\\left(t_{m}\\right)\n\\end{array}\\right]\\]\nUsing these matrices, SINDy finds this sparse basis \\(\\mathbf{\\Xi}\\) over a given candidate library \\(\\mathbf{\\Theta}\\) by solving the sparse regression problem \\(\\dot{X} =\\mathbf{\\Theta}\\mathbf{\\Xi}\\) with \\(L_1\\) regularization, i.e. minimizing the objective function \\(\\left\\Vert \\mathbf{\\dot{X}} - \\mathbf{\\Theta}\\mathbf{\\Xi} \\right\\Vert_2 + \\lambda \\left\\Vert \\mathbf{\\Xi}\\right\\Vert_1\\). This method and other variants of SInDy, along with specialized optimizers for the LASSO \\(L_1\\) optimization problem, have been implemented in packages like DataDrivenDiffEq.jl and pysindy. The result of these methods is LaTeX for the missing dynamical system.\nNotice that to use this method, derivative data \\(\\dot{X}\\) is required. While in most publications on the subject this information is assumed. To find this, \\(\\dot{X}\\) is calculated directly from the time series \\(X\\) by fitting a cubic spline and taking the approximated derivatives at the observation points. However, for this estimation to be stable one needs a fairly dense timeseries for the interpolation. To alleviate this issue, the universal differential equations work estimates terms of partially described models and then uses the neural network as an oracle for the derivative values to learn from subsets of the dynamical system. This allows for the neural network’s training to smooth out the derivative estimate between points while incorporating extra scientific information.\nOther ways are being investigated for incorporating deep learning into the model discovery process. For example, extensions have been investigated where elements are defined by neural networks representing a basis of the Koopman operator. Additionally, much work is going on in improving the efficiency of the symbolic regression methods themselves, and making the methods implicit and parallel."
  },
  {
    "objectID": "diffeq_machine_learning.html#surrogate-acceleration-methods",
    "href": "diffeq_machine_learning.html#surrogate-acceleration-methods",
    "title": "14  Mixing Differential Equations and Neural Networks for Physics-Informed Learning",
    "section": "14.7 Surrogate Acceleration Methods",
    "text": "14.7 Surrogate Acceleration Methods\nAnother approach for mixing neural networks with differential equations is as a surrogate method. These methods are more mathematically trivial than the previous ideas, but can still achieve interesting results. A full example is explained in this video.\nSay we have some function \\(g(p)\\) which depends on a solution to a differential equation \\(u(t;p)\\) and choices of parameters \\(p\\). Computationally how we evaluate this function is we do the following:\n\nSolve the differential equation with parameters \\(p\\)\nEvaluate \\(g\\) on the numerical solution for \\(u\\)\n\nHowever, this process is computationally expensive since it requires the numerical solution of \\(u\\) for every evaluation. Thus, one can look at this setup and see \\(g(p)\\) itself is a nonlinear function. The idea is to train a neural network to be the function \\(g(p)\\), i.e. directly put in \\(p\\) and return the appropriate value without ever solving the differential equation.\nThe video highlights an important fact about this method: it can be computationally expensive to train this kind of surrogate since many data points \\((p,g(p))\\) are required. In fact, many more data points than you might use. However, after training, the surrogate network for \\(g(p)\\) can be a lot faster than the original simulation-based approach. This means that this is a method for accelerating real-time solutions by doing upfront computations. The total compute time will always be more, but in some sense the cost is amortized or shifted to be done before hand, so that the model does not need to be simulated on the fly. This can allow for things like computationally expensive models of drone flight to be used in a real-time controller.\nThis technique goes a long way back, but some recent examples of this have been shown. For example, there’s this paper which “accelerated” the solution of the 3-body problem using a neural network surrogate trained over a few days to get a 1 million times acceleration (after generating many points beforehand of course! In the paper, notice that it took 10 days to generate the training dataset). Additionally, there is this deep learning trebuchet example which showcased that inverse problems, i.e. control or finding parameters, can be completely encapsulated as a \\(g(p)\\) and learned with sufficient data."
  },
  {
    "objectID": "probabilistic_programming.html#youtube-video",
    "href": "probabilistic_programming.html#youtube-video",
    "title": "15  From Optimization to Probabilistic Programming",
    "section": "15.1 Youtube Video",
    "text": "15.1 Youtube Video\nWith a high degree of probability, all things are probabilistic. In all of the cases we have previously looked at (differential equations, neural networks, neural differential equations, physics-informed neural networks, etc.) we have incorporated data into our models using point estimates, i.e. getting “exact fits”. However, data has noise and uncertainty. We want to extend our previous modeling approaches to include probabilistic estimates. This is known as probabilistic programming, or Bayesian estimation on general programming models. To approach this topic, we will first introduce the Bayesian way of thinking about variables as random variables, estimating probabilistic programs, and how efficient probabilistic programming frameworks incorporate differentiable programming."
  },
  {
    "objectID": "probabilistic_programming.html#bayesian-modeling-in-a-nutshell",
    "href": "probabilistic_programming.html#bayesian-modeling-in-a-nutshell",
    "title": "15  From Optimization to Probabilistic Programming",
    "section": "15.2 Bayesian Modeling in a Nutshell",
    "text": "15.2 Bayesian Modeling in a Nutshell\nThe idea of Bayesian modeling is to treat your variables as a random variable with respect to some distribution. As a starting point, think about the linear model\n\\[f(x) = ax\\]\nThe standard way to think of the linear model is that \\(a\\) is a variable, and so you put a value \\(x\\) in and compute \\(ax\\). However, in the Bayesian sense, the value of \\(a\\) can be a random variable. A random variable \\(Z\\) is a variable which has probability of taking certain values from a probability distribution. If we say that \\(Z \\sim f(y)\\), then we are saying that the probability that \\(Z\\) takes a value in the set \\(\\Omega\\) is:\n\\[\\int_\\Omega f(y)dy\\]\nFor example, if \\(Z\\) is a scalar, then the probability that \\(Z \\in [0,1]\\) is:\n\\[\\int_0^1 f(y)dy\\]\nDiscrete probability distributions can be handled by either using distribution quantities and measures in the integral, or by simply saying \\(f(y)\\) is the probability that \\(Z = y\\).\nGiven this representation of variables, \\(ax\\) where \\(a\\) follows a probability distribution induces a probability distribution on \\(f(x)\\). To numerically acquire this distribution, one can use Monte Carlo sampling. This is simply the repeat process of:\n\nSample variables\nCompute output\n\nDoing this repeatedly then produces samples of \\(f(x)\\) from which a numerical representation of the distribution can be had. From there, going to a multivariable linear model like \\(f(x) = Ax\\) is the same idea. Going to \\(f(x)\\) where \\(f\\) is an arbitrary program is still the same idea: sample every variable in the program, compute the output, and repeat for many samples. \\(f\\) can be a neural network where all of the parameters are probabilistic, or it can be an ODE solver with probabilistic parameters."
  },
  {
    "objectID": "probabilistic_programming.html#quick-example",
    "href": "probabilistic_programming.html#quick-example",
    "title": "15  From Optimization to Probabilistic Programming",
    "section": "15.3 Quick Example",
    "text": "15.3 Quick Example\nLet’s do a quick example with the Lotka-Volterra equations. Recall that this is the ordinary differential equation defined by the following system:\n\nusing OrdinaryDiffEq, Plots\nfunction lotka_volterra(du,u,p,t)\n  du[1] = p[1] * u[1] - p[2] * u[1]*u[2]\n  du[2] = -p[3] * u[2] + p[4] * u[1]*u[2]\nend\nθ = [1.5,1.0,3.0,1.0]\nu0 = [1.0;1.0]\ntspan = (0.0,10.0)\nprob1 = ODEProblem(lotka_volterra,u0,tspan,θ)\nsol = solve(prob1,Tsit5())\nplot(sol)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow let’s assume that the θ’s are random. With Julia we can make variables into random variables by using Distributions.jl:\n\nusing Distributions\nθ = [Uniform(0.5,1.5),Beta(5,1),Normal(3,0.5),Gamma(5,2)]\n\n4-element Vector{Distribution{Univariate, Continuous}}:\n Uniform{Float64}(a=0.5, b=1.5)\n Beta{Float64}(α=5.0, β=1.0)\n Normal{Float64}(μ=3.0, σ=0.5)\n Gamma{Float64}(α=5.0, θ=2.0)\n\n\nfrom which we can sample points and propagate through the solver:\n\n_θ = rand.(θ)\nprob1 = ODEProblem(lotka_volterra,u0,tspan,_θ)\nsol = solve(prob1,Tsit5())\nplot(sol)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nand from which we can get an ensemble of solutions:\n\nprob_func = function (prob,i,repeat)\n  remake(prob,p=rand.(θ))\nend\nensemble_prob = EnsembleProblem(ODEProblem(lotka_volterra,u0,tspan,θ),\n                                prob_func = prob_func)\nsol = solve(ensemble_prob,Tsit5(),EnsembleThreads(),trajectories=1000)\n\nusing DiffEqBase.EnsembleAnalysis\nplot(EnsembleSummary(sol))\n\n┌ Warning: Using arrays or dicts to store parameters of different types can hurt performance.\n│ Consider using tuples instead.\n└ @ SciMLBase ~/.julia/packages/SciMLBase/szsYq/src/performance_warnings.jl:32\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom just a few variables having probabilities, every variable has an induced probability: there is a probability distribution on the integrator states, the output at time t_i, etc."
  },
  {
    "objectID": "probabilistic_programming.html#bayesian-estimation-with-point-estimates-bayes-rule-maximum-likelihood-and-map",
    "href": "probabilistic_programming.html#bayesian-estimation-with-point-estimates-bayes-rule-maximum-likelihood-and-map",
    "title": "15  From Optimization to Probabilistic Programming",
    "section": "15.4 Bayesian Estimation with Point Estimates: Bayes’ Rule, Maximum Likelihood, and MAP",
    "text": "15.4 Bayesian Estimation with Point Estimates: Bayes’ Rule, Maximum Likelihood, and MAP\nRecall from our previous studies that the difficult part of modeling is not necessarily the forward modeling approach, rather it’s the incorporation of data or the estimation problem that is difficult. When your variables are now random distributions, how do you “fit” them?\nThe answer comes from Bayes’ rule, which is the following. Assume you had a prior distribution \\(p(\\theta)\\) for the probability that \\(X\\) is a given value \\(\\theta\\). Then the posterior probability distribution, \\(p(\\theta|D)\\), or the distribution which is updated to include data, is given by:\n\\[p(\\theta|D) = \\frac{p(D|\\theta)p(\\theta)}{\\int_\\Omega p(D|\\theta)p(\\theta)d\\theta}\\]\nThe scaling factor on the denominator is simply a constant to make the distribution integrate 1 (so that the resulting function is a probability distribution!). The numerator is simply the prior distribution multiplied by the likelihood of seeing the data given the value of the random variable. The prior distribution must be given but notice that the likelihood has another name: the likelihood is the model.\nThe reason why it’s the same thing is because the model is what tells you the expected outcomes given a value of the random variable, and your data is on an expected outcome! However, the likelihood encodes a little bit more information in that it again is a distribution and not a point estimate. We need to make a choice for our measurement distribution on our model’s results.\n\n15.4.0.1 Quick Question: Why is this referred to as measurement noise? Why is it not process noise?\nA common choice for the measurement distribution is the Normal distribution. This comes from the Central Limit Theorem (CLT) which essentially states that, given enough interacting mechanisms, the average values of things “tend to become normally distributed”. The true statement of the CLT is much more complex, but that is a decent working definition for practical use. The normal distribution is defined by two parameters, \\(\\mu\\) and \\(\\sigma\\), and is given by the following function:\n\\[f(x;\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp(\\frac{-(x-\\mu)^2}{2\\sigma^2})\\]\nThis is a bell curve centered at \\(\\mu\\) with a variance of \\(\\sigma\\). Our best guess for the output, i.e. the model’s prediction, should be the average measurement, meaning that \\(\\mu\\) is the result from the simulator. \\(\\sigma\\) is a parameter for how much measurement error we expect (some intuition on \\(\\sigma\\) will come soon).\nLet’s return to thinking about the ODE example. In this case we have \\(\\theta\\) as a vector of random variables. This means that \\(u(t;\\theta)\\) is a random variable for the ODE \\(u'= ...\\)’s solution at a given point in time \\(t\\). If we have a measurement at a time \\(t_i\\) and assume our measurement noise is normally distributed with some constant measurement noise \\(\\sigma\\), then the likelihood of our data would be \\(f(x_i;u(t_i;\\theta),\\sigma)\\) at each data point \\((t_i,x_i)\\). From probability we know that seeing the composition of events is given by the multiplication of probabilities, so the probability of seeing the full dataset given observations \\(D = (t_i,x_i)\\) along the timeseries is:\n\\[p(D|\\theta) = \\prod_i f(x_i;u(t_i;\\theta),\\sigma)\\]\nThis can be read as: solve the model with the given parameters, and the probability of having seen the measurement is thus given by a product of normal distribution calculations. Note that in many cases the product is not numerically stable (and grows exponentially), and so the likelihood is transformed to the log-likelihood. To get this expression, we take the log of both sides and notice that the product becomes a summation, and thus:\n\\[\\begin{align}\n\\log p(D|\\theta) &= \\sum_i \\log f(x_i;u(t_i;\\theta),\\sigma)\\\\\n                 &= \\frac{N}{\\sqrt{2\\pi}\\sigma} + \\frac{1}{2\\sigma^2} \\sum_i -(x-\\mu)^2\n\\end{align}\\]\nNotice that maximizing this log-likelihood is equivalent to minimizing the L2 norm of the solution against the data!. Thus we can see a few things:\n\nPrevious parameter estimation by minimizing a norm against data can be seen as maximum likelihood with some measurement distribution. L2 norm corresponds to assuming measurement noise is normally distributed and all of the measurements have the same error variance.\nBy the same derivation, having different error variances with normally distributed errors is equivalent to doing weighted L2 estimation.\n\nThis reformulation (generalization?) to likelihoods of probability distributions is known as maximum likelihood estimation (MLE), but is equivalent to our previous forms of parameter estimation using point estimates against data. However, this calculation is ignoring Bayes’ rule, and is thus not finding the parameters which have the highest probability. To do that, we need to go back to Bayes’ rule which states that:\n\\[\\log p(\\theta|D) = \\log p(D|\\theta) + \\log p(\\theta) - C\\]\nThus, maximizing the log-likelihood is “almost” the same as finding the most probable parameters, except that we need to add weights given \\(\\log p(\\theta)\\) from our prior distribution! If we assume our prior distribution is flat, like a uniform distribution, then we have a non-informative prior and the maximum posterior point matches that of the maximum likelihood estimation. However, this formulation allows us to get point estimates in a way that takes into account prior knowledge, and is call maximum a posteriori estimation (MAP)."
  },
  {
    "objectID": "probabilistic_programming.html#bayesian-estimation-of-posterior-distributions-with-monte-carlo",
    "href": "probabilistic_programming.html#bayesian-estimation-of-posterior-distributions-with-monte-carlo",
    "title": "15  From Optimization to Probabilistic Programming",
    "section": "15.5 Bayesian Estimation of Posterior Distributions with Monte Carlo",
    "text": "15.5 Bayesian Estimation of Posterior Distributions with Monte Carlo\nThe previous discussion still solely focused on getting point estimates for the most probable parameters. However, what if we wanted to find the distributions of the parameters, i.e. the full \\(p(D|\\theta)\\)? Outside of very few small models, this cannot be done analytically and is thus the basic problem of probabilistic programming. There are two general approaches:\n\nSampling-based approaches. Sample parameters \\(\\theta_i\\) in such a manner that the array \\([\\theta_i]\\) converges to an array sampled from the true distribution, and thus with enough samples one can capture the distribution numerically.\nVariational inference. Find some way to represent the probability distribution and push forward the distributions at every step of the program.\n\n\n15.5.0.1 Recovering Distributions from Sampled Points\nIt’s clear from above that if you have a distribution, like Normal(5,1), that you can sample from the distribution to get an array of values which follow the distribution. However, in order for the following sampling approaches to make sense, we need to see how to recover a distribution from discrete samples. So let’s say you had a bunch of normally distributed points:\n\nX = Normal(5,1)\nx = [rand(X) for i in 1:100]\nscatter(x,[1 for i in 1:100])\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice that there are more points in the areas of higher probability. Thus the density of sampled points gives us an estimate for the probability of having points in a given area. We can then count the number of points in a bin and divide by the total number of points in order to get the probability of being in a specific region. This is depicted by a histogram:\n\nhistogram(x)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nand we see this converges when we get more points:\n\nhistogram([rand(X) for i in 1:10000],normed=true)\nusing StatsPlots\nplot!(X,lw=5)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA continuous form of this is the kernel density estimate, which is essentially a smoothed binning approach.\n\nusing KernelDensity\nplot(kde([rand(X) for i in 1:10000]),lw=5)\nplot!(X,lw=5)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThus, for the sampling-based approaches, we simply need to arrive at an array which is sampled according to the distribution that we want to estimate, and from that array we can recover the distribution.\n\n\n15.5.1 Sampling Distributions with the Metropolis Hastings Algorithm\nThe Metropolis-Hastings algorithm is the simplest form of Markov Chain Monte Carlo (MCMC) which gives a way of sampling the \\(\\theta\\) distribution. To see how this algorithm works, let’s understand the ratio between two points in the posterior probability. If we have \\(x_i\\) and \\(x_j\\), the ratio of the two probabilities would be given by:\n\\[\\frac{p(x_i|D)}{p(x_j|D)} = \\frac{p(D|x_i)p(x_i)}{p(D|x_j)p(x_j)}\\]\n(notice that the integration constant cancels). This motivates the idea that all we have to do is ensure we only go to a point \\(x_j\\) from \\(x_i\\) with probability difference that matches that ratio, and over time if we do this between “all points” we will have the right number of “each point” in the distribution (quotes because it’s continuous). With a bit more rigour we arrive at the following algorithm:\n\nStarting at \\(x_i\\), take \\(x_{i+1}\\) from a sampling algorithm \\(g(x_{i+1}|x_i)\\).\nCalculate \\(A = \\min\\left(1,\\frac{p(D|x_{i+1})p(x_{i+1})g(x_i|x_{i+1})}{p(D|x_i)p(x_i)g(x_{i+1}|x_i)}\\right)\\). Notice that if we require \\(g\\) to be symmetric, then this simplifies to the probability ratio \\(A = \\min\\left(1,\\frac{p(D|x_{i+1})p(x_{i+1})}{p(D|x_i)p(x_i)}\\right)\\)\nUse a random number to accept the step with a probability \\(A\\). Go back to step 1, incrementing \\(i\\) if accepted, otherwise just repeat.\n\nI.e, we just walk around the space biasing the acceptance of a step by the factor \\(\\frac{p(x_i|D)}{p(x_j|D)}\\) and sooner or later we will have spent the right amount of time in each area, giving the correct distribution.\n(This can be rigorously proven, and those details are left out.)\n\n\n15.5.2 The Cost of Bayesian Estimation\nLet’s take a quick moment to understand the high cost of Bayesian posterior estimations. While before we were getting point estimates, now we are trying to recover a full probability distribution, and each accept/reject probability calculation requires evaluating the likelihood at some point. Remember, the likelihood is generated by our simulator, and thus every evaluation here is an ODE solver call or a neural network forward pass! This means that to get good distributions, we are solving the ODE hundreds of thousands of times, i.e. even more than when doing parameter estimation! This is something to keep in mind.\nHowever, notice that this process is trivially parallelizable. We can just have parallel chains on going, i.e. start 16 processes all doing Metropolis-Hastings, and in the end they are all sampling from the same distribution, so the final array can simply be the pooled results of each chain.\n\n\n15.5.3 Hamiltonian Monte Carlo\nMetropolis-Hastings is easy to motivate and implement. However, it does not do well in high dimensional spaces because it searches in all directions. For example, it’s common for the sampling distribution \\(g\\) to be a multivariable distribution (i.e. normal in all directions). However, high dimensional objects commonly sit on low dimensional manifolds (known as the manifold hypothesis). If that’s the case, the most probable set of parameters is something that is low dimensional. For example, parameters may compensate for one another, and so \\(\\theta_1^2 + \\theta_2^2 + \\theta_3^2 = 1\\) might be the manifold on which all of the most probable choices for \\(\\theta\\) lie, in which case we need sample on the sphere instead of all of \\(\\mathbb{R}^3\\).\nHowever, it’s quick to see that this will give Metropolis-Hastings some trouble, since it will use a normal distribution around the current point, and thus even if we start on the sphere, it will have a high chance of trying a point not on the sphere in the next round! This can be depicted as:\n\nRecall that every single rejection is still evaluating the likelihood (since it’s calculating an acceptance probability, finding it near zero, rejecting and starting again), and every likelihood call is calling our simulator, and so this is sllllllooooooooooooooowwwwwwwww in high dimensions!\nWhat we need to do instead is ensure that we walk along the path of high probability. What we want to do is thus build a vector field that matches our high probability regions\n\nand follow said vector field (following a vector field is solving what kind of equation?). The first idea one might have is to use the gradient. However, while this idea has the right intentions, the issue is that the gradient of the probability will average out all of the possible probabilities, and will thus flow towards the mode of the distribution:\n\nTo overcome this issue, we look to physical systems and see that a satellite orbiting a planet always nicely stays on some manifold instead of following the gradient:\n\nThe reason why it does is because it has momentum. Recall from basic physics that one way to describe a physical system is through Hamiltonian mechanics, where \\(H(x,p)\\) is the energy associated with the state \\((x,p)\\) (normally \\(x\\) is location and \\(p\\) is momentum). Due to conservation of energy, the solution of the dynamical equations leads to \\(H(x,p)\\) being constant, and thus the dynamics follow the level sets of \\(H\\). From the Hamiltonian the dynamics of the system are:\n\\[\\begin{align}\n\\frac{dx}{dt} &=  \\frac{dH}{dp}\\\\\n              &= -\\frac{dH}{dx}\n\\end{align}\\]\nHere we want our Hamiltonian to be our posterior probability, so that way we stay on the manifold of high probability. This means:\n\\[H(x,p) = - \\log \\pi(x,p)\\]\nwhere \\(\\pi(x,p) = \\pi(p|x)\\pi(x)\\) (where I am now using \\(pi\\) for probability since \\(p\\) is momentum!). So to lift from a probability over parameters to one that includes momentum, we simply need to choose a conditional distribution \\(\\pi(p|x)\\). This would mean that\n\\[\\begin{align}\nH(x,p) &= -log \\pi(p|x) - \\log \\pi(x)\\\\\n       &= K(p,x) + V(x)\n\\end{align}\\]\nwhere \\(K\\) is the kinetic energy and \\(V\\) is the potential. Thus the potential energy is directly given by the posterior calculation, and the kinetic energy is thus a choice that is used to build the correct Hamiltonian. Hamiltonian Monte Carlo methods then dig into good ways to choose the kinetic energy function. This is done at the start (along with the choice of ODE solver time step) in such a way that it maximizes acceptance probabilities.\n\n\n15.5.4 Connections to Differentiable Programming\n\\(-\\frac{dH}{dx}\\) requires calculating the gradient of the likelihood function with respect to the parameters, so we are once again using the gradient of our simulator! This means that all of our previous discussion on automatic differentiation and differentiable programming applies to the Hamiltonian Monte Carlo context.\nThere’s another thread to follow that transformations of probability distributions are pushforwards of the Jacobian transformations (given the transformation of an integral formula), and this is used when doing variational inference.\n\n\n15.5.5 Symplectic and Geometric Integration\nOne way to integrate the system of ODEs which result from the Hamiltonian system is to convert it to a system of first order ODEs and solve it directly. However, this loses information and can result in drift. This is demonstrated by looking at the long time solution of the pendulum:\n\nusing ParameterizedFunctions\nu0 = [1.,0.]\nharmonic! = @ode_def HarmonicOscillator begin\n   dv = -x\n   dx = v\nend\ntspan = (0.0,10.0)\ntspan = (0.0,10000.0)\nprob = ODEProblem(harmonic!,u0,tspan)\nsol = solve(prob,Tsit5())\ngr(fmt=:png) # Make it a PNG instead of an SVG since there's a lot of points!\nplot(sol,vars=(1,2))\n\n┌ Warning: To maintain consistency with solution indexing, keyword argument vars will be removed in a future version. Please use keyword argument idxs instead.\n│   caller = ip:0x0\n└ @ Core :-1\n\n\n\n\n\n\nplot(sol)\n\n\n\n\nWhat is an oscillatory system slowly loses energy and falls inward towards the center. To avoid this issue, we can do a few things:\n\nProject back to the manifold after steps. That can be costly (but almost might only need to happen every once in awhile!)\nUse a symplectic integrator.\n\nA symplectic integrator is an integrator who’s solution lives on a symplectic manifold, i.e. it preserves area in in the \\((x,p)\\) ellipses as it numerically approximates the flow. This means that:\n\nLong-time integrations are truly cyclic with only floating point drift.\nSteps preserve area. In the sense of Hamiltonian Monte Carlo, this means preserve probability and thus increase the acceptance rate.\n\nThese properties are demonstrated in the Kepler problem demo. However, note that while the solution lives on a symplectic manifold, it isn’t necessarily the correct symplectic manifold. The shift in the manifold is \\(\\mathcal{O}(\\Delta t^k)\\) where \\(k\\) is the order of the method. For more information on symplectic integration, consult this StackOverflow response which goes into depth.\n\n\n15.5.6 Application: Bayesian Estimation of Differential Equation Parameters\nFor a full demo of probabilistic programming on a differential equation system, see this tutorial on Bayesian inference of pendulum parameteres utilizing DifferentialEquations.jl and DiffEqBayes.jl."
  },
  {
    "objectID": "probabilistic_programming.html#bayesian-estimation-of-posterior-distributions-with-variational-inference",
    "href": "probabilistic_programming.html#bayesian-estimation-of-posterior-distributions-with-variational-inference",
    "title": "15  From Optimization to Probabilistic Programming",
    "section": "15.6 Bayesian Estimation of Posterior Distributions with Variational Inference",
    "text": "15.6 Bayesian Estimation of Posterior Distributions with Variational Inference\nInstead of using sampling, one can use variational inference to push through probability distributions. There are many ways to do variational inference, but a lot of the methods can be very model-specific. However, a recent change to probabilistic programming has been the development of Automatic Differentiation Variational Inference (ADVI): a general variational inference method which is not model-specific and instead uses AD. This has allowed for large expensive models to get effective distributional estimation, something that wasn’t previously possible with HMC. In this section we will build up this methodology and understand its performance characteristics.\n\n15.6.1 ADVI as Optimization\nIn this form of variational inference, we wish to directly estimate the posterior distribution. To do so, we pick a functional form to represent the solution \\(q(\\theta; \\phi)\\) where \\(\\phi\\) are latent variables. We want our resulting distribution to fit the posterior, and tus we enforce that:\n\\[\\phi^\\ast = \\text{argmin}_{\\phi} \\text{KL} \\left(q(\\theta; \\phi) \\Vert p(\\theta | D)\\right)\\]\nwhere KL is the KL-divergence. KL-divergence is a distance function over probability distributions, and so this is simply a cost function over the distance between a chosen distribution and a desired distribution, where when \\(\\phi\\) are good we will have \\(q\\) as a good approximation to the posterior.\nHowever, the KL divergence lacks an analytical form because it requires knowing the posterior, the quantity we are trying to numerically estimate. However, it turns out that we can instead maximize the Evidence Lower Bound (ELBO):\n\\[\\mathcal{L}(\\phi) = \\mathbb{E}_{q}[\\log p(x,\\theta)] - \\mathbb{E}_q [\\log q(\\theta; \\phi)]\\]\nThe ELBO is equivalent to the negative KL divergence up to a constant \\(\\log p(x)\\), which means that maximizing this is equivalent to minimizing the KL divergence.\nOne last detail is necessary in order for this problem to be tractable. To know the set of possible values to optimize over, we assume that the support of \\(q\\) is a subset of the support of the prior. This means that our prior has to cover the probability distribution, which makes sense and matches Cromwell’s rule for MCMC.\nAt this point we now assume that \\(q\\) is Gaussian. When we rewrite the ELBO in terms of the standard Gaussian, we receive an expectation that is automatically differentiable. Calculating gradients is thus done with AD. Using only one or a few solves gives a noisy gradient to sample and optimize the latent variables to hone in on latent variables."
  },
  {
    "objectID": "probabilistic_programming.html#a-note-on-implementation-of-optimization-for-probabilistic-programming",
    "href": "probabilistic_programming.html#a-note-on-implementation-of-optimization-for-probabilistic-programming",
    "title": "15  From Optimization to Probabilistic Programming",
    "section": "15.7 A Note on Implementation of Optimization for Probabilistic Programming",
    "text": "15.7 A Note on Implementation of Optimization for Probabilistic Programming\nVariable domains can be constrained. For example, you may require a positive value. This can be handled by a transformation. For example, if \\(y\\) must be positive, then one can optimize implicitly using \\(\\exp(y)\\) at every point, this allowing \\(y\\) to be any real value with then \\(\\exp(y)\\) is positive. This turns the problem into an unconstrained optimization over the real numbers, and similar transformations can be done with any of the standard probability distribution’s support function.\n\n15.7.0.1 Citation\nFor Hamiltonian Monte Carlo, the images were taken from A Conceptual Introduction to Hamiltonian Monte Carlo by Michael Betancourt."
  },
  {
    "objectID": "global_sensitivity.html#youtube-video",
    "href": "global_sensitivity.html#youtube-video",
    "title": "16  Global Sensitivity Analysis",
    "section": "16.1 Youtube Video",
    "text": "16.1 Youtube Video\nSensitivity analysis is the measure of how sensitive a model is to changes in parameters, i.e. how much the output changes given a change in the input. Clearly, derivatives are a measure of sensitivity, but derivative are local sensitivity measures because they are only the derivative at a single point. However, the idea of probabilistic programming starts to bring up an alternative question: how does the output of a model generally change with a change in the input? This kind of question requires an understanding of global sensitivity of a model. While there isn’t a single definition of the concept, there are a few methods that individuals have employed to estimate the global sensitivity.\nReference implementations of these methods can be found in GlobalSensitivity.jl"
  },
  {
    "objectID": "global_sensitivity.html#setup-for-global-sensitivity",
    "href": "global_sensitivity.html#setup-for-global-sensitivity",
    "title": "16  Global Sensitivity Analysis",
    "section": "16.2 Setup for Global Sensitivity",
    "text": "16.2 Setup for Global Sensitivity\nIn our global sensitivity analysis, we have a model \\(f\\) and want to understand the relationship\n\\[y = f(x_i)\\]\nRecall \\(f\\) can be a neural network, an ODE solve, etc. where the \\(X_i\\) are items like initial conditions and parameters. What we want to do is understand how much the total changes in \\(y\\) can be attributed to changes in specific \\(x_i\\).\nHowever, this is not an actionable form since we don’t know what valid inputs into \\(f\\) look like. Thus any global sensitivity study at least needs a domain for the \\(x_i\\), at least in terms of bounds. This is still underdefined because what makes one thing that it’s not more likely for \\(x_i\\) to be near the lower part of the bound instead of the upper part? Thus, for global sensitivity analysis to be well-defined, \\(x_i\\) must take a distributional form, i.e. be random variables. Thus \\(f\\) is a deterministic program with probabilistic inputs, and we want to determine the effects of the distributional inputs on the distribution of the output.\n\n16.2.1 Reasons for Global Sensitivity Analysis\nWhat are the things we can learn from doing such a global sensitivity analysis?\n\nYou can learn what variables would need to be changed to drive the solution in a given direction or control the system. If your model is exact and the parameters are known, the “standard” methods apply, but if your model is only approximate, a global sensitivity metric may be a better prediction as to how variables cause changes.\nYou can learn if there are any variables which do not have a true effect on the output. These variables would be practically unidentifiable from data and models can be reduced by removing the terms. It also is predictive as to robustness properties.\nYou can find ways to automatically sparsify a model by dropping off the components which contribute the least. This matters in automatically generated or automatically detected models, where many pieces may be spurious and global sensitivities would be a method to detect that in a manner that is not sensitive to the chosen parameters."
  },
  {
    "objectID": "global_sensitivity.html#global-sensitivity-analysis-measures",
    "href": "global_sensitivity.html#global-sensitivity-analysis-measures",
    "title": "16  Global Sensitivity Analysis",
    "section": "16.3 Global Sensitivity Analysis Measures",
    "text": "16.3 Global Sensitivity Analysis Measures\n\n16.3.1 Linear Global Sensitivity Metrics: Correlations and Regressions\nThe first thing that you can do is approximate the full model with a linear surrogate, i.e.\n\\[y = AX\\]\nfor some linear model. A regression can be done on the outputs of the model in order to find the linear approximation. The best fitting global linear model then gives coefficients for the global sensitivities via the individual effects, i.e. for\n\\[y = \\sum_i \\beta_i x_i\\],\nthe \\(\\beta_i\\) are the global effect. Just as with any use of a linear model, the same ideas apply. The coefficient of determination (\\(R^2\\)) is a measure of how well the model fits. However, one major change needs to be done in order to ensure that the solutions are comparable between different models. The dependence of the solution on the units can cause the coefficients to be large/small. Thus we need to normalize the data, i.e. use the transformation\n\\[\\tilde{x_i} = \\frac{x_i-E[x_i]}{V[x_i]}\\] \\[\\tilde{y_i} = \\frac{y_i-E[y_i]}{V[y_i]}\\]\nThe normalized coefficients are known as the Standardized Regression Coefficients (SRC) and are a measure of the global effects.\nNotice that while the \\(\\beta_i\\) capture the mean effects, it holds that\n\\[V(y) = \\sum_i \\beta^2_i x_i\\]\nand thus the variance due to \\(x_i\\) can be measured as:\n\\[SRC_i = \\beta_i \\sqrt{\\frac{V[x_i]}{V[y]}}\\]\nThis interpretation is the same as the solution from the normalized variables.\nFrom the same linear model, two other global sensitivity metrics are defined. The Correlation Coefficients (CC) are simply the correlations:\n\\[CC_i = \\frac{\\text{cov}(x_i,y)}{\\sqrt{V[x_i]V[y]}}\\]\nSimilarly, the Partial Correlation Coefficient is the correlation coefficient where the linear effect of the other terms are removed, i.e. for \\(S_i = {x_1,x_2,\\ldots,x_{j-1},x_{j+1},\\ldots,x_n}\\) we have\n\\[PCC_{i|S_i} = \\frac{\\text{cov}(x_i,y|S)j)}{\\sqrt{V[x_i|S_i]V[y|S_i]}}\\]\n\n\n16.3.2 Derivative-based Global Sensitivity Measures (DGSM)\nTo go beyond just a linear model, one might want to do successive linearization. Since derivatives are a form of linearization, then one may thing to average derivatives. This averaging of derivatives is the DGSM method. If the \\(x_i\\) are random variables with joint CDF \\(F(x)\\), then it holds that:\n\\[v_i = \\int_{R^d} \\left(\\frac{\\partial f(x)}{\\partial x_i}\\right)^2 dF(x) = \\mathbb{E}\\left[\\left(\\frac{\\partial f(x)}{\\partial x_i}\\right)^2\\right],\\]\nWe can also define the mean measure, which is simply:\n\\[w_i = \\int_{R^d} \\frac{\\partial f(x)}{\\partial x_i} dF(x) = \\mathbb{E}\\left[\\frac{\\partial f(x)}{\\partial x_i}\\right].\\]\nThus a global variance estimate would be \\(v_i - w_i^2\\).\n\n\n16.3.3 ADVI for Global Sensitivity\nNote that the previously discussed method for probabilistic programming, ADVI, is a method for producing a Gaussian approximation for a probabilistic program. The resulting mean-field or full Gaussian approximations are variance index calculations!\n\n\n16.3.4 The Morris One-At-A-Time (OAT) Method\nInstead of using derivatives, one can use finite difference approximations. Normally you want to use small \\(\\Delta x\\), but if we are averaging derivatives over a large area, then in reality we don’t really need a small \\(\\Delta x\\)!\nThis is where the Morris method comes in. The basic idea is that moving in one direction at a time is a derivative estimate, and if we step large enough then the next derivative estimate may be sufficiently different enough to contribute well to the total approximation. Thus we do the following:\n\nTake a random starting point\nRandomly choose a direction \\(i\\) and make a change \\(\\Delta x_i\\) only in that direction.\nCalculate the derivative approximation from that change. Repeat 2 and 3.\n\nKeep doing this for enough steps, and the average of your derivative approximations becomes a global index. Notice that this reuses every simulation as part of two separate estimates, making it much more computationally efficient than the other methods. However, it accounts for average changes and not necessarily measurements gives a value that’s a decomposition of a total variance. But its computational cost makes it attractive for making quick estimates of the global sensitivities.\nFor practical usage, a few changes have to be done. First of all, notice that positive and negative change can cancel out. Thus if one want to measure of associated variance, one should use absolute values or squared differences. Also, one needs to make sure that these trajectories get good coverage of the input space. Define the distance between two trajectories as the sum of the geometric distances between all pairs of points. Generate many more trajectories than necessary and choose the \\(r\\) trajectories with the largest distance. If the model evaluations are expensive, this is significantly cheap enough in comparison that it’s okay to do.\n\n\n16.3.5 Sobol’s Method (ANOVA)\nSobol’s method is a true nonlinear decomposition of variance and it is thus considered one of the gold standards. For Sobol’s method, we define the decomposition\n\\[f(x) = f_0 + \\sum_i f_i(x_i) + \\sum_{i,j} f_{ij}(x_i,x_j) + \\ldots\\]\nwhere\n\\[f_0 = \\int_\\Omega f(x) dx\\]\nand orthogonality holds:\n\\[f_{i,j,\\ldots}(x_i,x_j,\\ldots)dx = 0\\]\nby the definitions:\n\\[f_i(x_i) = E(y|x_i) - f_0\\]\n\\[f_{ij}(x_i,y_j) = E(y|x_i,x_j) - f_0 - f_i - f_j\\]\nAssuming that \\(f(x)\\) is L2, it holds that\n\\[\\int_\\Omega f^2(x)dx - f_0^2 = \\sum_s \\sum_i \\int f^2_{i_1,i_2,\\ldots,i_s} dx\\]\nand thus\n\\[V[y] = \\sum V_i + \\sum V_{ij} + \\ldots\\]\nwhere\n\\[V_i = V[E_{x_{\\sim i}}[y|x_i]]\\] \\[V_{ij} = V[E_{x_{\\sim ij}}[y|x_i,x_j]]-V_i - V_j\\]\nwhere \\(X_{\\sim i}\\) means all of the variables except \\(X_i\\). This means that the total variance can be decomposed into each of these variances.\nFrom there, the fractional contribution to the total variance is thus the index:\n\\[S_i = \\frac{V_i}{Var[y]}\\]\nand similarly for the second, third, etc. indices.\nAdditionally, if there are too many variables, one can compute the contribution of \\(x_i\\) including all of its interactions as:\n\\[S_{T_i} = \\frac{E_{X_{\\sim i}}[Var[y|X_{\\sim i}]]}{Var[y]} = 1 - \\frac{Var_{X_{\\sim i}}[E_{X_i}[y|x_{\\sim i}]]}{Var[y]}\\]\n\n16.3.5.1 Computational Tractability and Quasi-Monte Carlo\nNotice that every single expectation has an integral in it, so the variance is defined as integrals of integrals, making this a very challenging calculation. Thus instead of directly calculating the integrals, in many cases Monte Carlo estimators are used. Instead of a pure Monte Carlo method, one generally uses a low-discrepancy sequence (a form of quasi-Monte Carlo) to effectively sample the search space.\nThe following generates for example a Sobol sequence:\n\nusing Sobol, Plots\ns = SobolSeq(2)\np = hcat([next!(s) for i = 1:1024]...)'\nscatter(p[:,1], p[:,2])\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnother common quasi-Monte Carlo sequence is the Latin Hypercube, which is a generalization of the Latin Square where in every row, column, etc. only one point is given, allowing a linear spread over a high dimensional space.\n\nusing LatinHypercubeSampling\np = LHCoptim(120,2,1000)\nscatter(p[1][:,1],p[1][:,2])\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor a reference library with many different quasi-Monte Carlo samplers, check out QuasiMonteCarlo.jl."
  },
  {
    "objectID": "global_sensitivity.html#fourier-amplitude-sensitivity-sampling-fast-and-efast",
    "href": "global_sensitivity.html#fourier-amplitude-sensitivity-sampling-fast-and-efast",
    "title": "16  Global Sensitivity Analysis",
    "section": "16.4 Fourier Amplitude Sensitivity Sampling (FAST) and eFAST",
    "text": "16.4 Fourier Amplitude Sensitivity Sampling (FAST) and eFAST\nThe FAST method is a change to the Sobol method to allow for faster convergence. First transform the variables \\(x_i\\) onto the space \\([0,1]\\). Then, instead of the linear decomposition, one decomposes into a Fourier basis:\n\\[f(x_i,x_2,\\ldots,x_n) = \\sum_{m_1 = -\\infty}^{\\infty} \\ldots \\sum_{m_n = -\\infty}^{\\infty} C_{m_1m_2\\ldots m_n}\\exp\\left(2\\pi i (m_1 x_1 + \\ldots + m_n x_n)\\right)\\]\nwhere\n\\[C_{m_1m_2\\ldots m_n} = \\int_0^1 \\ldots \\int_0^1 f(x_i,x_2,\\ldots,x_n) \\exp\\left(-2\\pi i (m_1 x_1 + \\ldots + m_n x_n)\\right)\\]\nThe ANOVA like decomposition is thus\n\\[f_0 = C_{0\\ldots 0}\\]\n\\[f_j = \\sum_{m_j \\neq 0} C_{0\\ldots 0 m_j 0 \\ldots 0} \\exp (2\\pi i m_j x_j)\\]\n\\[f_{jk} = \\sum_{m_j \\neq 0} \\sum_{m_k \\neq 0} C_{0\\ldots 0 m_j 0 \\ldots m_k 0 \\ldots 0} \\exp \\left(2\\pi i (m_j x_j + m_k x_k)\\right)\\]\nThe first order conditional variance is thus:\n\\[V_j = \\int_0^1 f_j^2 (x_j) dx_j = \\sum_{m_j \\neq 0} |C_{0\\ldots 0 m_j 0 \\ldots 0}|^2\\]\nor\n\\[V_j = 2\\sum_{m_j = 1}^\\infty \\left(A_{m_j}^2 + B_{m_j}^2 \\right)\\]\nwhere \\(C_{0\\ldots 0 m_j 0 \\ldots 0} = A_{m_j} + i B_{m_j}\\). By Fourier series we know this to be:\n\\[A_{m_j} = \\int_0^1 \\ldots \\int_0^1 f(x)\\cos(2\\pi m_j x_j)dx\\]\n\\[B_{m_j} = \\int_0^1 \\ldots \\int_0^1 f(x)\\sin(2\\pi m_j x_j)dx\\]\n\n16.4.0.1 Implementation via the Ergodic Theorem\nDefine\n\\[X_j(s) = \\frac{1}{2\\pi} (\\omega_j s \\mod 2\\pi)\\]\nBy the ergodic theorem, if \\(\\omega_j\\) are irrational numbers, then the dynamical system will never repeat values and thus it will create a solution that is dense in the plane (Let’s prove a bit later). As an animation:\n\n(here, \\(\\omega_1 = \\pi\\) and \\(\\omega_2 = 7\\))\nThis means that:\n\\[A_{m_j} = \\lim_{T\\rightarrow \\infty} \\frac{1}{2T} \\int_{-T}^T f(x)\\cos(m_j \\omega_j s)ds\\]\n\\[B_{m_j} = \\lim_{T\\rightarrow \\infty} \\frac{1}{2T} \\int_{-T}^T f(x)\\sin(m_j \\omega_j s)ds\\]\ni.e. the multidimensional integral can be approximated by the integral over a single line.\nOne can satisfy this approximately to get a simpler form for the integral. Using \\(\\omega_i\\) as integers, the integral is periodic and so only integrating over \\(2\\pi\\) is required. This would mean that:\n\\[A_{m_j} \\approx \\frac{1}{2\\pi} \\int_{-\\pi}^\\pi f(x)\\cos(m_j \\omega_j s)ds\\]\n\\[B_{m_j} \\approx \\frac{1}{2\\pi} \\int_{-\\pi}^\\pi f(x)\\sin(m_j \\omega_j s)ds\\]\nIt’s only approximate since the sequence cannot be dense. For example, with \\(\\omega_1 = 11\\) and \\(\\omega_2 = 7\\):\n\nA higher period thus gives a better fill of the space and thus a better approximation, but may require a more points. However, this transformation makes the true integrals simple one dimensional quadratures which can be efficiently computed.\nTo get the total index from this method, one can calculate the total contribution of the complementary set, i.e. \\(V_{c_i} = \\sum_{j \\neq i} V_j\\) and then\n\\[S_{T_i} = 1 - S_{c_i}\\]\nNote that this then is a fast measure for the total contribution of variable \\(i\\), including all higher-order nonlinear interactions, all from one-dimensional integrals! (This extension is called extended FAST or eFAST)\n\n\n16.4.0.2 Proof of the Ergodic Theorem\nLook at the map \\(x_{n+1} = x_n + \\alpha (\\text{mod} 1)\\), where \\(\\alpha\\) is irrational. This is the irrational rotation map that corresponds to our problem. We wish to prove that in any interval \\(I\\), there is a point of our orbit in this interval.\nFirst let’s prove a useful result: our points get arbitrarily close. Assume that for some finite \\(\\epsilon\\) that no two points are \\(\\epsilon\\) apart. This means that we at most have spacings of \\(\\epsilon\\) between the points, and thus we have at most \\(\\frac{2\\pi}{\\epsilon}\\) points (rounded up). This means our orbit is periodic. This means that there is a \\(p\\) such that\n\\[x_{n+p} = x_n\\]\nwhich means that \\(p \\alpha = 1\\) or \\(p = \\frac{1}{\\alpha}\\) which is a contradiction since \\(\\alpha\\) is irrational.\nThus for every \\(\\epsilon\\) there are two points which are \\(\\epsilon\\) apart. Now take any arbitrary \\(I\\). Let \\(\\epsilon &lt; d/2\\) where \\(d\\) is the length of the interval. We have just shown that there are two points \\(\\epsilon\\) apart, so there is a point that is \\(x_{n+m}\\) and \\(x_{n+k}\\) which are \\(&lt;\\epsilon\\) apart. Assuming WLOG \\(m&gt;k\\), this means that \\(m-k\\) rotations takes one from \\(x_{n+k}\\) to \\(x_{n+m}\\), and so \\(m-k\\) rotations is a rotation by \\(\\epsilon\\). If we do \\(\\frac{1}{\\epsilon}\\) rounded up rotations, we will then cover the space with intervals of length epsilon, each with one point of the orbit in it. Since \\(\\epsilon &lt; d/2\\), one of those intervals is completely encapsulated in \\(I\\), which means there is at least one point in our orbit that is in \\(I\\).\nThus for every interval we have at least one point in our orbit that lies in it, proving that the rotation map with irrational \\(\\alpha\\) is dense. Note that during the proof we essentially showed as well that if \\(\\alpha\\) is rational, then the map is periodic based on the denominator of the map in its reduced form."
  },
  {
    "objectID": "global_sensitivity.html#a-quick-note-on-parallelism",
    "href": "global_sensitivity.html#a-quick-note-on-parallelism",
    "title": "16  Global Sensitivity Analysis",
    "section": "16.5 A Quick Note on Parallelism",
    "text": "16.5 A Quick Note on Parallelism\nVery quick note: all of these are hyper parallel since it does the same calculation per parameter or trajectory, and each calculation is long. For quasi-Monte Carlo, after generating “good enough” trajectories, one can evaluate the model at all points in parallel, and then simply do the GSA index measurement. For FAST, one can do each quadrature in parallel."
  },
  {
    "objectID": "profiling.html#youtube-video",
    "href": "profiling.html#youtube-video",
    "title": "17  Code Profiling and Optimization",
    "section": "17.1 Youtube Video",
    "text": "17.1 Youtube Video\nThis is just a quick look into code profiling. By now we should be writing high performance parallel code which is combining machine learning and scientific computing techniques and doing large-scale parameter analyses on the models. However, at this point it may be difficult to understand where our performance difficulties lie. This is where we turn to code profiling tooling."
  },
  {
    "objectID": "profiling.html#type-inference-checking",
    "href": "profiling.html#type-inference-checking",
    "title": "17  Code Profiling and Optimization",
    "section": "17.2 Type Inference Checking",
    "text": "17.2 Type Inference Checking\nThe most common way for code to slow down is via type-inference issues. One can normally work through them by “thinking like a compiler” and seeing what would be inferable. For example, a common issue is to not concretely type one’s types. For example:\n\nstruct MyStruct\n  a::AbstractArray\nend\nx = MyStruct([1,2,3])\nfunction f(x)\n  x.a[1]\nend\nusing InteractiveUtils\n@code_warntype f(x)\n\nMethodInstance for f(::MyStruct)\n  from f(x) @ Main In[3]:5\nArguments\n  #self#\n\n\n::Core.Const(f)\n  x::MyStruct\nBody::Any\n1 ─\n\n\n %1 = Base\n\n\n.getproperty(x, :a)\n\n\n::AbstractArray\n│   %2 = Base.getindex(%1, 1)::Any\n└──      return %2\n\n\n\nIn this case, the return type is not inferred and using MyStruct will generate slow code. The reason for this is quite simple: x.a can only be inferred as AbstractArray, and thus the element type x.a[1] and the exact dispatch cannot be known until the function finds out at runtime what kind of array it is. As a result, the compiler throws the only thing it can: it puts Any as the inferred type and runs slow code.\nWe can instead utilize a concrete struct or use a parametric type to create a family of related structs:\n\nstruct MyStruct2{A &lt;: AbstractArray}\n  a::A\nend\nx2 = MyStruct2([1,2,3])\n@code_warntype f(x2)\n\nMethodInstance for f(::MyStruct2{Vector{Int64}})\n  from f(x) @ Main In[3]:5\nArguments\n  #self#::Core.Const(f)\n  x::MyStruct2{Vector{Int64}}\nBody::Int64\n1 ─ %1 = Base.getproperty(x, :a)::Vector{Int64}\n│   %2 = Base.getindex(%1, 1)::Int64\n└──      return %2\n\n\n\nand now it’s inferred because the information that it would need is inferrable.\nBut what if we needed help? The first tool of course is @code_warntype. But for deeper functions you may want more tooling. A nice tool is Traceur.jl which will alert you to the lines at which you have performance issues. In our example we see:\n\nusing Traceur\n@trace f(x)\n\n┌ Warning: dynamic dispatch to Base.getindex(Base.getfield(x, a), 1)\n└ @ none:-1\n┌ Warning: f returns Any\n└ @ none:2\nwhich points out our first problem is getting the untyped array out of the MyStruct. On larger functions it can do even more:\n\nfunction naive_sum(xs)\n  s = 0\n  for x in xs\n    s += x\n  end\n  return s\nend\n@trace naive_sum([1.])\n\n┌ Warning:  is assigned as Tuple{Int64,Int64}\n└ @ array.jl:-1\n┌ Warning:  is assigned as Nothing\n└ @ array.jl:-1\n┌ Warning:  is assigned as Union{Nothing, Tuple{Float64,Int64}}\n└ @ none:-1\n┌ Warning:  is assigned as Union{Nothing, Tuple{Float64,Int64}}\n└ @ none:-1\n┌ Warning: s is assigned as Int64\n└ @ none:-1\n┌ Warning: s is assigned as Float64\n└ @ none:-1\n┌ Warning: naive_sum returns Union{Float64, Int64}\n└ @ none:2\nand alert you to multiple lines which are causing problems.\nHowever, for even larger functions you can still have many issues that are hard to dig into with Julia a linear tool. For thus, Cthulhu.jl’s @descend macro lets you interactively dig into the function to find the problematic lines. For the best introduction, watch Valentin Churavy’s JuliaCon 2019 talk"
  },
  {
    "objectID": "profiling.html#flame-graphs",
    "href": "profiling.html#flame-graphs",
    "title": "17  Code Profiling and Optimization",
    "section": "17.3 Flame Graphs",
    "text": "17.3 Flame Graphs\nFlame graphs are a common tool for illustrating performance. To demonstrate this let’s look at the solution to an ODE from DifferentialEquations.jl’s OrdinaryDiffEq.jl. The code is the following:\n\nusing OrdinaryDiffEq\nfunction lorenz(du,u,p,t)\n du[1] = 10.0(u[2]-u[1])\n du[2] = u[1]*(28.0-u[3]) - u[2]\n du[3] = u[1]*u[2] - (8/3)*u[3]\nend\nu0 = [1.0;0.0;0.0]\ntspan = (0.0,100.0)\nprob = ODEProblem(lorenz,u0,tspan)\nsol = solve(prob,Tsit5())\nusing Plots; plot(sol,vars=(1,2,3))\n\n┌ Warning: To maintain consistency with solution indexing, keyword argument vars will be removed in a future version. Please use keyword argument idxs instead.\n│   caller = ip:0x0\n└ @ Core :-1\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo generate the flame graph, first we want to create a profile. To do this we will use the Profile module’s @profile. Note that a profile should be “sufficiently large”, so on quick functions you may want to run the code plenty of times. Make sure the profile does not include compilation if you want good results!\n\n# No compilation in the results\nsol = solve(prob,Tsit5())\n\nusing Profile\n# Profile 1000 runs\n@profile for i in 1:1000 sol = solve(prob,Tsit5()) end\n\nThis profiler is a statistical or sampling profiler, which means it periodically samples where it is at in a code and thus understands the hotspots in the code by tallying how many samples are in a certain area. We can first visualize this by printing it out:\n\n#Profile.print()\n\nHowever, that printout can often times be hard to read. Instead, we can visualize it with a flame graph. There are many ways to get the flame graph, if you’re in Juno, you can simply do:\n\nJuno.profiler()\n\n\n(Note that if you’re not using Juno, there are equivalent tools in the package ecosystem. ProfileView.jl is a very simple flame graph generator, and PProf.jl exports to Google PProf which has many more features)\nEach block corresponds to a function call. The horizontal length is the amount of time spent in that function, while the vertical grouping is for call nesting, i.e. you are below the function that called you. The portion that is circled is where the mouse pointer was at, and while hovering over this it said what function it corresponded to: recursivecopy in RecursiveArrayTools.jl. If we click on this, it sends us to the hotspot:\n\nJuno gives you a light indicator that tells you how much time is spent at a given line. Here we see that most of the time is spent inside of the map operation which calls recursivecopy on the elements, which then does copy(a).\nThis tells us that the main cost of our code is the part that is copying the arrays to save them! Thus let’s generate a new profile where the ODE solver saves less:\n\nProfile.clear()\n# No compilation in the results\nsol = solve(prob,Tsit5(),save_everystep=false)\n\n# Profile 1000 runs\n@profile for i in 1:1000 sol = solve(prob,Tsit5(),save_everystep=false) end\n\n\nJuno.profiler()\n\n\nNow we see that the majority of the time is spent in the perform_step! method, which is:\n\nWe can notice that there is still quite a bit of jitter in the profile since each of the f calls here should be exactly the same length, but upping the number of solves in the loop would help with that:\n\n@profile for i in 1:10000 sol = solve(prob,Tsit5(),save_everystep=false) end\n\n\nJuno.profiler()\n\n\nNow that this looks like a fairly good profile, we can use this to dig in and find out what lines need to be optimized!"
  },
  {
    "objectID": "uncertainty.html#youtube-video",
    "href": "uncertainty.html#youtube-video",
    "title": "18  Uncertainty Programming, Generalized Uncertainty Quantification",
    "section": "18.1 Youtube Video",
    "text": "18.1 Youtube Video\nIn this lecture we will mix two separate topics: uncertainty quantification and adaptivity of algorithms. Using compiler-based tooling, similar to how automatic differentiation and probabilistic programming toolchains, we will show how one can begin to pushforward uncertainties of a model or calculation. This leads to an idea of uncertainty programming, a term which is not in use but should be justified by these notes."
  },
  {
    "objectID": "uncertainty.html#what-is-uncertainty-quantification",
    "href": "uncertainty.html#what-is-uncertainty-quantification",
    "title": "18  Uncertainty Programming, Generalized Uncertainty Quantification",
    "section": "18.2 What is Uncertainty Quantification?",
    "text": "18.2 What is Uncertainty Quantification?\nUncertainty quantification is the identification and quantification of sources of uncertainty. In our training of a neural differential equation, we have seen that the question of uncertainty can quickly become muddled. Results are inexact because of:\n\nTruncation errors in the ODE solve\nTruncation errors in the adjoint ODE solve\nTruncation errors in the interpolation calculation\nNumerical errors in every dot product along the way (!)\nNumerical errors in matrix multiplication and linear solving (latter when implicit)\nNumerical errors in backpropagation\nMeasurement errors in our fitting data\nRandomness in the optimizer (when stochastic, like ADAM)\nWhat is the error in the model specification / model form?\n\n“How correct is my model?” is thus a very involved question, since you’d have to know that every source of uncertainty is contained. In some cases we have rigorous mathematical results proving bounds. In other cases, we need to find empirical ways to quantify what’s going on using our known bounds."
  },
  {
    "objectID": "uncertainty.html#some-high-level-uq-techniques",
    "href": "uncertainty.html#some-high-level-uq-techniques",
    "title": "18  Uncertainty Programming, Generalized Uncertainty Quantification",
    "section": "18.3 Some High Level UQ Techniques",
    "text": "18.3 Some High Level UQ Techniques\nTwo high level UQ techniques fall out of methodologies we have recently discussed. If we fit a model \\(f\\) to data, be it a neural network, a neural ODE, or some physical ODE model, we can fit it probabilistically using the Bayesian estimation or probabilistic programming tools previously described. With this form of fitting, one can ask the question “what are the likely results from the model given these parameter distributions?”, which can then be answered through Monte Carlo sampling.\nAnother form of high level UQ is global sensitivity analysis, which gives a measurement for how much the output is going to vary over a wide range and thus relates uncertainties in the input to uncertainties in the output."
  },
  {
    "objectID": "uncertainty.html#pushforward-methods-for-uncertainties",
    "href": "uncertainty.html#pushforward-methods-for-uncertainties",
    "title": "18  Uncertainty Programming, Generalized Uncertainty Quantification",
    "section": "18.4 Pushforward Methods for Uncertainties",
    "text": "18.4 Pushforward Methods for Uncertainties\nInstead of relying on expensive Monte Carlo methods for the pushforward of an uncertainty, we can derive a more programmatic approach to uncertainty quantification through the use of uncertain number arithmetic.\nTo start, let’s first revive the old physics way to doing simple uncertainty quantification. If you have two numbers, \\(x = a \\pm b\\), one might remember the rules like,\n\\[\\alpha + a = (\\alpha + a) \\pm b\\] \\[\\alpha a = \\alpha a \\pm |\\alpha| b\\]\nLet’s investigate this a bit more and see if we can develop an arithmetic, like dual numbers, to then propagate through whole programs. This idea comes from the arithmetic on normally distributed random variables. If we interpret \\(x \\sim N(a,b)\\), i.e. a normally distributed random variable with mean \\(a\\) and standard deviation \\(b\\), then the distributions follow that:\n\\[\\alpha + a \\sim N(\\alpha + a,b)\\] \\[\\alpha a \\sim N(\\alpha a,|\\alpha|b)\\]\nFrom here we can begin to expand to multiple variables. If \\(f = Ax\\) where \\(x ~ N(\\mu,\\Sigma)\\) is a multidimensional random variable, then\n\\[E[f] = A\\mu\\]\nand\n\\[V[f] = A \\Sigma A^T\\]\nNow take a nonlinear \\(f(x)\\). By a Taylor expansion we have that\n\\[f(x) = f_0 + Jx + \\ldots\\]\ni.e. the linear approximation is \\(f_0 + Jx\\) where \\(f_0 = f(\\mu)\\) and \\(J\\) is the Jacobian matrix. If we do a pushforward on the linear approximation, we receive\n\\[f(x) ~ N(f(\\mu),J\\Sigma J^T)\\]\nwhich gives the rules for the pushforward on any possible function through the linearization. But the linearization is the same as the forward differencing ones, meaning that we can augment existing tooling for forward-mode automatic differentiation to perform pushforwards of uncertain quantities. A library which does this is Measurements.jl. Note that this library additionally tracks the correlations between each variable so that the second order terms are accurate.\n\n18.4.1 Measurements.jl in Practice: Measurements on DifferentialEquations\nSince DifferentialEquations.jl takes in arbitrary number types, we can have it recompile to do the arithmetics of uncertainty propagation. For example, the following solves the pendulum of arbitrary amplitude with respect to uncertain parameters and initial conditions:\n\\[\\ddot{\\theta} + \\frac{g}{L} \\sin(\\theta) = 0\\]\n\nusing OrdinaryDiffEq, Measurements\ngaccel = 9.79 ± 0.02; # Gravitational constants\nL = 1.00 ± 0.01; # Length of the pendulum\n\n#Initial Conditions\nu₀ = [0 ± 0, π / 3 ± 0.02] # Initial speed and initial angle\ntspan = (0.0, 6.3)\n\n#Define the problem\nfunction simplependulum(du,u,p,t)\n    θ  = u[1]\n    dθ = u[2]\n    du[1] = dθ\n    du[2] = -(gaccel / L) * sin(θ)\nend\n\n#Pass to solvers\nprob = ODEProblem(simplependulum, u₀, tspan)\nsol = solve(prob, Tsit5(), reltol = 1e-6)\n\nusing Plots\nplot(sol,plotdensity=100,vars=2)\n\n┌ Warning: To maintain consistency with solution indexing, keyword argument vars will be removed in a future version. Please use keyword argument idxs instead.\n│   caller = ip:0x0\n└ @ Core :-1\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom here it is clear that as the pendulum goes forward, the uncertainty grows since the exact period is unclear. Notice another nice feature is on display here: the Plots.jl Recipe System. The plotting just worked because the recipes are a type-recursive system. The three steps were:\n\nThe DifferentialEquations solution recipe transformed the ODE solution into an array of measurement variables\nThe Measurements recipe transformed the measurement variables into an array of floats along with a series of error bars\nThis array of floats was recognized as a native format, and thus the plot was made.\n\nNote that this idea of discretizing distributions and pushing them through a full calculation can be done with more accuracy by using things like orthogonal polynomial expansions. This is the polynomial chaos expansion approach which we will not cover, but there is a PolyChaos.jl package one can explore."
  },
  {
    "objectID": "uncertainty.html#quantifying-numerical-uncertainty-with-intervals",
    "href": "uncertainty.html#quantifying-numerical-uncertainty-with-intervals",
    "title": "18  Uncertainty Programming, Generalized Uncertainty Quantification",
    "section": "18.5 Quantifying Numerical Uncertainty with Intervals",
    "text": "18.5 Quantifying Numerical Uncertainty with Intervals\nWhile Measurements gives a sense of uncertainty quantification for unknown inputs, a different form of uncertainty quantification is that for floating point uncertainties. For example, when you calculate \\(sin(2.3)\\) on your computer, this has an error in the approximation, and what if we wanted to push these errors forward to get an interval which bounds the possible values given the numerical uncertainty? This is done via interval arithmetics.\nFor this we can use IntervalArithmetic.jl. The idea of interval arithmetic is to work rigorously on sets of real numbers, i.e.\n\\[[a,b] = \\{x\\in\\mathbb{R} : a\\leq x \\leq b\\}\\]\nWe can construct an interval given the interval method:\n\nusing IntervalArithmetic\nx = interval(0.1,0.2)\n\nWARNING: using IntervalArithmetic.± in module Main conflicts with an existing identifier.\n\n\n[0.1, 0.200001]\n\n\nor using the shorthand\n\n0.1..0.2\n\n[0.0999999, 0.200001]\n\n\nHere the operator catches the constants at compile time and notices that 0.1 and 0.2 cannot be exactly represented in floating point numbers, and thus on the left side it rounds down by one floating point number and on the right it rounds up by one floating point number to make sure the set rigorously contains the correct value.\nFrom here, non-monotone functions can propagate intervals like:\n\n[a, b]^2 := [a^2, b^2]  if 0 &lt; a &lt; b\n          = [0, max(a^2, b^2)]  if a &lt; 0 &lt; b\n          = [b^2, a^2] if a &lt; b &lt; 0\n\nThe rules can get fairly complicated and may need to be derived for each individual elementary function, but, just like automatic differentiation, recursion can be performed to get to a bottom of primitives which is known to then propagate forward the intervals.\nBecause this form of uncertainty quantification is rigorous, we can prove theorem on it. For example, let’s say we want to show that\n\nh(x) = x^2 - 2\n\nh (generic function with 1 method)\n\n\nhas no roots in \\([3,4]\\). Since these are rigorous bounds, it holds that\n\nh(3..4)\n\n[7, 14]\n\n\nis a rigorous bound on the possible values, and thus it must not have a root in this interval.\n\n18.5.1 Problems with Interval Arithmetic\nInterval arithmetic is nice… but it can have some issues. It’s rigorous but it’s also conservative, meaning that the intervals can be much larger than one would expect given the actual uncertainties seen in practice. One phenomena which causes this can be seen by looking at that pendulum:\n\ngaccel = 9.77..9.81; # Gravitational constants\nL = 0.99..1.01; # Length of the pendulum\n\n#Initial Conditions\nu₀ = [0..0, ((π / 3)-0.02)..((π / 3)+0.02)] # Initial speed and initial angle\ntspan = (0.0, 6.3)\n\n#Define the problem\nfunction simplependulum(du,u,p,t)\n    θ  = u[1]\n    dθ = u[2]\n    du[1] = dθ\n    du[2] = -(gaccel / L) * sin(θ)\nend\n\n#Pass to solvers\nprob = ODEProblem(simplependulum, u₀, tspan)\nsol = solve(prob, Tsit5(), adaptive=false, dt=0.1, reltol = 1e-6)\n\nretcode: Success\nInterpolation: specialized 4th order \"free\" interpolation\nt: 64-element Vector{Float64}:\n 0.0\n 0.1\n 0.2\n 0.30000000000000004\n 0.4\n 0.5\n 0.6\n 0.7\n 0.7999999999999999\n 0.8999999999999999\n 0.9999999999999999\n 1.0999999999999999\n 1.2\n ⋮\n 5.1999999999999975\n 5.299999999999997\n 5.399999999999997\n 5.4999999999999964\n 5.599999999999996\n 5.699999999999996\n 5.799999999999995\n 5.899999999999995\n 5.999999999999995\n 6.099999999999994\n 6.199999999999994\n 6.3\nu: 64-element Vector{Vector{Interval{Float64}}}:\n [[0, 0], [1.02719, 1.0672]]\n [[0.0768938, 0.129145], [0.649387, 1.34336]]\n [[-0.65616, 1.04792], [-4.56529, 6.2509]]\n [[-17.5149, 18.0655], [-12.0408, 13.5935]]\n [[-43.3001, 44.0059], [-19.5521, 21.1047]]\n [[-74.7788, 75.6399], [-27.0633, 28.616]]\n [[-111.952, 112.968], [-34.5745, 36.1272]]\n [[-154.818, 155.989], [-42.0857, 43.6384]]\n [[-203.377, 204.704], [-49.597, 51.1497]]\n [[-257.63, 259.113], [-57.1082, 58.6609]]\n [[-317.577, 319.215], [-64.6194, 66.1721]]\n [[-383.218, 385.01], [-72.1307, 73.6833]]\n [[-454.552, 456.5], [-79.6419, 81.1946]]\n ⋮\n [[-7976.68, 7984.84], [-380.091, 381.644]]\n [[-8281.46, 8289.77], [-387.603, 389.155]]\n [[-8591.92, 8600.39], [-395.114, 396.667]]\n [[-8908.08, 8916.71], [-402.625, 404.178]]\n [[-9229.93, 9238.71], [-410.136, 411.689]]\n [[-9557.48, 9566.42], [-417.648, 419.2]]\n [[-9890.72, 9899.81], [-425.159, 426.712]]\n [[-10229.7, 10238.9], [-432.67, 434.223]]\n [[-10574.3, 10583.7], [-440.181, 441.734]]\n [[-10924.6, 10934.2], [-447.693, 449.245]]\n [[-11280.7, 11290.4], [-455.204, 456.756]]\n [[-11642.4, 11652.2], [-462.715, 464.268]]\n\n\nWhile we start out with reasonably small intervals, it turns out that every operation is calculating “what is the largest I could be? What is the smallest I could be?”. Comparing these extremes at every operation means that, yes, by the end, given the uncertainty in the period, the solution lies in the interval \\([-11642.4,11652.2]\\), but that’s not a particularly helpful estimate! This demonstrates the exponential explosion of interval estimates.\nBut note that part of why it got so large is because we started with “such large” intervals. If we only used this to measure the uncertainty of the floating point arithmetic, then the intervals are much better contained:\n\ngaccel = 9.8..9.8; # Gravitational constants\nL = 1.0..1.0; # Length of the pendulum\n\n#Initial Conditions\nu₀ = [0..0, (π / 3)..(π / 3)] # Initial speed and initial angle\ntspan = (0.0, 6.3)\n\n#Define the problem\nfunction simplependulum(du,u,p,t)\n    θ  = u[1]\n    dθ = u[2]\n    du[1] = dθ\n    du[2] = -(gaccel / L) * sin(θ)\nend\n\n#Pass to solvers\nprob = ODEProblem(simplependulum, u₀, tspan)\nsol = solve(prob, Vern9(), adaptive=false, dt=0.001, reltol = 1e-6)\n\nretcode: Success\nInterpolation: specialized 9th order lazy interpolation\nt: 6301-element Vector{Float64}:\n 0.0\n 0.001\n 0.002\n 0.003\n 0.004\n 0.005\n 0.006\n 0.007\n 0.008\n 0.009000000000000001\n 0.010000000000000002\n 0.011000000000000003\n 0.012000000000000004\n ⋮\n 6.289000000000435\n 6.290000000000435\n 6.291000000000436\n 6.292000000000436\n 6.293000000000436\n 6.294000000000437\n 6.295000000000437\n 6.296000000000437\n 6.297000000000438\n 6.298000000000438\n 6.299000000000438\n 6.3\nu: 6301-element Vector{Vector{Interval{Float64}}}:\n [[0, 0], [1.04719, 1.0472]]\n [[0.00104719, 0.0010472], [1.04719, 1.0472]]\n [[0.00209438, 0.00209439], [1.04717, 1.04718]]\n [[0.00314154, 0.00314155], [1.04715, 1.04716]]\n [[0.00418868, 0.00418869], [1.04711, 1.04712]]\n [[0.00523577, 0.00523578], [1.04706, 1.04707]]\n [[0.00628281, 0.00628282], [1.04701, 1.04702]]\n [[0.00732979, 0.0073298], [1.04694, 1.04695]]\n [[0.0083767, 0.00837671], [1.04686, 1.04687]]\n [[0.00942353, 0.00942354], [1.04678, 1.04679]]\n [[0.0104702, 0.0104703], [1.04668, 1.04669]]\n [[0.0115168, 0.0115169], [1.04657, 1.04658]]\n [[0.0125634, 0.0125635], [1.04645, 1.04646]]\n ⋮\n [[-3.53441, 3.95227], [-6.87853, 8.84023]]\n [[-3.54751, 3.96734], [-6.89601, 8.8577]]\n [[-3.56065, 3.98244], [-6.91348, 8.87517]]\n [[-3.57381, 3.99756], [-6.93095, 8.89265]]\n [[-3.58701, 4.01272], [-6.94843, 8.91012]]\n [[-3.60024, 4.02791], [-6.9659, 8.9276]]\n [[-3.6135, 4.04313], [-6.98337, 8.94507]]\n [[-3.62679, 4.05838], [-7.00085, 8.96254]]\n [[-3.64011, 4.07367], [-7.01832, 8.98002]]\n [[-3.65346, 4.08898], [-7.0358, 8.99749]]\n [[-3.66684, 4.10432], [-7.05327, 9.01496]]\n [[-3.68026, 4.1197], [-7.07074, 9.03244]]"
  },
  {
    "objectID": "uncertainty.html#contextual-uncertainty-quantification",
    "href": "uncertainty.html#contextual-uncertainty-quantification",
    "title": "18  Uncertainty Programming, Generalized Uncertainty Quantification",
    "section": "18.6 Contextual Uncertainty Quantification",
    "text": "18.6 Contextual Uncertainty Quantification\nThose previous methods were non-contextual and worked directly through program modification. However, by not “clumping” interactions, uncertainty quantification can have overestimates like is seen with the interval growth. Thus, just like with reverse-mode AD, can we instead look for higher order uncertainty primitives on which to build such a system? When digging into reverse-mode AD, we saw that adjoint problems in engineering corresponded to the the reverse-mode rules for things like linear solve, eigenvalue problems, and the solution of ODEs. There does not seem to be a general analogue in the case of uncertainty quantification, but there is hope. Since this is a big enough field, people have found special cases where uncertainty can be quantified in interesting manners. Let’s look specifically at ODEs."
  },
  {
    "objectID": "uncertainty.html#quantifying-uncertainty-in-ode-solves-for-adaptivity",
    "href": "uncertainty.html#quantifying-uncertainty-in-ode-solves-for-adaptivity",
    "title": "18  Uncertainty Programming, Generalized Uncertainty Quantification",
    "section": "18.7 Quantifying Uncertainty in ODE Solves for Adaptivity",
    "text": "18.7 Quantifying Uncertainty in ODE Solves for Adaptivity\nHowever, in some sense, adaptive numerical methods work by embedding a form of uncertainty quantification. Let’s take a look at the Bogaki-Shampine method for solving ODEs:\nNotice that there’s a \\(y_{n+1}\\) and a \\(z_{n+1}\\) for two separate solutions for the next time step. It so happens that \\(y\\) is \\(\\mathcal{O}(\\Delta t^3)\\) while \\(z\\) is \\(\\mathcal{O}(\\Delta t^2)\\), meaning that \\(E = z_{n+1} - y_{n+1}\\) is a \\(\\mathcal{O}(\\Delta t^2)\\) estimate for the error in a given step, since the two must both be “\\(\\Delta t^2\\) close enough” to the true solution. Similarly, when we looked at the Dormand-Prince method in our homework, the tableau:\n\n\n\nDP tableau\n\n\nhad a second row as well, with the first being \\(\\mathcal{O}(n^5)\\) and the second being \\(\\mathcal{O}(n^4)\\). Thus these Runge-Kutta methods naturally have error estimators. In standard usage, they are compared to the tolerances, like:\n\\[q = \\frac{E}{\\text{reltol}\\max(z_n,z_{n+1}) + \\text{abstol}}\\]\nand when \\(q&lt;1\\), the \\(\\Delta t\\) gives an error larger than the tolerances and so the step is rejected, decreased, and tried again. In many cases, one may control the error proportionally to this error estimator, i.e. the next \\(\\Delta t\\) is the product \\(q \\Delta t\\).\nThat’s all for adapting to a tolerance, but can we use this to propagate uncertainties? It turns out we can. This is known as the ProbInts method. Essentially, instead of an ODE, we can think of having solved a stochastic differential equation whose additive noise term of size which matches our error estimate. Specifically, adding a noise which is normally distributed with mean zero and standard deviation \\((\\Delta t)^{p}\\), where \\(p\\) is the order of the adaptive error estimate (i.e. the order of the lower approximation), is an approximation to the possible values that could have occurred given the noise that was seen. By adding this at every step, we can then recover a distribution of possible solutions/trajectories.\n\n18.7.1 ProbInts in Action\n\nusing DiffEqUncertainty\nfunction fitz(du,u,p,t)\n  V,R = u\n  a,b,c = p\n  du[1] = c*(V - V^3/3 + R)\n  du[2] = -(1/c)*(V -  a - b*R)\nend\nu0 = [-1.0;1.0]\ntspan = (0.0,20.0)\np = (0.2,0.2,3.0)\nprob = ODEProblem(fitz,u0,tspan,p)\n\ncb = AdaptiveProbIntsUncertainty(5) # 5th order method\nsol = solve(prob,Tsit5())\nensemble_prob = EnsembleProblem(prob)\nsim = solve(ensemble_prob,Tsit5(),trajectories=100,callback=cb)\nplot(sim,vars=(0,1),linealpha=0.4)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncb = AdaptiveProbIntsUncertainty(5)\nsol = solve(prob,Tsit5())\nensemble_prob = EnsembleProblem(prob)\nsim = solve(ensemble_prob,Tsit5(),trajectories=100,callback=cb,abstol=1e-3,reltol=1e-1)\nplot(sim,vars=(0,1),linealpha=0.4)\n\n┌ Warning: Instability detected. Aborting\n└ @ SciMLBase ~/.julia/packages/SciMLBase/szsYq/src/integrator_interface.jl:606\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice that while an interval estimate would have grown to allow all extremes together, this form keeps the trajectories alive, allowing them to fall back to the mode, which decreases the true uncertainty. This is thus a good explanation as to why general methods will overestimate uncertainty."
  },
  {
    "objectID": "uncertainty.html#adjoints-of-uncertainty-and-the-koopman-operator",
    "href": "uncertainty.html#adjoints-of-uncertainty-and-the-koopman-operator",
    "title": "18  Uncertainty Programming, Generalized Uncertainty Quantification",
    "section": "18.8 Adjoints of Uncertainty and the Koopman Operator",
    "text": "18.8 Adjoints of Uncertainty and the Koopman Operator\nEverything that we’ve demonstrated here so far can be thought of as “forward mode uncertainty quantification”. For every example we have constructed a method such that, for a known probability distribution in x, we build the probability distribution of the output of the program, and then compute quantities from that. On a dynamical system this pushforward of a measure is denoted by the Frobenius-Perron operator. With a pushforward operator \\(P\\) and an initial uncertainty density \\(f\\), we can represent calculating the expected value of some cost function on the solution via:\n\\[\\mathbb{E}[g(x)|X \\sim Pf] = \\int_{S(A)} P f(x) g(x) dx\\]\nwhere \\(S\\) is the program, i.e. \\(S(A)\\) is the total set of points by pushing every value of \\(A\\) through our program, and \\(P f(x)\\) is the pushforward operator applied to the probability distribution. What this means is that, to calculate the expectation on the output of our program, like to calculate the mean value of the ODE’s solution given uncertainty in the parameters, we can pushforward the probability distribution to construct \\(Pf\\) and on this probability distribution calculate the expected value of some \\(g\\) cost function on the solution.\nThe problem, as seen earlier, is that pushing forward entire probability distributions is a fairly expensive process. We can instead think about doing the adjoint to this cost function, i.e. pulling back the cost function and computing it on the initial density. In terms of inner product notation, this would be doing:\n\\[\\langle Pf,g \\rangle = \\langle f, Ug \\rangle\\]\nmeaning \\(U\\) is the adjoint operator to the pushforward \\(P\\). This operator is known as the Koopman operator. There are many properties one can use about the Koopman operator, one special property being it’s a linear operator on the space of observables, but it also gives a nice expression for computing uncertainty expectations. Using the Koopman operator, we can rewrite the expectation as:\n\\[\\mathbb{E}[g(x)|X \\sim Pf] = \\mathbb{E}[Ug(x)|X \\sim f]\\]\nor perform the integral on the pullback of the cost function, i.e.\n\\[\\mathbb{E}[g(x)|X \\sim f] = \\int_A Ug(x) f(x) dx\\]\nIn images it looks like:\n\n\n\nKoopman vs FP\n\n\nThis expression gives us a fast way to compute expectations on the program output without having to compute the full uncertainty distribution on the output. This can thus be used for optimization under uncertainty, i.e. the optimization of loss functions with respect to expectations of the program’s output under the assumption of given input uncertainty distributions. For more information, see The Koopman Expectation: An Operator Theoretic Method for Efficient Analysis and Optimization of Uncertain Hybrid Dynamical Systems."
  }
]