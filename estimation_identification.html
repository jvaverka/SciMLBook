<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>MIT Parallel Computing and Scientific Machine Learning - 9&nbsp; Basic Parameter Estimation, Reverse-Mode AD, and Inverse Problems</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./adjoints.html" rel="next">
<link href="./stiff_odes.html" rel="prev">
<link href="./sciml-book-logo.svg" rel="icon" type="image/svg+xml">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./automatic_differentiation.html">Modern Approaches</a></li><li class="breadcrumb-item"><a href="./estimation_identification.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Basic Parameter Estimation, Reverse-Mode AD, and Inverse Problems</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./sciml-book-logo.svg" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">MIT Parallel Computing and Scientific Machine Learning</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/jvaverka/SciMLBook" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./MIT-Parallel-Computing-and-Scientific-Machine-Learning.pdf">
              <i class="bi bi-bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./MIT-Parallel-Computing-and-Scientific-Machine-Learning.epub">
              <i class="bi bi-bi-journal pe-1"></i>
            Download ePub
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction to Parallel Computing</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./optimize.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Optimizing Serial Code</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sciml.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction to Scientific Machine Learning through Physics-Informed Neural Networks</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Parallel Computing</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dynamical_systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">How Loops Work, An Introduction to Discrete Dynamics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./parallelism_overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">The Basics of Single Node Parallel Computing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./styles_of_parallelism.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">The Different Flavors of Parallelism</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./discretizing_odes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Ordinary Differential Equations, Applications and Discretizations</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Modern Approaches</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./automatic_differentiation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Forward-Mode Automatic Differentiation (AD) via High Dimensional Algebras</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./stiff_odes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Solving Stiff Ordinary Differential Equations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./estimation_identification.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Basic Parameter Estimation, Reverse-Mode AD, and Inverse Problems</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./adjoints.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Differentiable Programming and Neural Differential Equations</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Modern Architectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mpi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Introduction to MPI.jl</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./gpus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">GPU programming</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">Advanced Differential Equations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pdes_and_convolutions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">PDEs, Convolutions, and the Mathematics of Locality</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./diffeq_machine_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Mixing Differential Equations and Neural Networks for Physics-Informed Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./probabilistic_programming.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">From Optimization to Probabilistic Programming</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./global_sensitivity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Global Sensitivity Analysis</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
 <span class="menu-text">Advanced Topics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./profiling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Code Profiling and Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./uncertainty.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Uncertainty Programming, Generalized Uncertainty Quantification</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#youtube-video-link" id="toc-youtube-video-link" class="nav-link active" data-scroll-target="#youtube-video-link"><span class="header-section-number">9.1</span> Youtube Video Link</a></li>
  <li><a href="#the-shooting-method-for-parameter-fitting" id="toc-the-shooting-method-for-parameter-fitting" class="nav-link" data-scroll-target="#the-shooting-method-for-parameter-fitting"><span class="header-section-number">9.2</span> The Shooting Method for Parameter Fitting</a>
  <ul class="collapse">
  <li><a href="#methods-for-optimization" id="toc-methods-for-optimization" class="nav-link" data-scroll-target="#methods-for-optimization"><span class="header-section-number">9.2.1</span> Methods for Optimization</a></li>
  <li><a href="#connection-between-optimization-and-differential-equations" id="toc-connection-between-optimization-and-differential-equations" class="nav-link" data-scroll-target="#connection-between-optimization-and-differential-equations"><span class="header-section-number">9.2.2</span> Connection Between Optimization and Differential Equations</a></li>
  <li><a href="#neural-network-training-as-a-shooting-method-for-functions" id="toc-neural-network-training-as-a-shooting-method-for-functions" class="nav-link" data-scroll-target="#neural-network-training-as-a-shooting-method-for-functions"><span class="header-section-number">9.2.3</span> Neural Network Training as a Shooting Method for Functions</a></li>
  <li><a href="#recurrent-neural-networks" id="toc-recurrent-neural-networks" class="nav-link" data-scroll-target="#recurrent-neural-networks"><span class="header-section-number">9.2.4</span> Recurrent Neural Networks</a></li>
  </ul></li>
  <li><a href="#computing-gradients" id="toc-computing-gradients" class="nav-link" data-scroll-target="#computing-gradients"><span class="header-section-number">9.3</span> Computing Gradients</a>
  <ul class="collapse">
  <li><a href="#forward-mode-automatic-differentiation-for-gradients" id="toc-forward-mode-automatic-differentiation-for-gradients" class="nav-link" data-scroll-target="#forward-mode-automatic-differentiation-for-gradients"><span class="header-section-number">9.3.1</span> Forward-Mode Automatic Differentiation for Gradients</a></li>
  <li><a href="#the-adjoint-technique-and-reverse-accumulation" id="toc-the-adjoint-technique-and-reverse-accumulation" class="nav-link" data-scroll-target="#the-adjoint-technique-and-reverse-accumulation"><span class="header-section-number">9.3.2</span> The Adjoint Technique and Reverse Accumulation</a></li>
  <li><a href="#logistic-regression-example" id="toc-logistic-regression-example" class="nav-link" data-scroll-target="#logistic-regression-example"><span class="header-section-number">9.3.3</span> Logistic Regression Example</a></li>
  <li><a href="#backpropagation-of-a-neural-network" id="toc-backpropagation-of-a-neural-network" class="nav-link" data-scroll-target="#backpropagation-of-a-neural-network"><span class="header-section-number">9.3.4</span> Backpropagation of a Neural Network</a></li>
  <li><a href="#reverse-mode-automatic-differentiation-and-vjps" id="toc-reverse-mode-automatic-differentiation-and-vjps" class="nav-link" data-scroll-target="#reverse-mode-automatic-differentiation-and-vjps"><span class="header-section-number">9.3.5</span> Reverse-Mode Automatic Differentiation and vjps</a></li>
  <li><a href="#primitives-of-reverse-mode" id="toc-primitives-of-reverse-mode" class="nav-link" data-scroll-target="#primitives-of-reverse-mode"><span class="header-section-number">9.3.6</span> Primitives of Reverse Mode</a></li>
  <li><a href="#multivariate-derivatives-from-reverse-mode" id="toc-multivariate-derivatives-from-reverse-mode" class="nav-link" data-scroll-target="#multivariate-derivatives-from-reverse-mode"><span class="header-section-number">9.3.7</span> Multivariate Derivatives from Reverse Mode</a></li>
  <li><a href="#multi-seeding" id="toc-multi-seeding" class="nav-link" data-scroll-target="#multi-seeding"><span class="header-section-number">9.3.8</span> Multi-Seeding</a></li>
  <li><a href="#sparse-reverse-mode-ad" id="toc-sparse-reverse-mode-ad" class="nav-link" data-scroll-target="#sparse-reverse-mode-ad"><span class="header-section-number">9.3.9</span> Sparse Reverse Mode AD</a></li>
  <li><a href="#forward-mode-vs-reverse-mode" id="toc-forward-mode-vs-reverse-mode" class="nav-link" data-scroll-target="#forward-mode-vs-reverse-mode"><span class="header-section-number">9.3.10</span> Forward Mode vs Reverse Mode</a></li>
  <li><a href="#side-note-on-mixed-mode" id="toc-side-note-on-mixed-mode" class="nav-link" data-scroll-target="#side-note-on-mixed-mode"><span class="header-section-number">9.3.11</span> Side Note on Mixed Mode</a></li>
  <li><a href="#forward-over-reverse-and-hessian-free-products" id="toc-forward-over-reverse-and-hessian-free-products" class="nav-link" data-scroll-target="#forward-over-reverse-and-hessian-free-products"><span class="header-section-number">9.3.12</span> Forward-Over-Reverse and Hessian-Free Products</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">9.3.13</span> References</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/jvaverka/SciMLBook/edit/main/estimation_identification.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/jvaverka/SciMLBook/issues/new" class="toc-action">Report an issue</a></p><p><a href="https://github.com/jvaverka/SciMLBook/blob/main/estimation_identification.qmd" class="toc-action">View source</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Basic Parameter Estimation, Reverse-Mode AD, and Inverse Problems</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="youtube-video-link" class="level2" data-number="9.1">
<h2 data-number="9.1" class="anchored" data-anchor-id="youtube-video-link"><span class="header-section-number">9.1</span> <a href="https://youtu.be/XQAe4pEZ6L4">Youtube Video Link</a></h2>
<p>Have a model. Have data. Fit model to data.</p>
<p>This is a problem that goes under many different names: <em>parameter estimation</em>, <em>inverse problems</em>, <em>training</em>, etc. In this lecture we will go through the methods for how that’s done, starting with the basics and bringing in the recent techniques from machine learning that can be used to improve the basic implementations.</p>
</section>
<section id="the-shooting-method-for-parameter-fitting" class="level2" data-number="9.2">
<h2 data-number="9.2" class="anchored" data-anchor-id="the-shooting-method-for-parameter-fitting"><span class="header-section-number">9.2</span> The Shooting Method for Parameter Fitting</h2>
<p>Assume that we have some model <span class="math inline">\(u = f(p)\)</span> where <span class="math inline">\(p\)</span> is our parameters, where we put in some parameters and receive our simulated data <span class="math inline">\(u\)</span>. How should you choose <span class="math inline">\(p\)</span> such that <span class="math inline">\(u\)</span> best fits that data? The <em>shooting method</em> directly uses this high level definition of the model by putting a cost function on the output <span class="math inline">\(C(p)\)</span>. This cost function is dependent on a user-choice and it’s model-dependent. However, a common one is the L2-loss. If <span class="math inline">\(y\)</span> is our expected data, then the L2-loss function against the data is simply:</p>
<p><span class="math display">\[C(p) = \Vert f(p) - y \Vert\]</span></p>
<p>where <span class="math inline">\(C(p): \mathbb{R}^n \rightarrow \mathbb{R}\)</span> is a function that returns a scalar. The shooting method then directly optimizes this cost function by having the optimizer generate a data given new choices of <span class="math inline">\(p\)</span>.</p>
<section id="methods-for-optimization" class="level3" data-number="9.2.1">
<h3 data-number="9.2.1" class="anchored" data-anchor-id="methods-for-optimization"><span class="header-section-number">9.2.1</span> Methods for Optimization</h3>
<p>There are many different nonlinear optimization methods which can be used for this purpose, and for a full survey one should look at packages like <a href="https://github.com/JuliaOpt/JuMP.jl">JuMP</a>, <a href="https://github.com/JuliaNLSolvers/Optim.jl">Optim.jl</a>, and <a href="https://github.com/JuliaOpt/NLopt.jl">NLopt.jl</a>.</p>
<p>There are generally two sets of methods: global and local optimization methods. Local optimization methods attempt to find the best nearby extrema by finding a point where the gradient <span class="math inline">\(\frac{dC}{dp} = 0\)</span>. Global optimization methods attempt to explore the whole space and find the best of the extrema. Global methods tend to employ a lot more heuristics and are extremely computationally difficult, and thus many studies focus on local optimization. We will focus strictly on local optimization, but one may want to look into global optimization for many applications of parameter estimation.</p>
<p>Most local optimizers make use of derivative information in order to accelerate the solver. The simplest of which is the method of <em>gradient descent</em>. In this method, given a set of parameters <span class="math inline">\(p_i\)</span>, the next step of parameters one will try is:</p>
<p><span class="math display">\[p_{i+1} = p_i - \alpha \frac{dC}{dP}\]</span></p>
<p>that is, update <span class="math inline">\(p_i\)</span> by walking in the downward direction of the gradient. Instead of using just first order information, one may want to directly solve the rootfinding problem <span class="math inline">\(\frac{dC}{dp} = 0\)</span> using Newton’s method. Newton’s method in this case looks like:</p>
<p><span class="math display">\[p_{i+1} = p_i - (\frac{d}{dp}\frac{dC}{dp})^{-1} \frac{dC}{dp}\]</span></p>
<p>But notice that the Jacobian of the gradient is the Hessian, and thus we can rewrite this as:</p>
<p><span class="math display">\[p_{i+1} = p_i - H(p_i)^{-1} \frac{dC(p_i)}{dp}\]</span></p>
<p>where <span class="math inline">\(H(p)\)</span> is the Hessian matrix <span class="math inline">\(H_{ij} = \frac{dC}{dx_i dx_j}\)</span>. However, solving a system of equations which involves the Hessian can be difficult (just like the Jacobian, but now with another layer of differentiation!), and thus many optimization techniques attempt to avoid the Hessian. A commonly used technique that is somewhat in the middle is the <em>BFGS</em> technique, which is a gradient-based optimization method that attempts to approximate the Hessian along the way to modify its stepping behavior. It uses the history of previously calculated points in order to build this quick Hessian approximate. If one keeps only a constant length history, say 5 time points, then one arrives at the <em>l-BFGS</em> technique, which is one of the most common large-scale optimization techniques.</p>
</section>
<section id="connection-between-optimization-and-differential-equations" class="level3" data-number="9.2.2">
<h3 data-number="9.2.2" class="anchored" data-anchor-id="connection-between-optimization-and-differential-equations"><span class="header-section-number">9.2.2</span> Connection Between Optimization and Differential Equations</h3>
<p>There is actually a strong connection between optimization and differential equations. Let’s say we wanted to follow the gradient of the solution towards a local minimum. That would mean that the flow that we would wish to follow is given by an ODE, specifically the ODE:</p>
<p><span class="math display">\[p' = -\frac{dC}{dp}\]</span></p>
<p>If we apply the Euler method to this ODE, then we receive</p>
<p><span class="math display">\[p_{n+1} = p_n - \alpha \frac{dC(p_n)}{dp}\]</span></p>
<p>and we thus recover the gradient descent method. Now assume that you want to use implicit Euler. Then we would have the system</p>
<p><span class="math display">\[p_{n+1} = p_n - \alpha \frac{dC(p_{n+1})}{dp}\]</span></p>
<p>which we would then move to one side:</p>
<p><span class="math display">\[p_{n+1} - p_n + \alpha \frac{dC(p_{n+1})}{dp} = 0\]</span></p>
<p>and solve each step via a Newton method. For this Newton method, we need to take the Jacobian of this gradient function, and once again the Hessian arrives as the fundamental quantity.</p>
</section>
<section id="neural-network-training-as-a-shooting-method-for-functions" class="level3" data-number="9.2.3">
<h3 data-number="9.2.3" class="anchored" data-anchor-id="neural-network-training-as-a-shooting-method-for-functions"><span class="header-section-number">9.2.3</span> Neural Network Training as a Shooting Method for Functions</h3>
<p>A one layer dense neuron is traditionally written as the function:</p>
<p><span class="math display">\[layer(x) = \sigma.(Wx + b)\]</span></p>
<p>where <span class="math inline">\(x \in \mathbb{R}^n\)</span>, <span class="math inline">\(W \in \mathbb{R}^{m \times n}\)</span>, <span class="math inline">\(b \in \mathbb{R}^{m}\)</span> and <span class="math inline">\(\sigma\)</span> is some choice of <span class="math inline">\(\mathbb{R}\rightarrow\mathbb{R}\)</span> nonlinear function, where the <code>.</code> is the Julia dot to signify element-wise operation.</p>
<p>A traditional <em>neural network</em>, <em>feed-forward network</em>, or <em>multi-layer perceptron</em> is a 3 layer function, i.e.</p>
<p><span class="math display">\[NN(x) = W_3 \sigma_2.(W_2\sigma_1.(W_1x + b_1) + b_2) + b_3\]</span></p>
<p>where the first layer is called the input layer, the second is called the hidden layer, and the final is called the output layer. This specific function was seen as desirable because of the <em>Universal Approximation Theorem</em>, which is formally stated as follows:</p>
<p>Let <span class="math inline">\(\sigma\)</span> be a nonconstant, bounded, and continuous function. Let <span class="math inline">\(I_m = [0,1]^m\)</span>. The space of real-valued continuous functions on <span class="math inline">\(I_m\)</span> is denoted by <span class="math inline">\(C(I_m)\)</span>. For any <span class="math inline">\(\epsilon &gt;0\)</span> and any <span class="math inline">\(f\in C(I_m)\)</span>, there exists an integer <span class="math inline">\(N\)</span>, real constants <span class="math inline">\(W_i\)</span> and <span class="math inline">\(b_i\)</span> s.t.</p>
<p><span class="math display">\[\Vert NN(x) - f(x) \Vert &lt; \epsilon\]</span></p>
<p>for all <span class="math inline">\(x \in I_m\)</span>. Equivalently, <span class="math inline">\(NN\)</span> given parameters is dense in <span class="math inline">\(C(I_m)\)</span>.</p>
<p>However, it turns out that using only one hidden layer can require exponential growth in the size of said hidden layer, where the size is given by the number of columns in <span class="math inline">\(W_1\)</span>. To counteract this, <em>deep neural networks</em> were developed to be in the form of the recurrence relation:</p>
<p><span class="math display">\[v_{i+1} = \sigma_i.(W_i v_{i} + b_i)\]</span> <span class="math display">\[v_1 = x\]</span> <span class="math display">\[DNN(x) = v_{n}\]</span></p>
<p>for some <span class="math inline">\(n\)</span> where <span class="math inline">\(n\)</span> is the number of <em>layers</em>. Given a sufficient size of the hidden layers, this kind of function is a universal approximator (2017). Although it’s not quite known yet, some results have shown that this kind of function is able to fit high dimensional functions without the <em>curse of dimensionality</em>, i.e.&nbsp;the number of parameters does not grow exponentially with the input size. More mathematical results in this direction are still being investigated.</p>
<p>However, this theory gives a direct way to transform the fitting of an arbitrary function into a parameter shooting problem. Given an unknown function <span class="math inline">\(f\)</span> one wishes to fit, one can place the cost function</p>
<p><span class="math display">\[C(p) = \Vert DNN(x;p) - f(x) \Vert\]</span></p>
<p>where <span class="math inline">\(DNN(x;p)\)</span> signifies the deep neural network given by the parameters <span class="math inline">\(p\)</span>, where the full set of parameters is the <span class="math inline">\(W_i\)</span> and <span class="math inline">\(b_i\)</span>. To make the evaluation of that function be practical, we can instead say we wish to evaluate the difference at finitely many points:</p>
<p><span class="math display">\[C(p) = \sum_k^N \Vert DNN(x_k;p) - f(x_k) \Vert\]</span></p>
<p><em>Training</em> a neural network is machine learning speak for finding the <span class="math inline">\(p\)</span> which minimizes this cost function. Notice that this is then a shooting method problem, where a cost function is defined by direct evaluations of the model with some choice of parameters.</p>
</section>
<section id="recurrent-neural-networks" class="level3" data-number="9.2.4">
<h3 data-number="9.2.4" class="anchored" data-anchor-id="recurrent-neural-networks"><span class="header-section-number">9.2.4</span> Recurrent Neural Networks</h3>
<p>Recurrent neural networks are networks which are given by the recurrence relation:</p>
<p><span class="math display">\[x_{k+1} = x_k + DNN(x_k,k;p)\]</span></p>
<p>Given our machinery, we can see this is equivalent to the Euler discretization with <span class="math inline">\(\Delta t = 1\)</span> on the <em>neural ordinary differential equation</em> defined by:</p>
<p><span class="math display">\[x' = DNN(x,t;p)\]</span></p>
<p>Thus a recurrent neural network is a sequence of applications of a neural network (or possibly a neural network indexed by integer time).</p>
</section>
</section>
<section id="computing-gradients" class="level2" data-number="9.3">
<h2 data-number="9.3" class="anchored" data-anchor-id="computing-gradients"><span class="header-section-number">9.3</span> Computing Gradients</h2>
<p>This shows that many different problems, from training neural networks to fitting differential equations, all have the same underlying mathematical structure which requires the ability to compute the gradient of a cost function given model evaluations. However, this simply reduces to computing the gradient of the model’s output given the parameters. To see this, let’s take for example the L2 loss function, i.e.</p>
<p><span class="math display">\[C(p) = \sum_i^N \Vert f(x_i;p) - y_i \Vert\]</span></p>
<p>for some finite data points <span class="math inline">\(y_i\)</span>. In the ODE model, <span class="math inline">\(y_i\)</span> are time series points. In the general neural network, <span class="math inline">\(y_i = d(x_i)\)</span> for the function we wish to fit <span class="math inline">\(d\)</span>. In data science applications of machine learning, <span class="math inline">\(y_i = d_i\)</span> the discrete data points we wish to fit. In any of these cases, we see that by the chain rule we have</p>
<p><span class="math display">\[\frac{dC}{dp} = \sum_i^N 2 \left(f(x_i;p) - y_i \right) \frac{df(x_i)}{dp}\]</span></p>
<p>and therefore, knowing how to efficiently compute <span class="math display">\[\frac{df(x_i)}{dp}\]</span> is the essential question for shooting-based parameter fitting.</p>
<section id="forward-mode-automatic-differentiation-for-gradients" class="level3" data-number="9.3.1">
<h3 data-number="9.3.1" class="anchored" data-anchor-id="forward-mode-automatic-differentiation-for-gradients"><span class="header-section-number">9.3.1</span> Forward-Mode Automatic Differentiation for Gradients</h3>
<p>Let’s recall the forward-mode method for computing gradients. For an arbitrary nonlinear function <span class="math inline">\(f\)</span> with scalar output, we can compute derivatives by putting a dual number in. For example, with</p>
<p><span class="math display">\[d = d_0 + v_1 \epsilon_1 + \ldots + v_m \epsilon_m\]</span></p>
<p>we have that</p>
<p><span class="math display">\[f(d) = f(d_0) + f'(d_0)v_1 \epsilon_1 + \ldots + f'(d_0)v_m \epsilon_m\]</span></p>
<p>where <span class="math inline">\(f'(d_0)v_i\)</span> is the direction derivative in the direction of <span class="math inline">\(v_i\)</span>. To compute the gradient with respond to the input, we thus need to make <span class="math inline">\(v_i = e_i\)</span>.</p>
<p>However, in this case we now do not want to compute the derivative with respect to the input! Instead, now we have <span class="math inline">\(f(x;p)\)</span> and want to compute the derivatives with respect to <span class="math inline">\(p\)</span>. This simply means that we want to take derivatives in the directions of the parameters. To do this, let:</p>
<p><span class="math display">\[x = x_0 + 0 \epsilon_1 + \ldots + 0 \epsilon_k\]</span> <span class="math display">\[P = p + e_1 \epsilon_1 + \ldots + e_k \epsilon_k\]</span></p>
<p>where there are <span class="math inline">\(k\)</span> parameters. We then have that</p>
<p><span class="math display">\[f(x;P) = f(x;p) + \frac{df}{dp_1} \epsilon_1 + \ldots + \frac{df}{dp_k} \epsilon_k\]</span></p>
<p>as the output, and thus a <span class="math inline">\(k+1\)</span>-dimensional number computes the gradient of the function with respect to <span class="math inline">\(k\)</span> parameters.</p>
<p>Can we do better?</p>
</section>
<section id="the-adjoint-technique-and-reverse-accumulation" class="level3" data-number="9.3.2">
<h3 data-number="9.3.2" class="anchored" data-anchor-id="the-adjoint-technique-and-reverse-accumulation"><span class="header-section-number">9.3.2</span> The Adjoint Technique and Reverse Accumulation</h3>
<p>The fast method for computing gradients goes under many times. The <em>adjoint technique</em>, <em>backpropagation</em>, and <em>reverse-mode automatic differentiation</em> are in some sense all equivalent phrases given to this method from different disciplines. To understand the adjoint technique, we will look at the multivariate chain rule on a <em>computation graph</em>. Recall that for <span class="math inline">\(f(x(t),y(t))\)</span> that we have:</p>
<p><span class="math display">\[\frac{df}{dt} = \frac{df}{dx}\frac{dx}{dt} + \frac{df}{dy}\frac{dy}{dt}\]</span></p>
<p>We can visualize our direct dependences as the computation graph:</p>
<p><img src="https://user-images.githubusercontent.com/1814174/66461367-e3162380-ea46-11e9-8e80-09b32e138269.PNG" class="img-fluid"></p>
<p>i.e.&nbsp;<span class="math inline">\(t\)</span> directly determines <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> which then determines <span class="math inline">\(f\)</span>. To calculate Assume you’ve already evaluated <span class="math inline">\(f(t)\)</span>. If this has been done, then you’ve already had to calculate <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. Thus given the function <span class="math inline">\(f\)</span>, we can now calculate <span class="math inline">\(\frac{df}{dx}\)</span> and <span class="math inline">\(\frac{df}{dy}\)</span>, and then calculate <span class="math inline">\(\frac{dx}{dt}\)</span> and <span class="math inline">\(\frac{dy}{dt}\)</span>.</p>
<p>Now let’s put another layer in the computation. Let’s make <span class="math inline">\(f(x(v(t),w(t)),y(v(t),w(t))\)</span>. We can write out the full expression for the derivative. Notice that even with this additional layer, the statement we wrote above still holds:</p>
<p><span class="math display">\[\frac{df}{dt} = \frac{df}{dx}\frac{dx}{dt} + \frac{df}{dy}\frac{dy}{dt}\]</span></p>
<p>So given an evaluation of <span class="math inline">\(f\)</span>, we can (still) directly calculate <span class="math inline">\(\frac{df}{dx}\)</span> and <span class="math inline">\(\frac{df}{dy}\)</span>. But now, to calculate <span class="math inline">\(\frac{dx}{dt}\)</span> and <span class="math inline">\(\frac{dy}{dt}\)</span>, we do the next step of the chain rule:</p>
<p><span class="math display">\[\frac{dx}{dt} = \frac{dx}{dv}\frac{dv}{dt} + \frac{dx}{dw}\frac{dw}{dt}\]</span></p>
<p>and similar for <span class="math inline">\(y\)</span>. So plug it all in, and you see that our equations will grow wild if we actually try to plug it in! But it’s clear that, to calculate <span class="math display">\[\frac{df}{dt}\]</span>, we can first calculate <span class="math inline">\(\frac{df}{dx}\)</span>, and then multiply that to <span class="math inline">\(\frac{dx}{dt}\)</span>. If we had more layers, we could calculate the <em>sensitivity</em> (the derivative) of the output to the last layer, then and then the sensitivity to the second layer back is the sensitivity of the last layer multiplied to that, and the third layer back has the sensitivity of the second layer multiplied to it!</p>
</section>
<section id="logistic-regression-example" class="level3" data-number="9.3.3">
<h3 data-number="9.3.3" class="anchored" data-anchor-id="logistic-regression-example"><span class="header-section-number">9.3.3</span> Logistic Regression Example</h3>
<p>To better see this structure, let’s write out a simple example. Let our <em>forward pass</em> through our function be:</p>
<p><span class="math display">\[\begin{align}
z &amp;= wx + b\\
y &amp;= \sigma(z)\\
\mathcal{L} &amp;= \frac{1}{2}(y-t)^2\\
\mathcal{R} &amp;= \frac{1}{2}w^2\\
\mathcal{L}_{reg} &amp;= \mathcal{L} + \lambda \mathcal{R}\end{align}\]</span></p>
<p><img src="https://user-images.githubusercontent.com/1814174/66462825-e2cb5780-ea49-11e9-9804-240037fb6b56.PNG" class="img-fluid"></p>
<p>The formulation of the program here is called a <em>Wengert list, tape, or graph</em>. In this, <span class="math inline">\(x\)</span> and <span class="math inline">\(t\)</span> are inputs, <span class="math inline">\(b\)</span> and <span class="math inline">\(W\)</span> are parameters, <span class="math inline">\(z\)</span>, <span class="math inline">\(y\)</span>, <span class="math inline">\(\mathcal{L}\)</span>, and <span class="math inline">\(\mathcal{R}\)</span> are intermediates, and <span class="math inline">\(\mathcal{L}_{reg}\)</span> is our output.</p>
<p>This is a simple univariate logistic regression model. To do logistic regression, we wish to find the parameters <span class="math inline">\(w\)</span> and <span class="math inline">\(b\)</span> which minimize the distance of <span class="math inline">\(\mathcal{L}_{reg}\)</span> from a desired output, which is done by computing derivatives.</p>
<p>Let’s calculate the derivatives with respect to each quantity in reverse order. If our program is <span class="math inline">\(f(x) = \mathcal{L}_{reg}\)</span>, then we have that</p>
<p><span class="math display">\[\frac{df}{d\mathcal{L}_{reg}} = 1\]</span></p>
<p>as the derivatives of the last layer. To computerize our notation, let’s write</p>
<p><span class="math display">\[\overline{\mathcal{L}_{reg}} = \frac{df}{d\mathcal{L}_{reg}}\]</span></p>
<p>for our computed values. For the derivatives of the second to last layer, we have that:</p>
<p><span class="math display">\[\begin{align}
  \overline{\mathcal{R}} &amp;= \frac{df}{d\mathcal{L}_{reg}} \frac{d\mathcal{L}_{reg}}{d\mathcal{R}}\\
                         &amp;= \overline{\mathcal{L}_{reg}} \lambda \end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\overline{\mathcal{L}} &amp;= \frac{df}{d\mathcal{L}_{reg}} \frac{d\mathcal{L}_{reg}}{d\mathcal{L}}\\
                        &amp;= \overline{\mathcal{L}_{reg}} \end{align}\]</span></p>
<p>This was our observation from before that the derivative of the second layer is the partial derivative of the current values times the sensitivity of the final layer. And then we keep multiplying, so now for our next layer we have that:</p>
<p><span class="math display">\[\begin{align}
  \overline{y} &amp;= \overline{\mathcal{L}} \frac{d\mathcal{L}}{dy}\\
               &amp;= \overline{\mathcal{L}} (y-t) \end{align}\]</span></p>
<p>And notice that the chain rule holds since <span class="math inline">\(\overline{\mathcal{L}}\)</span> implicitly already has the multiplication by <span class="math inline">\(\overline{\mathcal{L}_{reg}}\)</span> inside of it. Then the next layer is:</p>
<p><span class="math display">\[\begin{align}
\frac{df}{z} &amp;= \overline{y} \frac{dy}{dz}\\
              &amp;= \overline{y} \sigma^\prime(z) \end{align}\]</span></p>
<p>Then the next layer. Notice that here, by the chain rule on <span class="math inline">\(w\)</span> we have that:</p>
<p><span class="math display">\[\begin{align}
  \overline{w} &amp;= \overline{z} \frac{\partial z}{\partial w} + \overline{\mathcal{R}} \frac{d \mathcal{R}}{dw}\\
               &amp;= \overline{z} x + \overline{\mathcal{R}} w\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\overline{b} &amp;= \overline{z} \frac{\partial z}{\partial b}\\
              &amp;= \overline{z} \end{align}\]</span></p>
<p>This completely calculates all derivatives. In conclusion, the rule is:</p>
<ul>
<li>You sum terms from each outward arrow</li>
<li>Each arrow has the derivative term of the end times the partial of the current term.</li>
<li>Recurse backwards to build simple linear combination expressions.</li>
</ul>
<p>You can thus think of the relations as a message passing relation in reverse to the forward pass:</p>
<p><img src="https://user-images.githubusercontent.com/1814174/66466679-1b226400-ea51-11e9-9e3c-5cc7939c243b.PNG" class="img-fluid"></p>
<p>Note that the reverse-pass has the values of the forward pass, like <span class="math inline">\(x\)</span> and <span class="math inline">\(t\)</span>, embedded within it.</p>
</section>
<section id="backpropagation-of-a-neural-network" class="level3" data-number="9.3.4">
<h3 data-number="9.3.4" class="anchored" data-anchor-id="backpropagation-of-a-neural-network"><span class="header-section-number">9.3.4</span> Backpropagation of a Neural Network</h3>
<p>Now let’s look at backpropagation of a deep neural network. Before getting to it in the linear algebraic sense, let’s write everything in terms of scalars. This means we can write a simple neural network as:</p>
<p><span class="math display">\[\begin{align}
  z_i &amp;= \sum_j W_{ij}^1 x_j + b_i^1\\
  h_i &amp;= \sigma(z_i)\\
  y_i &amp;= \sum_j W_{ij}^2 h_j + b_i^2\\
  \mathcal{L} &amp;= \frac{1}{2} \sum_k \left(y_k - t_k \right)^2 \end{align}\]</span></p>
<p>where I have chosen the L2 loss function. This is visualized by the computational graph:</p>
<p><img src="https://user-images.githubusercontent.com/1814174/66464817-ad286d80-ea4d-11e9-9a4c-f7bcf1b34475.PNG" class="img-fluid"></p>
<p>Then we can do the same process as before to get:</p>
<p><span class="math display">\[\begin{align}
  \overline{\mathcal{L}} &amp;= 1\\
  \overline{y_i} &amp;= \overline{\mathcal{L}} (y_i - t_i)\\
  \overline{w_{ij}^2} &amp;= \overline{y_i} h_j\\
  \overline{b_i^2} &amp;= \overline{y_i}\\
  \overline{h_i} &amp;= \sum_k (\overline{y_k}w_{ki}^2)\\
  \overline{z_i} &amp;= \overline{h_i}\sigma^\prime(z_i)\\
  \overline{w_{ij}^1} &amp;= \overline{z_i} x_j\\
  \overline{b_i^1} &amp;= \overline{z_i}\end{align}\]</span></p>
<p>just by examining the computation graph. Now let’s write this in linear algebraic form.</p>
<p><img src="https://user-images.githubusercontent.com/1814174/66465741-69366800-ea4f-11e9-9c20-07806214008b.PNG" class="img-fluid"></p>
<p>The forward pass for this simple neural network was:</p>
<p><span class="math display">\[\begin{align}
  z &amp;= W_1 x + b_1\\
  h &amp;= \sigma(z)\\
  y &amp;= W_2 h + b_2\\
  \mathcal{L} = \frac{1}{2} \Vert y-t \Vert^2 \end{align}\]</span></p>
<p>If we carefully decode our scalar expression, we see that we get the following:</p>
<p><span class="math display">\[\begin{align}
  \overline{\mathcal{L}} &amp;= 1\\
  \overline{y} &amp;= \overline{\mathcal{L}}(y-t)\\
  \overline{W_2} &amp;= \overline{y}h^{T}\\
  \overline{b_2} &amp;= \overline{y}\\
  \overline{h} &amp;= W_2^T \overline{y}\\
  \overline{z} &amp;= \overline{h} .* \sigma^\prime(z)\\
  \overline{W_1} &amp;= \overline{z} x^T\\
  \overline{b_1} &amp;= \overline{z} \end{align}\]</span></p>
<p>We can thus decode the rules as:</p>
<ul>
<li>Multiplying by the matrix going forwards means multiplying by the transpose going backwards. A term on the left stays on the left, and a term on the right stays on the right.</li>
<li>Element-wise operations give element-wise multiplication</li>
</ul>
<p>Notice that the summation is then easily encoded into this rule by the transpose operation.</p>
<p>We can write it in the general DNN form of:</p>
<p><span class="math display">\[r_i = W_i v_{i} + b_i\]</span> <span class="math display">\[v_{i+1} = \sigma_i.(r_i)\]</span> <span class="math display">\[v_1 = x\]</span> <span class="math display">\[\mathcal{L} = \frac{1}{2} \Vert v_{n} - t \Vert\]</span></p>
<p><span class="math display">\[\begin{align}
  \overline{\mathcal{L}} &amp;= 1\\
  \overline{v_n} &amp;= \overline{\mathcal{L}}(y-t)\\
  \overline{r_i} &amp;= \overline{v_i} .* \sigma_i^\prime (r_i)\\
  \overline{W_i} &amp;= \overline{v_i}r_{i-1}^{T}\\
  \overline{b_i} &amp;= \overline{v_i}\\
  \overline{v_{i-1}} &amp;= W_{i}^{T} \overline{v_i} \end{align}\]</span></p>
</section>
<section id="reverse-mode-automatic-differentiation-and-vjps" class="level3" data-number="9.3.5">
<h3 data-number="9.3.5" class="anchored" data-anchor-id="reverse-mode-automatic-differentiation-and-vjps"><span class="header-section-number">9.3.5</span> Reverse-Mode Automatic Differentiation and vjps</h3>
<p>Backpropagation of a neural network is thus a different way of accumulating derivatives. If <span class="math inline">\(f\)</span> is a composition of <span class="math inline">\(L\)</span> functions:</p>
<p><span class="math display">\[f = f^L \circ f^{L-1} \circ \ldots \circ f^1\]</span></p>
<p>Then the Jacobian matrix satisfies:</p>
<p><span class="math display">\[J = J_L J_{L-1} \ldots J_1\]</span></p>
<p>A program is essentially a nice way of writing a function in composition form. Forward-mode automatic differentiation worked by propagating forward the actions of the Jacobians at every step of the program:</p>
<p><span class="math display">\[Jv = J_L (J_{L-1} (\ldots (J_1 v) \ldots ))\]</span></p>
<p>effectively calculating the Jacobian of the program by multiplying by the Jacobians from left to right at each step of the way. This means doing primitive <span class="math inline">\(Jv\)</span> calculations on each underlying problem, and pushing that calculation through.</p>
<p>But what about reverse accumulation? This can be isolated to the simple expression graph:</p>
<p><img src="https://user-images.githubusercontent.com/1814174/66471491-650f4800-ea59-11e9-9b42-4b32d0d0d76f.PNG" class="img-fluid"></p>
<p>In backpropagation, we just showed that when doing reverse accumulation, the rule is that multiplication forwards is multiplication by the transpose backwards. So if the forward way to compute the Jacobian in reverse is to replace the matrix by its transpose:</p>
<p><img src="https://user-images.githubusercontent.com/1814174/66471687-c6cfb200-ea59-11e9-9b80-f9206ffda87f.PNG" class="img-fluid"></p>
<p>We can either look at it as <span class="math inline">\(J^T v\)</span>, or by transposing the equation <span class="math inline">\(v^T J\)</span>. It’s right there that we have a vector-transpose Jacobian product, or a <em>vjp</em>.</p>
<p>We can thus think of this as a different direction for the Jacobian accumulation. Reverse-mode automatic differentiation moves backwards through our composed Jacobian. For a value <span class="math inline">\(v\)</span> at the end, we can push it backwards:</p>
<p><span class="math display">\[v^T J = (\ldots ((v^T J_L) J_{L-1}) \ldots ) J_1\]</span></p>
<p>doing a vjp at every step of the way, which is simply doing reverse-mode AD of that function (and if it’s linear, then simply doing the matrix multiplication). Thus reverse-mode AD is just a grouping of vjps into a single larger expression, instead of linearizing every single step.</p>
</section>
<section id="primitives-of-reverse-mode" class="level3" data-number="9.3.6">
<h3 data-number="9.3.6" class="anchored" data-anchor-id="primitives-of-reverse-mode"><span class="header-section-number">9.3.6</span> Primitives of Reverse Mode</h3>
<p>For forward-mode AD, we saw that we could define primitives in order to accelerate the calculation. For example, knowing that</p>
<p><span class="math display">\[exp(x+\epsilon) = exp(x) + exp(x)\epsilon\]</span></p>
<p>allows the program to skip autodifferentiating through the code for <code>exp</code>. This was simple with forward-mode since we could represent the operation on a Dual number. What’s the equivalent for reverse-mode AD? The answer is the <em>pullback</em> function. If <span class="math inline">\(y = [y_1,y_2,\ldots] = f(x_1,x_2, \ldots)\)</span>, then <span class="math inline">\([\overline{x_1},\overline{x_2},\ldots]=\mathcal{B}_f^x(\overline{y})\)</span> is the pullback of <span class="math inline">\(f\)</span> at the point <span class="math inline">\(x\)</span>, defined for a scalar loss function <span class="math inline">\(L(y)\)</span> as:</p>
<p><span class="math display">\[\overline{x_i} = \frac{\partial L}{\partial x_i} = \sum_j \frac{\partial L}{\partial y_j} \frac{\partial y_j}{\partial x_i}\]</span></p>
<p>Using the notation from earlier, <span class="math inline">\(\overline{y} = \frac{\partial L}{\partial y}\)</span> is the derivative of the some intermediate w.r.t. the cost function, and thus</p>
<p><span class="math display">\[\overline{x_i} = \sum_j \overline{y_j} \frac{\partial y_j}{\partial x_i} = \mathcal{B}_f^x(\overline{y})\]</span></p>
<p>Note that <span class="math inline">\(\mathcal{B}_f^x(\overline{y})\)</span> is a function of <span class="math inline">\(x\)</span> because the reverse pass that is use embeds values from the forward pass, and the values from the forward pass to use are those calculated during the evaluation of <span class="math inline">\(f(x)\)</span>.</p>
<p>By the chain rule, if we don’t have a primitive defined for <span class="math inline">\(y_i(x)\)</span>, we can compute that by <span class="math inline">\(\mathcal{B}_{y_i}(\overline{y})\)</span>, and recursively apply this process until we hit rules that we know. The rules to start with are the scalar derivative rules with follow quite simply, and the multivariate rules which we derived above. For example, if <span class="math inline">\(y=f(x)=Ax\)</span>, then</p>
<p><span class="math display">\[\mathcal{B}_{f}^x(\overline{y}) = \overline{y}^T A\]</span></p>
<p>which is simply saying that the Jacobian of <span class="math inline">\(f\)</span> at <span class="math inline">\(x\)</span> is <span class="math inline">\(A\)</span>, and so the vjp is to multiply the vector transpose by <span class="math inline">\(A\)</span>.</p>
<p>Likewise, for element-wise operations, the Jacobian is diagonal, and thus the vjp is multiplying once again by a diagonal matrix against the derivative, deriving the same pullback as we had for backpropagation in a neural network. This then is a quicker encoding and derivation of backpropagation.</p>
</section>
<section id="multivariate-derivatives-from-reverse-mode" class="level3" data-number="9.3.7">
<h3 data-number="9.3.7" class="anchored" data-anchor-id="multivariate-derivatives-from-reverse-mode"><span class="header-section-number">9.3.7</span> Multivariate Derivatives from Reverse Mode</h3>
<p>Since the primitive of reverse mode is the vjp, we can understand its behavior by looking at a large primitive. In our simplest case, the function <span class="math inline">\(f(x)=Ax\)</span> outputs a vector value, which we apply our loss function <span class="math inline">\(L(y) = \Vert y-t \Vert\)</span> to get a scalar. Thus we seed the scalar output <span class="math inline">\(v=1\)</span>, and in the first step backwards we have a vector to scalar function, so the first pullback transforms from <span class="math inline">\(1\)</span> to the vector <span class="math inline">\(v_2 = 2|y-t|\)</span>. Then we take that vector and multiply it like <span class="math inline">\(v_2^T A\)</span> to get the derivatives w.r.t. <span class="math inline">\(x\)</span>.</p>
<p>Now let <span class="math inline">\(L(y)\)</span> be a vector function, i.e.&nbsp;we output a vector instead of a scalar from our loss function. Then <span class="math inline">\(v\)</span> is the <em>seed</em> to this process. Let’s assume that <span class="math inline">\(v = e_i\)</span>, one of the basis vectors. Then</p>
<p><span class="math display">\[v_i^T J = e_i^T J\]</span></p>
<p>pulls computes a row of the Jacobian. There, if we had a vector function <span class="math inline">\(y=f(x)\)</span>, the pullback <span class="math inline">\(\mathcal{B}_f^x(e_i)\)</span> is the row of the Jacobian <span class="math inline">\(f'(x)\)</span>. Concatenating these is thus a way to build a full Jacobian. The gradient is thus a special case where <span class="math inline">\(y\)</span> is scalar, and thus the resulting Jacobian is just a single row, and therefore we set the seed equal to <span class="math inline">\(1\)</span> to compute the unscaled gradient.</p>
</section>
<section id="multi-seeding" class="level3" data-number="9.3.8">
<h3 data-number="9.3.8" class="anchored" data-anchor-id="multi-seeding"><span class="header-section-number">9.3.8</span> Multi-Seeding</h3>
<p>Similarly to forward-mode having a dual number with multiple simultaneous derivatives through partials <span class="math inline">\(d = x + v_1 \epsilon_1 + \ldots + v_m \epsilon_m\)</span>, one can see that multi-seeding is an option in reverse-mode AD by, instead of pulling back a matrix instead of a row vector, where each row is a direction. Thus the matrix <span class="math inline">\(A = [v_1 v_2 \ldots v_n]^T\)</span> evaluated as <span class="math inline">\(\mathcal{B}_f^x(A)\)</span> is the equivalent operation to the forward-mode <span class="math inline">\(f(d)\)</span> for generalized multivariate multiseeded reverse-mode automatic differentiation. One should take care to recognize the Jacobian as a generalized linear operator in this case and ensure that the shapes in the program correctly handle this storage of the reverse seed. When linear, this will automatically make use of BLAS3 operations, making it an efficient form for neural networks.</p>
</section>
<section id="sparse-reverse-mode-ad" class="level3" data-number="9.3.9">
<h3 data-number="9.3.9" class="anchored" data-anchor-id="sparse-reverse-mode-ad"><span class="header-section-number">9.3.9</span> Sparse Reverse Mode AD</h3>
<p>Since the Jacobian is built row-by-row with reverse mode AD, the sparse differentiation discussion from forward-mode AD applies similarly but to the transpose. Therefore, in order to perform sparse reverse mode automatic differentiation, one would build up a connectivity graph of the columns, and perform a coloring algorithm on this graph. The seeds of the reverse call, <span class="math inline">\(v_i\)</span>, would then be the color vectors, which would compute compressed rows, that are then decompressed similarly to the forward-mode case.</p>
</section>
<section id="forward-mode-vs-reverse-mode" class="level3" data-number="9.3.10">
<h3 data-number="9.3.10" class="anchored" data-anchor-id="forward-mode-vs-reverse-mode"><span class="header-section-number">9.3.10</span> Forward Mode vs Reverse Mode</h3>
<p>Notice that a pullback of a single scalar gives the gradient of a function, while the <em>pushforward</em> using forward-mode of a dual gives a directional derivative. Forward mode computes columns of a Jacobian, while reverse mode computes gradients (rows of a Jacobian). Therefore, the relative efficiency of the two approaches is based on the size of the Jacobian. If <span class="math inline">\(f:\mathbb{R}^n \rightarrow \mathbb{R}^m\)</span>, then the Jacobian is of size <span class="math display">\[m \times n\]</span>. If <span class="math inline">\(m\)</span> is much smaller than <span class="math inline">\(n\)</span>, then computing by each row will be faster, and thus use reverse mode. In the case of a gradient, <span class="math inline">\(m=1\)</span> while <span class="math inline">\(n\)</span> can be large, leading to this phenomena. Likewise, if <span class="math inline">\(n\)</span> is much smaller than <span class="math inline">\(m\)</span>, then computing by each column will be faster. We will see shortly the reverse mode AD has a high overhead with respect to forward mode, and thus if the values are relatively equal (or <span class="math inline">\(n\)</span> and <span class="math inline">\(m\)</span> are small), forward mode is more efficient.</p>
<p>However, since optimization needs gradients, reverse-mode definitely has a place in the standard toolchain which is why backpropagation is so central to machine learning.</p>
</section>
<section id="side-note-on-mixed-mode" class="level3" data-number="9.3.11">
<h3 data-number="9.3.11" class="anchored" data-anchor-id="side-note-on-mixed-mode"><span class="header-section-number">9.3.11</span> Side Note on Mixed Mode</h3>
<p>Interestingly, one can find cases where mixing the forward and reverse mode results would give an asymptotically better result. For example, if a Jacobian was non-zero in only the first 3 rows and first 3 columns, then sparse forward mode would still require N partials and reverse mode would require M seeds. However, one forward mode call of 3 partials and one reverse mode call of 3 seeds would calculate all three rows and columns with <span class="math inline">\(\mathcal{O}(1)\)</span> work, as opposed to <span class="math inline">\(\mathcal{O}(N)\)</span> or <span class="math inline">\(\mathcal{O}(M)\)</span>. Exactly how to make use of this insight in an automated manner is an open research question.</p>
</section>
<section id="forward-over-reverse-and-hessian-free-products" class="level3" data-number="9.3.12">
<h3 data-number="9.3.12" class="anchored" data-anchor-id="forward-over-reverse-and-hessian-free-products"><span class="header-section-number">9.3.12</span> Forward-Over-Reverse and Hessian-Free Products</h3>
<p>Using this knowledge, we can also develop quick ways for computing the Hessian. Recall from earlier in the discussion that Hessians are the Jacobian of the gradient. So let’s say for a scalar function <span class="math inline">\(f\)</span> we want to compute the Hessian. To compute the gradient, we use the reverse-mode AD pullback <span class="math inline">\(\nabla f(x) = \mathcal{B}_f^x(1)\)</span>. Recall that the pullback is a function of <span class="math inline">\(x\)</span> since that is the value at which the values from the forward pass are taken. Then since the Jacobian of the gradient vector is <span class="math inline">\(n \times n\)</span> (as many terms in the gradient as there are inputs!), it holds that we want to use forward-mode AD for this Jacobian. Therefore, using the dual number <span class="math inline">\(x = x_0 + e_1 \epsilon_1 + \ldots + e_n \epsilon_n\)</span> the reverse mode gradient function computes the full Hessian in one forward pass. What this amounts to is pushing forward the dual number forward sensitivities when building the pullback, and then when doing the pullback the dual portions, will be holding vectors for the columns of the Hessian.</p>
<p>Similarly, Hessian-vector products without computing the Hessian can be computed using the Jacobian-vector product trick on the function defined by the gradient. Here, <span class="math inline">\(Hv\)</span> is equivalent to the dual part of</p>
<p><span class="math inline">\(\nabla f(x+v\epsilon) = \mathcal{B}_f^{x+v\epsilon}(1)\)</span></p>
<p>This means that our Newton method for optimization:</p>
<p><span class="math display">\[p_{i+1} = p_i - H(p_i)^{-1} \frac{dC(p_i)}{dp}\]</span></p>
<p>can be treated similarly to that for the nonlinear solving problem, where the linear system can be solved using Hessian-free vector products to build a Krylov subspace, giving rise to the <em>Hessian-free Newton Krylov</em> method for optimization.</p>
</section>
<section id="references" class="level3" data-number="9.3.13">
<h3 data-number="9.3.13" class="anchored" data-anchor-id="references"><span class="header-section-number">9.3.13</span> References</h3>
<p>We thank <a href="https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec06.pdf">Roger Grosse’s lecture notes</a> for the amazing tikz graphs.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./stiff_odes.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Solving Stiff Ordinary Differential Equations</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./adjoints.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Differentiable Programming and Neural Differential Equations</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>